{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global imports\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import imageio\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import BatchNormalization, Conv2D, LeakyReLU, MaxPooling2D, Flatten, Dense\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import librosa\n",
    "import tensorflow_addons as tfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPU\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # Restrict TensorFlow to only use the first GPU\n",
    "  try:\n",
    "    tf.config.set_visible_devices(gpus[0], 'GPU')\n",
    "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n",
    "  except RuntimeError as e:\n",
    "    # Visible devices must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mport train\n",
    "df_train = pd.read_csv('d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/train_split_Depression_AVEC2017.csv')\n",
    "df_test = pd.read_csv('d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/test_split_Depression_AVEC2017.csv')\n",
    "df_test_full = pd.read_csv('d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/full_test_split.csv')\n",
    "df_dev = pd.read_csv('d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/dev_split_Depression_AVEC2017.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Participant_ID</th>\n",
       "      <th>PHQ8_Binary</th>\n",
       "      <th>PHQ8_Score</th>\n",
       "      <th>Gender</th>\n",
       "      <th>PHQ8_NoInterest</th>\n",
       "      <th>PHQ8_Depressed</th>\n",
       "      <th>PHQ8_Sleep</th>\n",
       "      <th>PHQ8_Tired</th>\n",
       "      <th>PHQ8_Appetite</th>\n",
       "      <th>PHQ8_Failure</th>\n",
       "      <th>PHQ8_Concentrating</th>\n",
       "      <th>PHQ8_Moving</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>302</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>307</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>331</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>335</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>346</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Participant_ID  PHQ8_Binary  PHQ8_Score  Gender  PHQ8_NoInterest  \\\n",
       "0             302            0           4       1                1   \n",
       "1             307            0           4       0                0   \n",
       "2             331            0           8       1                1   \n",
       "3             335            1          12       0                1   \n",
       "4             346            1          23       0                2   \n",
       "\n",
       "   PHQ8_Depressed  PHQ8_Sleep  PHQ8_Tired  PHQ8_Appetite  PHQ8_Failure  \\\n",
       "0               1           0           1              0             1   \n",
       "1               1           0           1              0             2   \n",
       "2               1           1           1              1             1   \n",
       "3               1           3           2              3             1   \n",
       "4               3           3           3              3             3   \n",
       "\n",
       "   PHQ8_Concentrating  PHQ8_Moving  \n",
       "0                   0            0  \n",
       "1                   0            0  \n",
       "2                   1            1  \n",
       "3                   1            0  \n",
       "4                   3            3  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See head\n",
    "df_dev.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['Participant_ID'] = df_test['participant_ID']\n",
    "df_test.drop('participant_ID', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107 47 35 189\n"
     ]
    }
   ],
   "source": [
    "# Check if all data seems ok\n",
    "print(df_train.shape[0], df_test_full.shape[0], df_dev.shape[0], df_train.shape[0]+ df_test_full.shape[0]+ df_dev.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import imageio\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def load_spectrogram(df):\n",
    "    # Define root path of data\n",
    "    root_path = 'd:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data'  # Replace with your root path\n",
    "\n",
    "    # Add the column for spectrogram if it doesn't exist\n",
    "    if 'Spectrogram' not in df.columns:\n",
    "        # Define the shape of the fixed tensor\n",
    "        tensor_shape = (292, 252, 3)  # Example shape (height, width, channels)\n",
    "\n",
    "        # Create a fixed tensor of zeros with the desired shape\n",
    "        fixed_tensor = np.zeros(tensor_shape, dtype=np.uint8)  # Adjust dtype as needed\n",
    "\n",
    "        # Initialize the 'Spectrogram' column with the fixed tensor\n",
    "        df['Spectrogram'] = [fixed_tensor.copy() for _ in range(len(df))]\n",
    "\n",
    "    # Handle missing values in 'Participant_ID' column\n",
    "    df['Participant_ID'] = pd.to_numeric(df['Participant_ID'], errors='coerce')\n",
    "\n",
    "    # Iterate over the dataframe to add the image information\n",
    "    for index, row in df.iterrows():\n",
    "        if not pd.isnull(row['Participant_ID']):  # Check if Participant_ID is not NaN\n",
    "            participant_id = str(int(row['Participant_ID']))\n",
    "            folder_path = os.path.join(root_path, str(participant_id) + '_P')\n",
    "\n",
    "            if os.path.isdir(folder_path):\n",
    "                image_path = os.path.join(folder_path, 'resized_spectrogram_preprocessed_' + str(participant_id) + '_AUDIO.jpg')  # Replace with your image name\n",
    "                if os.path.isfile(image_path):\n",
    "                    image = imageio.imread(image_path)\n",
    "                    df.at[index, 'Spectrogram'] = image\n",
    "                else:\n",
    "                    print(f\"Image file not found at path {image_path}. Skipping...\")\n",
    "            else:\n",
    "                print(f\"Folder not found at path {folder_path}. Skipping...\")\n",
    "            \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\301_P\\resized_spectrogram_preprocessed_301_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\306_P\\resized_spectrogram_preprocessed_306_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\308_P\\resized_spectrogram_preprocessed_308_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\309_P\\resized_spectrogram_preprocessed_309_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\311_P\\resized_spectrogram_preprocessed_311_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\314_P\\resized_spectrogram_preprocessed_314_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\323_P\\resized_spectrogram_preprocessed_323_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\329_P\\resized_spectrogram_preprocessed_329_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\332_P\\resized_spectrogram_preprocessed_332_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\334_P\\resized_spectrogram_preprocessed_334_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\337_P\\resized_spectrogram_preprocessed_337_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\349_P\\resized_spectrogram_preprocessed_349_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\354_P\\resized_spectrogram_preprocessed_354_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\359_P\\resized_spectrogram_preprocessed_359_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\361_P\\resized_spectrogram_preprocessed_361_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\365_P\\resized_spectrogram_preprocessed_365_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\373_P\\resized_spectrogram_preprocessed_373_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\378_P\\resized_spectrogram_preprocessed_378_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\384_P\\resized_spectrogram_preprocessed_384_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\387_P\\resized_spectrogram_preprocessed_387_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\396_P\\resized_spectrogram_preprocessed_396_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\399_P\\resized_spectrogram_preprocessed_399_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\405_P\\resized_spectrogram_preprocessed_405_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\407_P\\resized_spectrogram_preprocessed_407_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\408_P\\resized_spectrogram_preprocessed_408_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\410_P\\resized_spectrogram_preprocessed_410_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\411_P\\resized_spectrogram_preprocessed_411_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\421_P\\resized_spectrogram_preprocessed_421_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\424_P\\resized_spectrogram_preprocessed_424_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\431_P\\resized_spectrogram_preprocessed_431_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\432_P\\resized_spectrogram_preprocessed_432_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\435_P\\resized_spectrogram_preprocessed_435_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\438_P\\resized_spectrogram_preprocessed_438_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\442_P\\resized_spectrogram_preprocessed_442_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\450_P\\resized_spectrogram_preprocessed_450_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\452_P\\resized_spectrogram_preprocessed_452_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\453_P\\resized_spectrogram_preprocessed_453_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\461_P\\resized_spectrogram_preprocessed_461_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\462_P\\resized_spectrogram_preprocessed_462_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\465_P\\resized_spectrogram_preprocessed_465_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\466_P\\resized_spectrogram_preprocessed_466_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\467_P\\resized_spectrogram_preprocessed_467_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\469_P\\resized_spectrogram_preprocessed_469_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\470_P\\resized_spectrogram_preprocessed_470_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\480_P\\resized_spectrogram_preprocessed_480_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\481_P\\resized_spectrogram_preprocessed_481_AUDIO.jpg. Skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\herna\\AppData\\Local\\Temp\\ipykernel_30556\\2803068170.py:33: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  image = imageio.imread(image_path)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_test = load_spectrogram(df_test)\n",
    "# Define root path of data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\301_P\\resized_spectrogram_preprocessed_301_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\306_P\\resized_spectrogram_preprocessed_306_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\308_P\\resized_spectrogram_preprocessed_308_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\309_P\\resized_spectrogram_preprocessed_309_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\311_P\\resized_spectrogram_preprocessed_311_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\314_P\\resized_spectrogram_preprocessed_314_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\323_P\\resized_spectrogram_preprocessed_323_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\329_P\\resized_spectrogram_preprocessed_329_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\332_P\\resized_spectrogram_preprocessed_332_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\334_P\\resized_spectrogram_preprocessed_334_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\337_P\\resized_spectrogram_preprocessed_337_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\349_P\\resized_spectrogram_preprocessed_349_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\354_P\\resized_spectrogram_preprocessed_354_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\359_P\\resized_spectrogram_preprocessed_359_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\361_P\\resized_spectrogram_preprocessed_361_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\365_P\\resized_spectrogram_preprocessed_365_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\373_P\\resized_spectrogram_preprocessed_373_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\378_P\\resized_spectrogram_preprocessed_378_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\384_P\\resized_spectrogram_preprocessed_384_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\387_P\\resized_spectrogram_preprocessed_387_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\396_P\\resized_spectrogram_preprocessed_396_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\399_P\\resized_spectrogram_preprocessed_399_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\405_P\\resized_spectrogram_preprocessed_405_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\407_P\\resized_spectrogram_preprocessed_407_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\408_P\\resized_spectrogram_preprocessed_408_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\410_P\\resized_spectrogram_preprocessed_410_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\411_P\\resized_spectrogram_preprocessed_411_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\421_P\\resized_spectrogram_preprocessed_421_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\424_P\\resized_spectrogram_preprocessed_424_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\431_P\\resized_spectrogram_preprocessed_431_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\432_P\\resized_spectrogram_preprocessed_432_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\435_P\\resized_spectrogram_preprocessed_435_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\438_P\\resized_spectrogram_preprocessed_438_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\442_P\\resized_spectrogram_preprocessed_442_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\450_P\\resized_spectrogram_preprocessed_450_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\452_P\\resized_spectrogram_preprocessed_452_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\453_P\\resized_spectrogram_preprocessed_453_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\461_P\\resized_spectrogram_preprocessed_461_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\462_P\\resized_spectrogram_preprocessed_462_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\465_P\\resized_spectrogram_preprocessed_465_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\466_P\\resized_spectrogram_preprocessed_466_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\467_P\\resized_spectrogram_preprocessed_467_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\469_P\\resized_spectrogram_preprocessed_469_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\470_P\\resized_spectrogram_preprocessed_470_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\480_P\\resized_spectrogram_preprocessed_480_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\481_P\\resized_spectrogram_preprocessed_481_AUDIO.jpg. Skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\herna\\AppData\\Local\\Temp\\ipykernel_30556\\2803068170.py:33: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  image = imageio.imread(image_path)\n"
     ]
    }
   ],
   "source": [
    "df_test_full = load_spectrogram(df_test_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = df_test['Spectrogram']\n",
    "y_test = df_test_full['PHQ_Binary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\303_P\\resized_spectrogram_preprocessed_303_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\304_P\\resized_spectrogram_preprocessed_304_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\305_P\\resized_spectrogram_preprocessed_305_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\310_P\\resized_spectrogram_preprocessed_310_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\312_P\\resized_spectrogram_preprocessed_312_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\313_P\\resized_spectrogram_preprocessed_313_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\315_P\\resized_spectrogram_preprocessed_315_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\316_P\\resized_spectrogram_preprocessed_316_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\317_P\\resized_spectrogram_preprocessed_317_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\318_P\\resized_spectrogram_preprocessed_318_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\319_P\\resized_spectrogram_preprocessed_319_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\320_P\\resized_spectrogram_preprocessed_320_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\321_P\\resized_spectrogram_preprocessed_321_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\322_P\\resized_spectrogram_preprocessed_322_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\324_P\\resized_spectrogram_preprocessed_324_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\325_P\\resized_spectrogram_preprocessed_325_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\326_P\\resized_spectrogram_preprocessed_326_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\327_P\\resized_spectrogram_preprocessed_327_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\328_P\\resized_spectrogram_preprocessed_328_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\330_P\\resized_spectrogram_preprocessed_330_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\333_P\\resized_spectrogram_preprocessed_333_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\336_P\\resized_spectrogram_preprocessed_336_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\338_P\\resized_spectrogram_preprocessed_338_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\339_P\\resized_spectrogram_preprocessed_339_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\340_P\\resized_spectrogram_preprocessed_340_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\341_P\\resized_spectrogram_preprocessed_341_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\343_P\\resized_spectrogram_preprocessed_343_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\344_P\\resized_spectrogram_preprocessed_344_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\345_P\\resized_spectrogram_preprocessed_345_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\347_P\\resized_spectrogram_preprocessed_347_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\348_P\\resized_spectrogram_preprocessed_348_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\350_P\\resized_spectrogram_preprocessed_350_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\351_P\\resized_spectrogram_preprocessed_351_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\352_P\\resized_spectrogram_preprocessed_352_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\353_P\\resized_spectrogram_preprocessed_353_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\355_P\\resized_spectrogram_preprocessed_355_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\356_P\\resized_spectrogram_preprocessed_356_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\357_P\\resized_spectrogram_preprocessed_357_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\358_P\\resized_spectrogram_preprocessed_358_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\360_P\\resized_spectrogram_preprocessed_360_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\362_P\\resized_spectrogram_preprocessed_362_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\363_P\\resized_spectrogram_preprocessed_363_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\364_P\\resized_spectrogram_preprocessed_364_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\366_P\\resized_spectrogram_preprocessed_366_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\368_P\\resized_spectrogram_preprocessed_368_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\369_P\\resized_spectrogram_preprocessed_369_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\370_P\\resized_spectrogram_preprocessed_370_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\371_P\\resized_spectrogram_preprocessed_371_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\372_P\\resized_spectrogram_preprocessed_372_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\374_P\\resized_spectrogram_preprocessed_374_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\375_P\\resized_spectrogram_preprocessed_375_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\376_P\\resized_spectrogram_preprocessed_376_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\379_P\\resized_spectrogram_preprocessed_379_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\380_P\\resized_spectrogram_preprocessed_380_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\383_P\\resized_spectrogram_preprocessed_383_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\385_P\\resized_spectrogram_preprocessed_385_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\386_P\\resized_spectrogram_preprocessed_386_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\391_P\\resized_spectrogram_preprocessed_391_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\392_P\\resized_spectrogram_preprocessed_392_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\393_P\\resized_spectrogram_preprocessed_393_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\397_P\\resized_spectrogram_preprocessed_397_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\400_P\\resized_spectrogram_preprocessed_400_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\401_P\\resized_spectrogram_preprocessed_401_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\402_P\\resized_spectrogram_preprocessed_402_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\409_P\\resized_spectrogram_preprocessed_409_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\412_P\\resized_spectrogram_preprocessed_412_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\414_P\\resized_spectrogram_preprocessed_414_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\415_P\\resized_spectrogram_preprocessed_415_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\416_P\\resized_spectrogram_preprocessed_416_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\419_P\\resized_spectrogram_preprocessed_419_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\423_P\\resized_spectrogram_preprocessed_423_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\425_P\\resized_spectrogram_preprocessed_425_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\426_P\\resized_spectrogram_preprocessed_426_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\427_P\\resized_spectrogram_preprocessed_427_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\428_P\\resized_spectrogram_preprocessed_428_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\429_P\\resized_spectrogram_preprocessed_429_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\430_P\\resized_spectrogram_preprocessed_430_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\433_P\\resized_spectrogram_preprocessed_433_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\434_P\\resized_spectrogram_preprocessed_434_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\437_P\\resized_spectrogram_preprocessed_437_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\441_P\\resized_spectrogram_preprocessed_441_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\443_P\\resized_spectrogram_preprocessed_443_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\444_P\\resized_spectrogram_preprocessed_444_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\445_P\\resized_spectrogram_preprocessed_445_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\446_P\\resized_spectrogram_preprocessed_446_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\447_P\\resized_spectrogram_preprocessed_447_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\448_P\\resized_spectrogram_preprocessed_448_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\449_P\\resized_spectrogram_preprocessed_449_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\454_P\\resized_spectrogram_preprocessed_454_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\455_P\\resized_spectrogram_preprocessed_455_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\456_P\\resized_spectrogram_preprocessed_456_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\457_P\\resized_spectrogram_preprocessed_457_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\459_P\\resized_spectrogram_preprocessed_459_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\463_P\\resized_spectrogram_preprocessed_463_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\464_P\\resized_spectrogram_preprocessed_464_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\468_P\\resized_spectrogram_preprocessed_468_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\471_P\\resized_spectrogram_preprocessed_471_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\473_P\\resized_spectrogram_preprocessed_473_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\474_P\\resized_spectrogram_preprocessed_474_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\475_P\\resized_spectrogram_preprocessed_475_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\478_P\\resized_spectrogram_preprocessed_478_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\479_P\\resized_spectrogram_preprocessed_479_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\485_P\\resized_spectrogram_preprocessed_485_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\486_P\\resized_spectrogram_preprocessed_486_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\487_P\\resized_spectrogram_preprocessed_487_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\488_P\\resized_spectrogram_preprocessed_488_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\491_P\\resized_spectrogram_preprocessed_491_AUDIO.jpg. Skipping...\n"
     ]
    }
   ],
   "source": [
    "df_train = load_spectrogram(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Participant_ID</th>\n",
       "      <th>PHQ8_Binary</th>\n",
       "      <th>PHQ8_Score</th>\n",
       "      <th>Gender</th>\n",
       "      <th>PHQ8_NoInterest</th>\n",
       "      <th>PHQ8_Depressed</th>\n",
       "      <th>PHQ8_Sleep</th>\n",
       "      <th>PHQ8_Tired</th>\n",
       "      <th>PHQ8_Appetite</th>\n",
       "      <th>PHQ8_Failure</th>\n",
       "      <th>PHQ8_Concentrating</th>\n",
       "      <th>PHQ8_Moving</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>302</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>307</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>331</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>335</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>346</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Participant_ID  PHQ8_Binary  PHQ8_Score  Gender  PHQ8_NoInterest  \\\n",
       "0             302            0           4       1                1   \n",
       "1             307            0           4       0                0   \n",
       "2             331            0           8       1                1   \n",
       "3             335            1          12       0                1   \n",
       "4             346            1          23       0                2   \n",
       "\n",
       "   PHQ8_Depressed  PHQ8_Sleep  PHQ8_Tired  PHQ8_Appetite  PHQ8_Failure  \\\n",
       "0               1           0           1              0             1   \n",
       "1               1           0           1              0             2   \n",
       "2               1           1           1              1             1   \n",
       "3               1           3           2              3             1   \n",
       "4               3           3           3              3             3   \n",
       "\n",
       "   PHQ8_Concentrating  PHQ8_Moving  \n",
       "0                   0            0  \n",
       "1                   0            0  \n",
       "2                   1            1  \n",
       "3                   1            0  \n",
       "4                   3            3  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dev.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\302_P\\resized_spectrogram_preprocessed_302_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\307_P\\resized_spectrogram_preprocessed_307_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\331_P\\resized_spectrogram_preprocessed_331_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\335_P\\resized_spectrogram_preprocessed_335_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\346_P\\resized_spectrogram_preprocessed_346_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\367_P\\resized_spectrogram_preprocessed_367_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\377_P\\resized_spectrogram_preprocessed_377_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\381_P\\resized_spectrogram_preprocessed_381_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\382_P\\resized_spectrogram_preprocessed_382_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\388_P\\resized_spectrogram_preprocessed_388_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\389_P\\resized_spectrogram_preprocessed_389_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\390_P\\resized_spectrogram_preprocessed_390_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\395_P\\resized_spectrogram_preprocessed_395_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\403_P\\resized_spectrogram_preprocessed_403_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\404_P\\resized_spectrogram_preprocessed_404_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\406_P\\resized_spectrogram_preprocessed_406_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\413_P\\resized_spectrogram_preprocessed_413_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\417_P\\resized_spectrogram_preprocessed_417_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\418_P\\resized_spectrogram_preprocessed_418_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\420_P\\resized_spectrogram_preprocessed_420_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\422_P\\resized_spectrogram_preprocessed_422_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\436_P\\resized_spectrogram_preprocessed_436_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\439_P\\resized_spectrogram_preprocessed_439_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\440_P\\resized_spectrogram_preprocessed_440_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\451_P\\resized_spectrogram_preprocessed_451_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\458_P\\resized_spectrogram_preprocessed_458_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\472_P\\resized_spectrogram_preprocessed_472_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\476_P\\resized_spectrogram_preprocessed_476_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\477_P\\resized_spectrogram_preprocessed_477_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\482_P\\resized_spectrogram_preprocessed_482_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\483_P\\resized_spectrogram_preprocessed_483_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\484_P\\resized_spectrogram_preprocessed_484_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\489_P\\resized_spectrogram_preprocessed_489_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\490_P\\resized_spectrogram_preprocessed_490_AUDIO.jpg. Skipping...\n",
      "Image file not found at path d:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/data\\492_P\\resized_spectrogram_preprocessed_492_AUDIO.jpg. Skipping...\n"
     ]
    }
   ],
   "source": [
    "df_dev = load_spectrogram(df_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Participant_ID</th>\n",
       "      <th>PHQ8_Binary</th>\n",
       "      <th>PHQ8_Score</th>\n",
       "      <th>Gender</th>\n",
       "      <th>PHQ8_NoInterest</th>\n",
       "      <th>PHQ8_Depressed</th>\n",
       "      <th>PHQ8_Sleep</th>\n",
       "      <th>PHQ8_Tired</th>\n",
       "      <th>PHQ8_Appetite</th>\n",
       "      <th>PHQ8_Failure</th>\n",
       "      <th>PHQ8_Concentrating</th>\n",
       "      <th>PHQ8_Moving</th>\n",
       "      <th>Spectrogram</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>302</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[[[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>307</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[[[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>331</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[[[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>335</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[[[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>346</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>[[[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Participant_ID  PHQ8_Binary  PHQ8_Score  Gender  PHQ8_NoInterest  \\\n",
       "0             302            0           4       1                1   \n",
       "1             307            0           4       0                0   \n",
       "2             331            0           8       1                1   \n",
       "3             335            1          12       0                1   \n",
       "4             346            1          23       0                2   \n",
       "\n",
       "   PHQ8_Depressed  PHQ8_Sleep  PHQ8_Tired  PHQ8_Appetite  PHQ8_Failure  \\\n",
       "0               1           0           1              0             1   \n",
       "1               1           0           1              0             2   \n",
       "2               1           1           1              1             1   \n",
       "3               1           3           2              3             1   \n",
       "4               3           3           3              3             3   \n",
       "\n",
       "   PHQ8_Concentrating  PHQ8_Moving  \\\n",
       "0                   0            0   \n",
       "1                   0            0   \n",
       "2                   1            1   \n",
       "3                   1            0   \n",
       "4                   3            3   \n",
       "\n",
       "                                         Spectrogram  \n",
       "0  [[[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], ...  \n",
       "1  [[[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], ...  \n",
       "2  [[[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], ...  \n",
       "3  [[[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], ...  \n",
       "4  [[[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], ...  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dev.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(292, 252, 3)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.iloc[[1]]['Spectrogram'][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of depressed instances: 86\n",
      "Number of non-depressed instances: 21\n"
     ]
    }
   ],
   "source": [
    "depressed_count = df_train['PHQ8_Depressed'].sum()\n",
    "non_depressed_count = len(df_train) - depressed_count\n",
    "\n",
    "print(\"Number of depressed instances:\", depressed_count)\n",
    "print(\"Number of non-depressed instances:\", non_depressed_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Participant_ID        43\n",
       "PHQ8_Binary           43\n",
       "PHQ8_Score            43\n",
       "Gender                43\n",
       "PHQ8_NoInterest       43\n",
       "PHQ8_Depressed        43\n",
       "PHQ8_Sleep            42\n",
       "PHQ8_Tired            43\n",
       "PHQ8_Appetite         43\n",
       "PHQ8_Failure          43\n",
       "PHQ8_Concentrating    43\n",
       "PHQ8_Moving           43\n",
       "Spectrogram           43\n",
       "dtype: int64"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[df_train['PHQ8_Depressed'] == 1].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Participant_ID        46\n",
       "PHQ8_Binary           46\n",
       "PHQ8_Score            46\n",
       "Gender                46\n",
       "PHQ8_NoInterest       46\n",
       "PHQ8_Depressed        46\n",
       "PHQ8_Sleep            46\n",
       "PHQ8_Tired            46\n",
       "PHQ8_Appetite         46\n",
       "PHQ8_Failure          46\n",
       "PHQ8_Concentrating    46\n",
       "PHQ8_Moving           46\n",
       "Spectrogram           46\n",
       "dtype: int64"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[df_train['PHQ8_Depressed'] == 0].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(107, 292, 252, 3)   (107,)\n",
      "(35, 292, 252, 3)   (35,)\n"
     ]
    }
   ],
   "source": [
    "# Define a custom callback to print the epoch number\n",
    "class PrintEpochNumber(Callback):\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        print(f\"Epoch {epoch + 1}/{self.params['epochs']}\")\n",
    "\n",
    "# Assuming df_train and df_dev are your DataFrames and they're already preprocessed\n",
    "X_train = np.array(df_train['Spectrogram'].tolist())\n",
    "y_train = df_train['PHQ8_Depressed'].values\n",
    "\n",
    "X_val = np.array(df_dev['Spectrogram'].tolist())\n",
    "y_val = df_dev['PHQ8_Depressed'].values\n",
    "\n",
    "print(X_train.shape, ' ', y_train.shape)\n",
    "print(X_val.shape, ' ', y_val.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joining dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_full['PHQ8_Binary'] = df_test_full['PHQ_Binary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_full.drop('PHQ_Binary', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df_train = pd.concat([df_test_full, df_dev, df_train], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(189, 14)\n",
      "(35, 13)\n",
      "(47, 5)\n",
      "(107, 13)\n"
     ]
    }
   ],
   "source": [
    "print(new_df_train.shape)\n",
    "print(df_dev.shape)\n",
    "print(df_test_full.shape)\n",
    "print(df_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Participant_ID', 'PHQ_Score', 'Gender', 'Spectrogram', 'PHQ8_Binary',\n",
      "       'PHQ8_Score', 'PHQ8_NoInterest', 'PHQ8_Depressed', 'PHQ8_Sleep',\n",
      "       'PHQ8_Tired', 'PHQ8_Appetite', 'PHQ8_Failure', 'PHQ8_Concentrating',\n",
      "       'PHQ8_Moving'],\n",
      "      dtype='object')\n",
      "Index(['Participant_ID', 'PHQ8_Binary', 'PHQ8_Score', 'Gender',\n",
      "       'PHQ8_NoInterest', 'PHQ8_Depressed', 'PHQ8_Sleep', 'PHQ8_Tired',\n",
      "       'PHQ8_Appetite', 'PHQ8_Failure', 'PHQ8_Concentrating', 'PHQ8_Moving',\n",
      "       'Spectrogram'],\n",
      "      dtype='object')\n",
      "Index(['Participant_ID', 'PHQ_Score', 'Gender', 'Spectrogram', 'PHQ8_Binary'], dtype='object')\n",
      "Index(['Participant_ID', 'PHQ8_Binary', 'PHQ8_Score', 'Gender',\n",
      "       'PHQ8_NoInterest', 'PHQ8_Depressed', 'PHQ8_Sleep', 'PHQ8_Tired',\n",
      "       'PHQ8_Appetite', 'PHQ8_Failure', 'PHQ8_Concentrating', 'PHQ8_Moving',\n",
      "       'Spectrogram'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(new_df_train.columns)\n",
    "print(df_dev.columns)\n",
    "print(df_test_full.columns)\n",
    "print(df_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Participant_ID</th>\n",
       "      <th>PHQ_Score</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Spectrogram</th>\n",
       "      <th>PHQ8_Binary</th>\n",
       "      <th>PHQ8_Score</th>\n",
       "      <th>PHQ8_NoInterest</th>\n",
       "      <th>PHQ8_Depressed</th>\n",
       "      <th>PHQ8_Sleep</th>\n",
       "      <th>PHQ8_Tired</th>\n",
       "      <th>PHQ8_Appetite</th>\n",
       "      <th>PHQ8_Failure</th>\n",
       "      <th>PHQ8_Concentrating</th>\n",
       "      <th>PHQ8_Moving</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>300</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>[[[0, 0, 4], [0, 0, 4], [0, 0, 4], [0, 0, 4], ...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>301</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>[[[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], ...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>306</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>[[[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], ...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>308</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0</td>\n",
       "      <td>[[[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], ...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>309</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1</td>\n",
       "      <td>[[[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], ...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>485</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>[[[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], ...</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>486</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>[[[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], ...</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>487</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>[[[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>488</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>[[[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>491</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>[[[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], ...</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>189 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Participant_ID  PHQ_Score  Gender  \\\n",
       "0               300        2.0       1   \n",
       "1               301        3.0       1   \n",
       "2               306        0.0       0   \n",
       "3               308       22.0       0   \n",
       "4               309       15.0       1   \n",
       "..              ...        ...     ...   \n",
       "102             485        NaN       1   \n",
       "103             486        NaN       0   \n",
       "104             487        NaN       0   \n",
       "105             488        NaN       0   \n",
       "106             491        NaN       0   \n",
       "\n",
       "                                           Spectrogram  PHQ8_Binary  \\\n",
       "0    [[[0, 0, 4], [0, 0, 4], [0, 0, 4], [0, 0, 4], ...            0   \n",
       "1    [[[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], ...            0   \n",
       "2    [[[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], ...            0   \n",
       "3    [[[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], ...            1   \n",
       "4    [[[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], ...            1   \n",
       "..                                                 ...          ...   \n",
       "102  [[[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], ...            0   \n",
       "103  [[[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], ...            0   \n",
       "104  [[[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], ...            0   \n",
       "105  [[[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], ...            0   \n",
       "106  [[[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], ...            0   \n",
       "\n",
       "     PHQ8_Score  PHQ8_NoInterest  PHQ8_Depressed  PHQ8_Sleep  PHQ8_Tired  \\\n",
       "0           NaN              NaN             NaN         NaN         NaN   \n",
       "1           NaN              NaN             NaN         NaN         NaN   \n",
       "2           NaN              NaN             NaN         NaN         NaN   \n",
       "3           NaN              NaN             NaN         NaN         NaN   \n",
       "4           NaN              NaN             NaN         NaN         NaN   \n",
       "..          ...              ...             ...         ...         ...   \n",
       "102         2.0              0.0             1.0         0.0         0.0   \n",
       "103         4.0              1.0             1.0         0.0         1.0   \n",
       "104         0.0              0.0             0.0         0.0         0.0   \n",
       "105         0.0              0.0             0.0         0.0         0.0   \n",
       "106         8.0              1.0             1.0         1.0         1.0   \n",
       "\n",
       "     PHQ8_Appetite  PHQ8_Failure  PHQ8_Concentrating  PHQ8_Moving  \n",
       "0              NaN           NaN                 NaN          NaN  \n",
       "1              NaN           NaN                 NaN          NaN  \n",
       "2              NaN           NaN                 NaN          NaN  \n",
       "3              NaN           NaN                 NaN          NaN  \n",
       "4              NaN           NaN                 NaN          NaN  \n",
       "..             ...           ...                 ...          ...  \n",
       "102            0.0           0.0                 0.0          1.0  \n",
       "103            0.0           1.0                 0.0          0.0  \n",
       "104            0.0           0.0                 0.0          0.0  \n",
       "105            0.0           0.0                 0.0          0.0  \n",
       "106            1.0           1.0                 1.0          1.0  \n",
       "\n",
       "[189 rows x 14 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df_train = new_df_train[['Participant_ID', 'PHQ8_Binary', 'Spectrogram']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(189, 3)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(new_df_train[new_df_train['PHQ8_Binary'] == 1].count())[\"PHQ8_Binary\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "133"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df_train[new_df_train['PHQ8_Binary'] == 0].count()['PHQ8_Binary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(189,)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df_train['Spectrogram'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Shape: (292, 252, 3)\n",
      "Unique Shape: (252, 792, 3)\n"
     ]
    }
   ],
   "source": [
    "# Assuming new_df_train['Spectrogram'].tolist() contains sequences\n",
    "sequences = new_df_train['Spectrogram'].tolist()\n",
    "\n",
    "# Set to keep track of unique shapes\n",
    "unique_shapes = set()\n",
    "\n",
    "# Print the shapes of each sequence\n",
    "for i, sequence in enumerate(sequences):\n",
    "    shape = sequence.shape if hasattr(sequence, 'shape') else \"Not a numpy array\"\n",
    "    unique_shapes.add(shape)\n",
    "\n",
    "# Print each unique shape\n",
    "for shape in unique_shapes:\n",
    "    print(\"Unique Shape:\", shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Participant_ID</th>\n",
       "      <th>PHQ8_Binary</th>\n",
       "      <th>Spectrogram</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>300</td>\n",
       "      <td>0</td>\n",
       "      <td>[[[0, 0, 4], [0, 0, 4], [0, 0, 4], [0, 0, 4], ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>301</td>\n",
       "      <td>0</td>\n",
       "      <td>[[[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>306</td>\n",
       "      <td>0</td>\n",
       "      <td>[[[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>308</td>\n",
       "      <td>1</td>\n",
       "      <td>[[[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>309</td>\n",
       "      <td>1</td>\n",
       "      <td>[[[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Participant_ID  PHQ8_Binary  \\\n",
       "0             300            0   \n",
       "1             301            0   \n",
       "2             306            0   \n",
       "3             308            1   \n",
       "4             309            1   \n",
       "\n",
       "                                         Spectrogram  \n",
       "0  [[[0, 0, 4], [0, 0, 4], [0, 0, 4], [0, 0, 4], ...  \n",
       "1  [[[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], ...  \n",
       "2  [[[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], ...  \n",
       "3  [[[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], ...  \n",
       "4  [[[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], ...  "
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row: Participant_ID                                                  300\n",
      "PHQ8_Binary                                                       0\n",
      "Spectrogram       [[[0, 0, 4], [0, 0, 4], [0, 0, 4], [0, 0, 4], ...\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Assuming new_df_train is the original DataFrame\n",
    "for index, row in new_df_train.iterrows():\n",
    "    sequence = row['Spectrogram']\n",
    "    if hasattr(sequence, 'shape') and sequence.shape == (252, 792, 3):\n",
    "        print(\"Row:\", row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (189,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[68], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m X_train \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_df_train\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSpectrogram\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m y_train \u001b[38;5;241m=\u001b[39m new_df_train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPHQ8_Binary\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (189,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "X_train = np.array(new_df_train['Spectrogram'].tolist())\n",
    "y_train = new_df_train['PHQ8_Binary'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "\n",
    "mean_acc_per_fold = []\n",
    "mean_loss_per_fold = []\n",
    "kfold_list = []\n",
    "mean_precision_per_fold = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ...\n",
      "Epoch 1/100\n",
      "126/126 - 15s - loss: 35.4326 - accuracy: 0.5159 - precision: 0.1622 - specificity_at_sensitivity: 0.0000e+00 - 15s/epoch - 119ms/step\n",
      "Epoch 2/100\n",
      "126/126 - 11s - loss: 0.2896 - accuracy: 0.8889 - precision: 0.8438 - specificity_at_sensitivity: 0.0222 - 11s/epoch - 91ms/step\n",
      "Epoch 3/100\n",
      "126/126 - 7s - loss: 0.0046 - accuracy: 1.0000 - precision: 1.0000 - specificity_at_sensitivity: 1.0000 - 7s/epoch - 55ms/step\n",
      "Epoch 4/100\n",
      "126/126 - 7s - loss: 4.3657e-04 - accuracy: 1.0000 - precision: 1.0000 - specificity_at_sensitivity: 1.0000 - 7s/epoch - 52ms/step\n",
      "Epoch 5/100\n",
      "126/126 - 8s - loss: 2.7112e-04 - accuracy: 1.0000 - precision: 1.0000 - specificity_at_sensitivity: 1.0000 - 8s/epoch - 62ms/step\n",
      "Epoch 6/100\n",
      "126/126 - 8s - loss: 1.9041e-04 - accuracy: 1.0000 - precision: 1.0000 - specificity_at_sensitivity: 1.0000 - 8s/epoch - 60ms/step\n",
      "Epoch 7/100\n",
      "126/126 - 7s - loss: 1.4288e-04 - accuracy: 1.0000 - precision: 1.0000 - specificity_at_sensitivity: 1.0000 - 7s/epoch - 59ms/step\n",
      "Epoch 8/100\n",
      "126/126 - 7s - loss: 1.1215e-04 - accuracy: 1.0000 - precision: 1.0000 - specificity_at_sensitivity: 1.0000 - 7s/epoch - 55ms/step\n",
      "Epoch 9/100\n",
      "126/126 - 7s - loss: 9.0638e-05 - accuracy: 1.0000 - precision: 1.0000 - specificity_at_sensitivity: 1.0000 - 7s/epoch - 55ms/step\n",
      "Epoch 10/100\n",
      "126/126 - 7s - loss: 7.4839e-05 - accuracy: 1.0000 - precision: 1.0000 - specificity_at_sensitivity: 1.0000 - 7s/epoch - 53ms/step\n",
      "Score for fold 1: loss of 2.2778918743133545; accuracy of 68.2539701461792%; precision: 0.0\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ...\n",
      "Epoch 1/100\n",
      "126/126 - 7s - loss: 15.6906 - accuracy: 0.6349 - precision_1: 0.3429 - specificity_at_sensitivity_1: 0.0000e+00 - 7s/epoch - 57ms/step\n",
      "Epoch 2/100\n",
      "126/126 - 6s - loss: 0.2487 - accuracy: 0.9365 - precision_1: 0.9091 - specificity_at_sensitivity_1: 0.0330 - 6s/epoch - 51ms/step\n",
      "Epoch 3/100\n",
      "126/126 - 6s - loss: 0.0048 - accuracy: 1.0000 - precision_1: 1.0000 - specificity_at_sensitivity_1: 1.0000 - 6s/epoch - 51ms/step\n",
      "Epoch 4/100\n",
      "126/126 - 6s - loss: 0.0021 - accuracy: 1.0000 - precision_1: 1.0000 - specificity_at_sensitivity_1: 1.0000 - 6s/epoch - 50ms/step\n",
      "Epoch 5/100\n",
      "126/126 - 6s - loss: 2.7379e-04 - accuracy: 1.0000 - precision_1: 1.0000 - specificity_at_sensitivity_1: 1.0000 - 6s/epoch - 51ms/step\n",
      "Epoch 6/100\n",
      "126/126 - 6s - loss: 2.3214e-04 - accuracy: 1.0000 - precision_1: 1.0000 - specificity_at_sensitivity_1: 1.0000 - 6s/epoch - 52ms/step\n",
      "Epoch 7/100\n",
      "126/126 - 6s - loss: 1.6003e-04 - accuracy: 1.0000 - precision_1: 1.0000 - specificity_at_sensitivity_1: 1.0000 - 6s/epoch - 50ms/step\n",
      "Epoch 8/100\n",
      "126/126 - 6s - loss: 9.1865e-05 - accuracy: 1.0000 - precision_1: 1.0000 - specificity_at_sensitivity_1: 1.0000 - 6s/epoch - 50ms/step\n",
      "Epoch 9/100\n",
      "126/126 - 6s - loss: 6.7563e-05 - accuracy: 1.0000 - precision_1: 1.0000 - specificity_at_sensitivity_1: 1.0000 - 6s/epoch - 51ms/step\n",
      "Epoch 10/100\n",
      "126/126 - 6s - loss: 5.3639e-05 - accuracy: 1.0000 - precision_1: 1.0000 - specificity_at_sensitivity_1: 1.0000 - 6s/epoch - 51ms/step\n",
      "Epoch 11/100\n",
      "126/126 - 6s - loss: 4.3883e-05 - accuracy: 1.0000 - precision_1: 1.0000 - specificity_at_sensitivity_1: 1.0000 - 6s/epoch - 49ms/step\n",
      "Epoch 12/100\n",
      "126/126 - 6s - loss: 3.7163e-05 - accuracy: 1.0000 - precision_1: 1.0000 - specificity_at_sensitivity_1: 1.0000 - 6s/epoch - 49ms/step\n",
      "Epoch 13/100\n",
      "126/126 - 6s - loss: 3.1306e-05 - accuracy: 1.0000 - precision_1: 1.0000 - specificity_at_sensitivity_1: 1.0000 - 6s/epoch - 49ms/step\n",
      "Score for fold 2: loss of 2.920461893081665; accuracy of 66.66666865348816%; precision_1: 0.0\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 3 ...\n",
      "Epoch 1/100\n",
      "126/126 - 7s - loss: 24.6019 - accuracy: 0.6270 - precision_2: 0.4062 - specificity_at_sensitivity_2: 0.0000e+00 - 7s/epoch - 55ms/step\n",
      "Epoch 2/100\n",
      "126/126 - 6s - loss: 0.5186 - accuracy: 0.8492 - precision_2: 0.7750 - specificity_at_sensitivity_2: 0.0118 - 6s/epoch - 50ms/step\n",
      "Epoch 3/100\n",
      "126/126 - 6s - loss: 0.1494 - accuracy: 0.9524 - precision_2: 0.9268 - specificity_at_sensitivity_2: 0.0118 - 6s/epoch - 48ms/step\n",
      "Epoch 4/100\n",
      "126/126 - 6s - loss: 0.0348 - accuracy: 0.9841 - precision_2: 0.9756 - specificity_at_sensitivity_2: 0.9882 - 6s/epoch - 49ms/step\n",
      "Epoch 5/100\n",
      "126/126 - 6s - loss: 0.0164 - accuracy: 0.9921 - precision_2: 0.9762 - specificity_at_sensitivity_2: 1.0000 - 6s/epoch - 50ms/step\n",
      "Epoch 6/100\n",
      "126/126 - 6s - loss: 7.0331e-05 - accuracy: 1.0000 - precision_2: 1.0000 - specificity_at_sensitivity_2: 1.0000 - 6s/epoch - 49ms/step\n",
      "Epoch 7/100\n",
      "126/126 - 6s - loss: 5.7779e-05 - accuracy: 1.0000 - precision_2: 1.0000 - specificity_at_sensitivity_2: 1.0000 - 6s/epoch - 50ms/step\n",
      "Epoch 8/100\n",
      "126/126 - 6s - loss: 5.1428e-05 - accuracy: 1.0000 - precision_2: 1.0000 - specificity_at_sensitivity_2: 1.0000 - 6s/epoch - 49ms/step\n",
      "Epoch 9/100\n",
      "126/126 - 6s - loss: 4.7156e-05 - accuracy: 1.0000 - precision_2: 1.0000 - specificity_at_sensitivity_2: 1.0000 - 6s/epoch - 50ms/step\n",
      "Score for fold 3: loss of 1.820389986038208; accuracy of 76.1904776096344%; precision_2: 0.0\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ...\n",
      "Epoch 1/100\n",
      "141/141 - 9s - loss: 30.5221 - accuracy: 0.5674 - precision_3: 0.1667 - specificity_at_sensitivity_3: 0.0000e+00 - 9s/epoch - 63ms/step\n",
      "Epoch 2/100\n",
      "141/141 - 8s - loss: 0.4859 - accuracy: 0.9220 - precision_3: 0.8250 - specificity_at_sensitivity_3: 0.0192 - 8s/epoch - 55ms/step\n",
      "Epoch 3/100\n",
      "141/141 - 9s - loss: 0.0877 - accuracy: 0.9645 - precision_3: 0.9444 - specificity_at_sensitivity_3: 0.8365 - 9s/epoch - 62ms/step\n",
      "Epoch 4/100\n",
      "141/141 - 72s - loss: 0.0060 - accuracy: 0.9929 - precision_3: 0.9737 - specificity_at_sensitivity_3: 1.0000 - 72s/epoch - 510ms/step\n",
      "Epoch 5/100\n",
      "141/141 - 785s - loss: 7.5092e-05 - accuracy: 1.0000 - precision_3: 1.0000 - specificity_at_sensitivity_3: 1.0000 - 785s/epoch - 6s/step\n",
      "Epoch 6/100\n",
      "141/141 - 13s - loss: 5.7487e-05 - accuracy: 1.0000 - precision_3: 1.0000 - specificity_at_sensitivity_3: 1.0000 - 13s/epoch - 95ms/step\n",
      "Epoch 7/100\n",
      "141/141 - 11s - loss: 4.7236e-05 - accuracy: 1.0000 - precision_3: 1.0000 - specificity_at_sensitivity_3: 1.0000 - 11s/epoch - 79ms/step\n",
      "Epoch 8/100\n",
      "141/141 - 10s - loss: 4.2852e-05 - accuracy: 1.0000 - precision_3: 1.0000 - specificity_at_sensitivity_3: 1.0000 - 10s/epoch - 74ms/step\n",
      "Score for fold 1: loss of 3.351741075515747; accuracy of 60.41666865348816%; precision_3: 0.0\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ...\n",
      "Epoch 1/100\n",
      "142/142 - 12s - loss: 8.0315 - accuracy: 0.6197 - precision_4: 0.3721 - specificity_at_sensitivity_4: 0.0000e+00 - 12s/epoch - 82ms/step\n",
      "Epoch 2/100\n",
      "142/142 - 54s - loss: 0.2386 - accuracy: 0.9155 - precision_4: 0.8780 - specificity_at_sensitivity_4: 0.0303 - 54s/epoch - 384ms/step\n",
      "Epoch 3/100\n",
      "142/142 - 65s - loss: 0.0052 - accuracy: 1.0000 - precision_4: 1.0000 - specificity_at_sensitivity_4: 1.0000 - 65s/epoch - 457ms/step\n",
      "Epoch 4/100\n",
      "142/142 - 27s - loss: 5.4743e-04 - accuracy: 1.0000 - precision_4: 1.0000 - specificity_at_sensitivity_4: 1.0000 - 27s/epoch - 191ms/step\n",
      "Epoch 5/100\n",
      "142/142 - 14s - loss: 3.4772e-04 - accuracy: 1.0000 - precision_4: 1.0000 - specificity_at_sensitivity_4: 1.0000 - 14s/epoch - 96ms/step\n",
      "Epoch 6/100\n",
      "142/142 - 25s - loss: 2.7232e-04 - accuracy: 1.0000 - precision_4: 1.0000 - specificity_at_sensitivity_4: 1.0000 - 25s/epoch - 174ms/step\n",
      "Epoch 7/100\n",
      "142/142 - 54s - loss: 2.3139e-04 - accuracy: 1.0000 - precision_4: 1.0000 - specificity_at_sensitivity_4: 1.0000 - 54s/epoch - 383ms/step\n",
      "Epoch 8/100\n",
      "142/142 - 33s - loss: 1.9397e-04 - accuracy: 1.0000 - precision_4: 1.0000 - specificity_at_sensitivity_4: 1.0000 - 33s/epoch - 233ms/step\n",
      "Epoch 9/100\n",
      "142/142 - 16s - loss: 1.6712e-04 - accuracy: 1.0000 - precision_4: 1.0000 - specificity_at_sensitivity_4: 1.0000 - 16s/epoch - 115ms/step\n",
      "Epoch 10/100\n",
      "142/142 - 56s - loss: 1.4548e-04 - accuracy: 1.0000 - precision_4: 1.0000 - specificity_at_sensitivity_4: 1.0000 - 56s/epoch - 396ms/step\n",
      "WARNING:tensorflow:5 out of the last 9 calls to <function Model.make_test_function.<locals>.test_function at 0x000001CE81D140D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Score for fold 2: loss of 1.7698100805282593; accuracy of 72.34042286872864%; precision_4: 0.0\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 3 ...\n",
      "Epoch 1/100\n",
      "142/142 - 15s - loss: 27.0182 - accuracy: 0.5845 - precision_5: 0.3171 - specificity_at_sensitivity_5: 0.0000e+00 - 15s/epoch - 105ms/step\n",
      "Epoch 2/100\n",
      "142/142 - 14s - loss: 0.4452 - accuracy: 0.9014 - precision_5: 0.8571 - specificity_at_sensitivity_5: 0.0306 - 14s/epoch - 96ms/step\n",
      "Epoch 3/100\n",
      "142/142 - 13s - loss: 0.0096 - accuracy: 1.0000 - precision_5: 1.0000 - specificity_at_sensitivity_5: 1.0000 - 13s/epoch - 89ms/step\n",
      "Epoch 4/100\n",
      "142/142 - 56s - loss: 0.0044 - accuracy: 1.0000 - precision_5: 1.0000 - specificity_at_sensitivity_5: 1.0000 - 56s/epoch - 396ms/step\n",
      "Epoch 5/100\n",
      "142/142 - 10s - loss: 7.4711e-04 - accuracy: 1.0000 - precision_5: 1.0000 - specificity_at_sensitivity_5: 1.0000 - 10s/epoch - 73ms/step\n",
      "Epoch 6/100\n",
      "142/142 - 12s - loss: 3.5878e-04 - accuracy: 1.0000 - precision_5: 1.0000 - specificity_at_sensitivity_5: 1.0000 - 12s/epoch - 86ms/step\n",
      "Epoch 7/100\n",
      "142/142 - 22s - loss: 2.8790e-04 - accuracy: 1.0000 - precision_5: 1.0000 - specificity_at_sensitivity_5: 1.0000 - 22s/epoch - 153ms/step\n",
      "Epoch 8/100\n",
      "142/142 - 26s - loss: 2.3613e-04 - accuracy: 1.0000 - precision_5: 1.0000 - specificity_at_sensitivity_5: 1.0000 - 26s/epoch - 186ms/step\n",
      "Epoch 9/100\n",
      "142/142 - 28s - loss: 2.0802e-04 - accuracy: 1.0000 - precision_5: 1.0000 - specificity_at_sensitivity_5: 1.0000 - 28s/epoch - 195ms/step\n",
      "Epoch 10/100\n",
      "142/142 - 26s - loss: 1.8603e-04 - accuracy: 1.0000 - precision_5: 1.0000 - specificity_at_sensitivity_5: 1.0000 - 26s/epoch - 182ms/step\n",
      "Epoch 11/100\n",
      "142/142 - 21s - loss: 1.6713e-04 - accuracy: 1.0000 - precision_5: 1.0000 - specificity_at_sensitivity_5: 1.0000 - 21s/epoch - 146ms/step\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x000001CE804EA4D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Score for fold 3: loss of 1.3832131624221802; accuracy of 74.46808218955994%; precision_5: 0.0\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 4 ...\n",
      "Epoch 1/100\n",
      "142/142 - 12s - loss: 42.9924 - accuracy: 0.5775 - precision_6: 0.3261 - specificity_at_sensitivity_6: 0.0000e+00 - 12s/epoch - 81ms/step\n",
      "Epoch 2/100\n",
      "142/142 - 10s - loss: 0.2468 - accuracy: 0.9296 - precision_6: 0.9250 - specificity_at_sensitivity_6: 0.7959 - 10s/epoch - 74ms/step\n",
      "Epoch 3/100\n",
      "142/142 - 11s - loss: 0.1144 - accuracy: 0.9577 - precision_6: 0.9318 - specificity_at_sensitivity_6: 0.8878 - 11s/epoch - 79ms/step\n",
      "Epoch 4/100\n",
      "142/142 - 36s - loss: 0.1200 - accuracy: 0.9366 - precision_6: 0.8723 - specificity_at_sensitivity_6: 0.9082 - 36s/epoch - 257ms/step\n",
      "Epoch 5/100\n",
      "142/142 - 11s - loss: 0.0027 - accuracy: 1.0000 - precision_6: 1.0000 - specificity_at_sensitivity_6: 1.0000 - 11s/epoch - 79ms/step\n",
      "Epoch 6/100\n",
      "142/142 - 12s - loss: 6.9853e-04 - accuracy: 1.0000 - precision_6: 1.0000 - specificity_at_sensitivity_6: 1.0000 - 12s/epoch - 84ms/step\n",
      "Epoch 7/100\n",
      "142/142 - 139s - loss: 4.9526e-04 - accuracy: 1.0000 - precision_6: 1.0000 - specificity_at_sensitivity_6: 1.0000 - 139s/epoch - 981ms/step\n",
      "Epoch 8/100\n",
      "142/142 - 84s - loss: 4.0294e-04 - accuracy: 1.0000 - precision_6: 1.0000 - specificity_at_sensitivity_6: 1.0000 - 84s/epoch - 591ms/step\n",
      "Epoch 9/100\n",
      "142/142 - 16s - loss: 3.4361e-04 - accuracy: 1.0000 - precision_6: 1.0000 - specificity_at_sensitivity_6: 1.0000 - 16s/epoch - 111ms/step\n",
      "Epoch 10/100\n",
      "142/142 - 13s - loss: 2.9526e-04 - accuracy: 1.0000 - precision_6: 1.0000 - specificity_at_sensitivity_6: 1.0000 - 13s/epoch - 93ms/step\n",
      "Epoch 11/100\n",
      "142/142 - 11s - loss: 2.6208e-04 - accuracy: 1.0000 - precision_6: 1.0000 - specificity_at_sensitivity_6: 1.0000 - 11s/epoch - 76ms/step\n",
      "Epoch 12/100\n",
      "142/142 - 9s - loss: 2.3623e-04 - accuracy: 1.0000 - precision_6: 1.0000 - specificity_at_sensitivity_6: 1.0000 - 9s/epoch - 61ms/step\n",
      "Epoch 13/100\n",
      "142/142 - 8s - loss: 2.1369e-04 - accuracy: 1.0000 - precision_6: 1.0000 - specificity_at_sensitivity_6: 1.0000 - 8s/epoch - 57ms/step\n",
      "Epoch 14/100\n",
      "142/142 - 8s - loss: 1.9451e-04 - accuracy: 1.0000 - precision_6: 1.0000 - specificity_at_sensitivity_6: 1.0000 - 8s/epoch - 57ms/step\n",
      "Epoch 15/100\n",
      "142/142 - 8s - loss: 1.8037e-04 - accuracy: 1.0000 - precision_6: 1.0000 - specificity_at_sensitivity_6: 1.0000 - 8s/epoch - 57ms/step\n",
      "Score for fold 4: loss of 1.423921823501587; accuracy of 74.46808218955994%; precision_6: 0.0\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ...\n",
      "Epoch 1/100\n",
      "151/151 - 10s - loss: 23.2849 - accuracy: 0.5762 - precision_7: 0.3269 - specificity_at_sensitivity_7: 0.0000e+00 - 10s/epoch - 63ms/step\n",
      "Epoch 2/100\n",
      "151/151 - 9s - loss: 0.6279 - accuracy: 0.8742 - precision_7: 0.8000 - specificity_at_sensitivity_7: 0.0000e+00 - 9s/epoch - 60ms/step\n",
      "Epoch 3/100\n",
      "151/151 - 9s - loss: 0.0964 - accuracy: 0.9669 - precision_7: 0.9556 - specificity_at_sensitivity_7: 0.9143 - 9s/epoch - 57ms/step\n",
      "Epoch 4/100\n",
      "151/151 - 10s - loss: 0.0162 - accuracy: 0.9934 - precision_7: 1.0000 - specificity_at_sensitivity_7: 1.0000 - 10s/epoch - 63ms/step\n",
      "Epoch 5/100\n",
      "151/151 - 10s - loss: 5.2514e-04 - accuracy: 1.0000 - precision_7: 1.0000 - specificity_at_sensitivity_7: 1.0000 - 10s/epoch - 67ms/step\n",
      "Epoch 6/100\n",
      "151/151 - 9s - loss: 2.8992e-04 - accuracy: 1.0000 - precision_7: 1.0000 - specificity_at_sensitivity_7: 1.0000 - 9s/epoch - 57ms/step\n",
      "Epoch 7/100\n",
      "151/151 - 9s - loss: 2.2539e-04 - accuracy: 1.0000 - precision_7: 1.0000 - specificity_at_sensitivity_7: 1.0000 - 9s/epoch - 57ms/step\n",
      "Epoch 8/100\n",
      "151/151 - 9s - loss: 1.8483e-04 - accuracy: 1.0000 - precision_7: 1.0000 - specificity_at_sensitivity_7: 1.0000 - 9s/epoch - 57ms/step\n",
      "Epoch 9/100\n",
      "151/151 - 9s - loss: 1.5692e-04 - accuracy: 1.0000 - precision_7: 1.0000 - specificity_at_sensitivity_7: 1.0000 - 9s/epoch - 62ms/step\n",
      "Epoch 10/100\n",
      "151/151 - 9s - loss: 1.3346e-04 - accuracy: 1.0000 - precision_7: 1.0000 - specificity_at_sensitivity_7: 1.0000 - 9s/epoch - 57ms/step\n",
      "Epoch 11/100\n",
      "151/151 - 9s - loss: 1.1576e-04 - accuracy: 1.0000 - precision_7: 1.0000 - specificity_at_sensitivity_7: 1.0000 - 9s/epoch - 63ms/step\n",
      "Score for fold 1: loss of 1.723726749420166; accuracy of 76.31579041481018%; precision_7: 1.0\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ...\n",
      "Epoch 1/100\n",
      "151/151 - 9s - loss: 26.7764 - accuracy: 0.5695 - precision_8: 0.2889 - specificity_at_sensitivity_8: 0.0000e+00 - 9s/epoch - 62ms/step\n",
      "Epoch 2/100\n",
      "151/151 - 9s - loss: 0.2501 - accuracy: 0.9139 - precision_8: 0.8837 - specificity_at_sensitivity_8: 0.0000e+00 - 9s/epoch - 57ms/step\n",
      "Epoch 3/100\n",
      "151/151 - 9s - loss: 0.0126 - accuracy: 0.9934 - precision_8: 1.0000 - specificity_at_sensitivity_8: 1.0000 - 9s/epoch - 57ms/step\n",
      "Epoch 4/100\n",
      "151/151 - 9s - loss: 0.0010 - accuracy: 1.0000 - precision_8: 1.0000 - specificity_at_sensitivity_8: 1.0000 - 9s/epoch - 57ms/step\n",
      "Epoch 5/100\n",
      "151/151 - 9s - loss: 4.9236e-04 - accuracy: 1.0000 - precision_8: 1.0000 - specificity_at_sensitivity_8: 1.0000 - 9s/epoch - 57ms/step\n",
      "Epoch 6/100\n",
      "151/151 - 9s - loss: 3.9064e-04 - accuracy: 1.0000 - precision_8: 1.0000 - specificity_at_sensitivity_8: 1.0000 - 9s/epoch - 57ms/step\n",
      "Epoch 7/100\n",
      "151/151 - 9s - loss: 3.1488e-04 - accuracy: 1.0000 - precision_8: 1.0000 - specificity_at_sensitivity_8: 1.0000 - 9s/epoch - 58ms/step\n",
      "Epoch 8/100\n",
      "151/151 - 9s - loss: 2.7204e-04 - accuracy: 1.0000 - precision_8: 1.0000 - specificity_at_sensitivity_8: 1.0000 - 9s/epoch - 57ms/step\n",
      "Epoch 9/100\n",
      "151/151 - 9s - loss: 2.3596e-04 - accuracy: 1.0000 - precision_8: 1.0000 - specificity_at_sensitivity_8: 1.0000 - 9s/epoch - 59ms/step\n",
      "Epoch 10/100\n",
      "151/151 - 10s - loss: 2.0853e-04 - accuracy: 1.0000 - precision_8: 1.0000 - specificity_at_sensitivity_8: 1.0000 - 10s/epoch - 68ms/step\n",
      "Epoch 11/100\n",
      "151/151 - 10s - loss: 1.8696e-04 - accuracy: 1.0000 - precision_8: 1.0000 - specificity_at_sensitivity_8: 1.0000 - 10s/epoch - 66ms/step\n",
      "Score for fold 2: loss of 1.308738112449646; accuracy of 73.68420958518982%; precision_8: 0.0\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 3 ...\n",
      "Epoch 1/100\n",
      "151/151 - 7s - loss: 42.5160 - accuracy: 0.6623 - precision_9: 0.3333 - specificity_at_sensitivity_9: 0.0000e+00 - 7s/epoch - 47ms/step\n",
      "Epoch 2/100\n",
      "151/151 - 6s - loss: 0.3156 - accuracy: 0.9139 - precision_9: 0.8684 - specificity_at_sensitivity_9: 0.0000e+00 - 6s/epoch - 39ms/step\n",
      "Epoch 3/100\n",
      "151/151 - 6s - loss: 0.0371 - accuracy: 0.9934 - precision_9: 0.9762 - specificity_at_sensitivity_9: 0.9909 - 6s/epoch - 39ms/step\n",
      "Epoch 4/100\n",
      "151/151 - 6s - loss: 0.0062 - accuracy: 1.0000 - precision_9: 1.0000 - specificity_at_sensitivity_9: 1.0000 - 6s/epoch - 39ms/step\n",
      "Epoch 5/100\n",
      "151/151 - 6s - loss: 0.0016 - accuracy: 1.0000 - precision_9: 1.0000 - specificity_at_sensitivity_9: 1.0000 - 6s/epoch - 39ms/step\n",
      "Epoch 6/100\n",
      "151/151 - 6s - loss: 9.5691e-04 - accuracy: 1.0000 - precision_9: 1.0000 - specificity_at_sensitivity_9: 1.0000 - 6s/epoch - 39ms/step\n",
      "Epoch 7/100\n",
      "151/151 - 7s - loss: 7.9129e-04 - accuracy: 1.0000 - precision_9: 1.0000 - specificity_at_sensitivity_9: 1.0000 - 7s/epoch - 45ms/step\n",
      "Epoch 8/100\n",
      "151/151 - 7s - loss: 6.9040e-04 - accuracy: 1.0000 - precision_9: 1.0000 - specificity_at_sensitivity_9: 1.0000 - 7s/epoch - 49ms/step\n",
      "Epoch 9/100\n",
      "151/151 - 6s - loss: 6.0752e-04 - accuracy: 1.0000 - precision_9: 1.0000 - specificity_at_sensitivity_9: 1.0000 - 6s/epoch - 39ms/step\n",
      "Epoch 10/100\n",
      "151/151 - 6s - loss: 5.3742e-04 - accuracy: 1.0000 - precision_9: 1.0000 - specificity_at_sensitivity_9: 1.0000 - 6s/epoch - 40ms/step\n",
      "Epoch 11/100\n",
      "151/151 - 6s - loss: 4.7951e-04 - accuracy: 1.0000 - precision_9: 1.0000 - specificity_at_sensitivity_9: 1.0000 - 6s/epoch - 39ms/step\n",
      "Epoch 12/100\n",
      "151/151 - 6s - loss: 4.3499e-04 - accuracy: 1.0000 - precision_9: 1.0000 - specificity_at_sensitivity_9: 1.0000 - 6s/epoch - 39ms/step\n",
      "Epoch 13/100\n",
      "151/151 - 6s - loss: 3.8950e-04 - accuracy: 1.0000 - precision_9: 1.0000 - specificity_at_sensitivity_9: 1.0000 - 6s/epoch - 39ms/step\n",
      "Epoch 14/100\n",
      "151/151 - 6s - loss: 3.5727e-04 - accuracy: 1.0000 - precision_9: 1.0000 - specificity_at_sensitivity_9: 1.0000 - 6s/epoch - 39ms/step\n",
      "Epoch 15/100\n",
      "151/151 - 6s - loss: 3.2944e-04 - accuracy: 1.0000 - precision_9: 1.0000 - specificity_at_sensitivity_9: 1.0000 - 6s/epoch - 41ms/step\n",
      "Epoch 16/100\n",
      "151/151 - 9s - loss: 2.9941e-04 - accuracy: 1.0000 - precision_9: 1.0000 - specificity_at_sensitivity_9: 1.0000 - 9s/epoch - 63ms/step\n",
      "Epoch 17/100\n",
      "151/151 - 9s - loss: 2.7534e-04 - accuracy: 1.0000 - precision_9: 1.0000 - specificity_at_sensitivity_9: 1.0000 - 9s/epoch - 57ms/step\n",
      "Epoch 18/100\n",
      "151/151 - 9s - loss: 2.5225e-04 - accuracy: 1.0000 - precision_9: 1.0000 - specificity_at_sensitivity_9: 1.0000 - 9s/epoch - 57ms/step\n",
      "Score for fold 3: loss of 2.299917221069336; accuracy of 60.52631735801697%; precision_9: 0.0\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 4 ...\n",
      "Epoch 1/100\n",
      "151/151 - 8s - loss: 12.9019 - accuracy: 0.5828 - precision_10: 0.2500 - specificity_at_sensitivity_10: 0.0000e+00 - 8s/epoch - 53ms/step\n",
      "Epoch 2/100\n",
      "151/151 - 6s - loss: 0.0733 - accuracy: 0.9735 - precision_10: 0.9756 - specificity_at_sensitivity_10: 0.9352 - 6s/epoch - 39ms/step\n",
      "Epoch 3/100\n",
      "151/151 - 6s - loss: 0.0066 - accuracy: 1.0000 - precision_10: 1.0000 - specificity_at_sensitivity_10: 1.0000 - 6s/epoch - 39ms/step\n",
      "Epoch 4/100\n",
      "151/151 - 6s - loss: 0.0016 - accuracy: 1.0000 - precision_10: 1.0000 - specificity_at_sensitivity_10: 1.0000 - 6s/epoch - 39ms/step\n",
      "Epoch 5/100\n",
      "151/151 - 8s - loss: 7.9947e-04 - accuracy: 1.0000 - precision_10: 1.0000 - specificity_at_sensitivity_10: 1.0000 - 8s/epoch - 50ms/step\n",
      "Epoch 6/100\n",
      "151/151 - 7s - loss: 4.2830e-04 - accuracy: 1.0000 - precision_10: 1.0000 - specificity_at_sensitivity_10: 1.0000 - 7s/epoch - 44ms/step\n",
      "Epoch 7/100\n",
      "151/151 - 6s - loss: 3.4754e-04 - accuracy: 1.0000 - precision_10: 1.0000 - specificity_at_sensitivity_10: 1.0000 - 6s/epoch - 40ms/step\n",
      "Epoch 8/100\n",
      "151/151 - 9s - loss: 2.8681e-04 - accuracy: 1.0000 - precision_10: 1.0000 - specificity_at_sensitivity_10: 1.0000 - 9s/epoch - 62ms/step\n",
      "Epoch 9/100\n",
      "151/151 - 10s - loss: 2.4610e-04 - accuracy: 1.0000 - precision_10: 1.0000 - specificity_at_sensitivity_10: 1.0000 - 10s/epoch - 66ms/step\n",
      "Epoch 10/100\n",
      "151/151 - 9s - loss: 2.1460e-04 - accuracy: 1.0000 - precision_10: 1.0000 - specificity_at_sensitivity_10: 1.0000 - 9s/epoch - 62ms/step\n",
      "Epoch 11/100\n",
      "151/151 - 10s - loss: 1.8947e-04 - accuracy: 1.0000 - precision_10: 1.0000 - specificity_at_sensitivity_10: 1.0000 - 10s/epoch - 66ms/step\n",
      "Score for fold 4: loss of 2.008481740951538; accuracy of 65.78947305679321%; precision_10: 0.0\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 5 ...\n",
      "Epoch 1/100\n",
      "152/152 - 11s - loss: 14.8815 - accuracy: 0.5658 - precision_11: 0.3125 - specificity_at_sensitivity_11: 0.0000e+00 - 11s/epoch - 72ms/step\n",
      "Epoch 2/100\n",
      "152/152 - 10s - loss: 0.3268 - accuracy: 0.9211 - precision_11: 0.8750 - specificity_at_sensitivity_11: 0.0000e+00 - 10s/epoch - 65ms/step\n",
      "Epoch 3/100\n",
      "152/152 - 10s - loss: 0.0097 - accuracy: 0.9934 - precision_11: 0.9796 - specificity_at_sensitivity_11: 1.0000 - 10s/epoch - 67ms/step\n",
      "Epoch 4/100\n",
      "152/152 - 10s - loss: 6.8251e-04 - accuracy: 1.0000 - precision_11: 1.0000 - specificity_at_sensitivity_11: 1.0000 - 10s/epoch - 68ms/step\n",
      "Epoch 5/100\n",
      "152/152 - 10s - loss: 4.6072e-04 - accuracy: 1.0000 - precision_11: 1.0000 - specificity_at_sensitivity_11: 1.0000 - 10s/epoch - 67ms/step\n",
      "Epoch 6/100\n",
      "152/152 - 9s - loss: 3.8941e-04 - accuracy: 1.0000 - precision_11: 1.0000 - specificity_at_sensitivity_11: 1.0000 - 9s/epoch - 56ms/step\n",
      "Epoch 7/100\n",
      "152/152 - 6s - loss: 3.2885e-04 - accuracy: 1.0000 - precision_11: 1.0000 - specificity_at_sensitivity_11: 1.0000 - 6s/epoch - 39ms/step\n",
      "Epoch 8/100\n",
      "152/152 - 8s - loss: 2.8344e-04 - accuracy: 1.0000 - precision_11: 1.0000 - specificity_at_sensitivity_11: 1.0000 - 8s/epoch - 54ms/step\n",
      "Epoch 9/100\n",
      "152/152 - 6s - loss: 2.5102e-04 - accuracy: 1.0000 - precision_11: 1.0000 - specificity_at_sensitivity_11: 1.0000 - 6s/epoch - 39ms/step\n",
      "Epoch 10/100\n",
      "152/152 - 8s - loss: 2.2499e-04 - accuracy: 1.0000 - precision_11: 1.0000 - specificity_at_sensitivity_11: 1.0000 - 8s/epoch - 50ms/step\n",
      "Epoch 11/100\n",
      "152/152 - 7s - loss: 1.9763e-04 - accuracy: 1.0000 - precision_11: 1.0000 - specificity_at_sensitivity_11: 1.0000 - 7s/epoch - 43ms/step\n",
      "Epoch 12/100\n",
      "152/152 - 6s - loss: 1.7644e-04 - accuracy: 1.0000 - precision_11: 1.0000 - specificity_at_sensitivity_11: 1.0000 - 6s/epoch - 39ms/step\n",
      "Epoch 13/100\n",
      "152/152 - 7s - loss: 1.5901e-04 - accuracy: 1.0000 - precision_11: 1.0000 - specificity_at_sensitivity_11: 1.0000 - 7s/epoch - 47ms/step\n",
      "Score for fold 5: loss of 1.238969087600708; accuracy of 78.37837934494019%; precision_11: 0.0\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ...\n",
      "Epoch 1/100\n",
      "157/157 - 11s - loss: 10.7353 - accuracy: 0.6242 - precision_12: 0.3243 - specificity_at_sensitivity_12: 0.0000e+00 - 11s/epoch - 72ms/step\n",
      "Epoch 2/100\n",
      "157/157 - 7s - loss: 0.3762 - accuracy: 0.8981 - precision_12: 0.8571 - specificity_at_sensitivity_12: 0.0090 - 7s/epoch - 42ms/step\n",
      "Epoch 3/100\n",
      "157/157 - 6s - loss: 0.0350 - accuracy: 0.9873 - precision_12: 0.9783 - specificity_at_sensitivity_12: 0.9910 - 6s/epoch - 39ms/step\n",
      "Epoch 4/100\n",
      "157/157 - 6s - loss: 0.0028 - accuracy: 1.0000 - precision_12: 1.0000 - specificity_at_sensitivity_12: 1.0000 - 6s/epoch - 39ms/step\n",
      "Epoch 5/100\n",
      "157/157 - 6s - loss: 0.0012 - accuracy: 1.0000 - precision_12: 1.0000 - specificity_at_sensitivity_12: 1.0000 - 6s/epoch - 39ms/step\n",
      "Epoch 6/100\n",
      "157/157 - 6s - loss: 7.5750e-04 - accuracy: 1.0000 - precision_12: 1.0000 - specificity_at_sensitivity_12: 1.0000 - 6s/epoch - 39ms/step\n",
      "Epoch 7/100\n",
      "157/157 - 6s - loss: 5.9611e-04 - accuracy: 1.0000 - precision_12: 1.0000 - specificity_at_sensitivity_12: 1.0000 - 6s/epoch - 39ms/step\n",
      "Epoch 8/100\n",
      "157/157 - 8s - loss: 4.9360e-04 - accuracy: 1.0000 - precision_12: 1.0000 - specificity_at_sensitivity_12: 1.0000 - 8s/epoch - 49ms/step\n",
      "Epoch 9/100\n",
      "157/157 - 8s - loss: 4.1732e-04 - accuracy: 1.0000 - precision_12: 1.0000 - specificity_at_sensitivity_12: 1.0000 - 8s/epoch - 53ms/step\n",
      "Epoch 10/100\n",
      "157/157 - 9s - loss: 3.5612e-04 - accuracy: 1.0000 - precision_12: 1.0000 - specificity_at_sensitivity_12: 1.0000 - 9s/epoch - 55ms/step\n",
      "Epoch 11/100\n",
      "157/157 - 10s - loss: 3.0729e-04 - accuracy: 1.0000 - precision_12: 1.0000 - specificity_at_sensitivity_12: 1.0000 - 10s/epoch - 65ms/step\n",
      "Epoch 12/100\n",
      "157/157 - 10s - loss: 2.6677e-04 - accuracy: 1.0000 - precision_12: 1.0000 - specificity_at_sensitivity_12: 1.0000 - 10s/epoch - 65ms/step\n",
      "Epoch 13/100\n",
      "157/157 - 10s - loss: 2.3822e-04 - accuracy: 1.0000 - precision_12: 1.0000 - specificity_at_sensitivity_12: 1.0000 - 10s/epoch - 67ms/step\n",
      "Epoch 14/100\n",
      "157/157 - 10s - loss: 2.0801e-04 - accuracy: 1.0000 - precision_12: 1.0000 - specificity_at_sensitivity_12: 1.0000 - 10s/epoch - 64ms/step\n",
      "Epoch 15/100\n",
      "157/157 - 9s - loss: 1.8281e-04 - accuracy: 1.0000 - precision_12: 1.0000 - specificity_at_sensitivity_12: 1.0000 - 9s/epoch - 57ms/step\n",
      "Epoch 16/100\n",
      "157/157 - 11s - loss: 1.6350e-04 - accuracy: 1.0000 - precision_12: 1.0000 - specificity_at_sensitivity_12: 1.0000 - 11s/epoch - 67ms/step\n",
      "Score for fold 1: loss of 2.1160316467285156; accuracy of 68.75%; precision_12: 0.0\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ...\n",
      "Epoch 1/100\n",
      "157/157 - 11s - loss: 16.7854 - accuracy: 0.5287 - precision_13: 0.1957 - specificity_at_sensitivity_13: 0.0000e+00 - 11s/epoch - 72ms/step\n",
      "Epoch 2/100\n",
      "157/157 - 10s - loss: 0.2816 - accuracy: 0.9427 - precision_13: 0.9111 - specificity_at_sensitivity_13: 0.0180 - 10s/epoch - 64ms/step\n",
      "Epoch 3/100\n",
      "157/157 - 10s - loss: 0.1377 - accuracy: 0.9618 - precision_13: 0.9167 - specificity_at_sensitivity_13: 0.0180 - 10s/epoch - 64ms/step\n",
      "Epoch 4/100\n",
      "157/157 - 10s - loss: 0.0482 - accuracy: 0.9809 - precision_13: 0.9778 - specificity_at_sensitivity_13: 0.9820 - 10s/epoch - 65ms/step\n",
      "Epoch 5/100\n",
      "157/157 - 10s - loss: 3.4518e-05 - accuracy: 1.0000 - precision_13: 1.0000 - specificity_at_sensitivity_13: 1.0000 - 10s/epoch - 65ms/step\n",
      "Epoch 6/100\n",
      "157/157 - 10s - loss: 1.8861e-05 - accuracy: 1.0000 - precision_13: 1.0000 - specificity_at_sensitivity_13: 1.0000 - 10s/epoch - 66ms/step\n",
      "Epoch 7/100\n",
      "157/157 - 10s - loss: 1.3108e-05 - accuracy: 1.0000 - precision_13: 1.0000 - specificity_at_sensitivity_13: 1.0000 - 10s/epoch - 67ms/step\n",
      "Epoch 8/100\n",
      "157/157 - 8s - loss: 9.8399e-06 - accuracy: 1.0000 - precision_13: 1.0000 - specificity_at_sensitivity_13: 1.0000 - 8s/epoch - 51ms/step\n",
      "Score for fold 2: loss of 5.118303298950195; accuracy of 68.75%; precision_13: 0.0\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 3 ...\n",
      "Epoch 1/100\n",
      "157/157 - 6s - loss: 33.7788 - accuracy: 0.5924 - precision_14: 0.3265 - specificity_at_sensitivity_14: 0.0000e+00 - 6s/epoch - 41ms/step\n",
      "Epoch 2/100\n",
      "157/157 - 10s - loss: 0.4319 - accuracy: 0.9045 - precision_14: 0.8478 - specificity_at_sensitivity_14: 0.0182 - 10s/epoch - 66ms/step\n",
      "Epoch 3/100\n",
      "157/157 - 10s - loss: 0.0590 - accuracy: 0.9809 - precision_14: 0.9783 - specificity_at_sensitivity_14: 0.8818 - 10s/epoch - 66ms/step\n",
      "Epoch 4/100\n",
      "157/157 - 10s - loss: 0.0156 - accuracy: 0.9936 - precision_14: 0.9792 - specificity_at_sensitivity_14: 0.9909 - 10s/epoch - 64ms/step\n",
      "Epoch 5/100\n",
      "157/157 - 10s - loss: 5.3667e-04 - accuracy: 1.0000 - precision_14: 1.0000 - specificity_at_sensitivity_14: 1.0000 - 10s/epoch - 61ms/step\n",
      "Epoch 6/100\n",
      "157/157 - 9s - loss: 1.9160e-04 - accuracy: 1.0000 - precision_14: 1.0000 - specificity_at_sensitivity_14: 1.0000 - 9s/epoch - 60ms/step\n",
      "Epoch 7/100\n",
      "157/157 - 9s - loss: 1.6235e-04 - accuracy: 1.0000 - precision_14: 1.0000 - specificity_at_sensitivity_14: 1.0000 - 9s/epoch - 60ms/step\n",
      "Epoch 8/100\n",
      "157/157 - 10s - loss: 1.3295e-04 - accuracy: 1.0000 - precision_14: 1.0000 - specificity_at_sensitivity_14: 1.0000 - 10s/epoch - 65ms/step\n",
      "Epoch 9/100\n",
      "157/157 - 10s - loss: 1.1363e-04 - accuracy: 1.0000 - precision_14: 1.0000 - specificity_at_sensitivity_14: 1.0000 - 10s/epoch - 67ms/step\n",
      "Score for fold 3: loss of 2.088351249694824; accuracy of 71.875%; precision_14: 0.0\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 4 ...\n",
      "Epoch 1/100\n",
      "158/158 - 11s - loss: 52.5988 - accuracy: 0.5633 - precision_15: 0.2653 - specificity_at_sensitivity_15: 0.0000e+00 - 11s/epoch - 73ms/step\n",
      "Epoch 2/100\n",
      "158/158 - 10s - loss: 0.2650 - accuracy: 0.9241 - precision_15: 0.8696 - specificity_at_sensitivity_15: 0.0089 - 10s/epoch - 65ms/step\n",
      "Epoch 3/100\n",
      "158/158 - 10s - loss: 0.0780 - accuracy: 0.9747 - precision_15: 0.9565 - specificity_at_sensitivity_15: 0.8929 - 10s/epoch - 64ms/step\n",
      "Epoch 4/100\n",
      "158/158 - 10s - loss: 0.0025 - accuracy: 1.0000 - precision_15: 1.0000 - specificity_at_sensitivity_15: 1.0000 - 10s/epoch - 62ms/step\n",
      "Epoch 5/100\n",
      "158/158 - 10s - loss: 3.8273e-04 - accuracy: 1.0000 - precision_15: 1.0000 - specificity_at_sensitivity_15: 1.0000 - 10s/epoch - 65ms/step\n",
      "Epoch 6/100\n",
      "158/158 - 11s - loss: 2.5405e-04 - accuracy: 1.0000 - precision_15: 1.0000 - specificity_at_sensitivity_15: 1.0000 - 11s/epoch - 67ms/step\n",
      "Epoch 7/100\n",
      "158/158 - 7s - loss: 2.1058e-04 - accuracy: 1.0000 - precision_15: 1.0000 - specificity_at_sensitivity_15: 1.0000 - 7s/epoch - 47ms/step\n",
      "Epoch 8/100\n",
      "158/158 - 5s - loss: 1.8544e-04 - accuracy: 1.0000 - precision_15: 1.0000 - specificity_at_sensitivity_15: 1.0000 - 5s/epoch - 34ms/step\n",
      "Epoch 9/100\n",
      "158/158 - 5s - loss: 1.6476e-04 - accuracy: 1.0000 - precision_15: 1.0000 - specificity_at_sensitivity_15: 1.0000 - 5s/epoch - 34ms/step\n",
      "Score for fold 4: loss of 2.6571156978607178; accuracy of 64.51612710952759%; precision_15: 0.0\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 5 ...\n",
      "Epoch 1/100\n",
      "158/158 - 7s - loss: 17.4271 - accuracy: 0.6076 - precision_16: 0.2821 - specificity_at_sensitivity_16: 0.0000e+00 - 7s/epoch - 41ms/step\n",
      "Epoch 2/100\n",
      "158/158 - 5s - loss: 0.3819 - accuracy: 0.8924 - precision_16: 0.8182 - specificity_at_sensitivity_16: 0.0000e+00 - 5s/epoch - 34ms/step\n",
      "Epoch 3/100\n",
      "158/158 - 5s - loss: 0.0208 - accuracy: 0.9937 - precision_16: 1.0000 - specificity_at_sensitivity_16: 1.0000 - 5s/epoch - 33ms/step\n",
      "Epoch 4/100\n",
      "158/158 - 5s - loss: 0.0012 - accuracy: 1.0000 - precision_16: 1.0000 - specificity_at_sensitivity_16: 1.0000 - 5s/epoch - 33ms/step\n",
      "Epoch 5/100\n",
      "158/158 - 5s - loss: 8.3409e-04 - accuracy: 1.0000 - precision_16: 1.0000 - specificity_at_sensitivity_16: 1.0000 - 5s/epoch - 34ms/step\n",
      "Epoch 6/100\n",
      "158/158 - 5s - loss: 6.5113e-04 - accuracy: 1.0000 - precision_16: 1.0000 - specificity_at_sensitivity_16: 1.0000 - 5s/epoch - 33ms/step\n",
      "Epoch 7/100\n",
      "158/158 - 5s - loss: 5.2828e-04 - accuracy: 1.0000 - precision_16: 1.0000 - specificity_at_sensitivity_16: 1.0000 - 5s/epoch - 33ms/step\n",
      "Epoch 8/100\n",
      "158/158 - 5s - loss: 4.5401e-04 - accuracy: 1.0000 - precision_16: 1.0000 - specificity_at_sensitivity_16: 1.0000 - 5s/epoch - 34ms/step\n",
      "Epoch 9/100\n",
      "158/158 - 5s - loss: 3.8517e-04 - accuracy: 1.0000 - precision_16: 1.0000 - specificity_at_sensitivity_16: 1.0000 - 5s/epoch - 33ms/step\n",
      "Epoch 10/100\n",
      "158/158 - 5s - loss: 3.3678e-04 - accuracy: 1.0000 - precision_16: 1.0000 - specificity_at_sensitivity_16: 1.0000 - 5s/epoch - 33ms/step\n",
      "Epoch 11/100\n",
      "158/158 - 5s - loss: 2.9709e-04 - accuracy: 1.0000 - precision_16: 1.0000 - specificity_at_sensitivity_16: 1.0000 - 5s/epoch - 33ms/step\n",
      "Epoch 12/100\n",
      "158/158 - 5s - loss: 2.6222e-04 - accuracy: 1.0000 - precision_16: 1.0000 - specificity_at_sensitivity_16: 1.0000 - 5s/epoch - 34ms/step\n",
      "Epoch 13/100\n",
      "158/158 - 5s - loss: 2.3332e-04 - accuracy: 1.0000 - precision_16: 1.0000 - specificity_at_sensitivity_16: 1.0000 - 5s/epoch - 33ms/step\n",
      "Epoch 14/100\n",
      "158/158 - 5s - loss: 2.1015e-04 - accuracy: 1.0000 - precision_16: 1.0000 - specificity_at_sensitivity_16: 1.0000 - 5s/epoch - 33ms/step\n",
      "Epoch 15/100\n",
      "158/158 - 5s - loss: 1.8951e-04 - accuracy: 1.0000 - precision_16: 1.0000 - specificity_at_sensitivity_16: 1.0000 - 5s/epoch - 33ms/step\n",
      "Score for fold 5: loss of 2.167332410812378; accuracy of 64.51612710952759%; precision_16: 0.0\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 6 ...\n",
      "Epoch 1/100\n",
      "158/158 - 6s - loss: 7.6867 - accuracy: 0.5380 - precision_17: 0.2653 - specificity_at_sensitivity_17: 0.0000e+00 - 6s/epoch - 39ms/step\n",
      "Epoch 2/100\n",
      "158/158 - 5s - loss: 0.2375 - accuracy: 0.9177 - precision_17: 0.8776 - specificity_at_sensitivity_17: 0.0093 - 5s/epoch - 33ms/step\n",
      "Epoch 3/100\n",
      "158/158 - 5s - loss: 0.0386 - accuracy: 0.9873 - precision_17: 0.9800 - specificity_at_sensitivity_17: 0.9444 - 5s/epoch - 34ms/step\n",
      "Epoch 4/100\n",
      "158/158 - 5s - loss: 0.0111 - accuracy: 0.9937 - precision_17: 1.0000 - specificity_at_sensitivity_17: 1.0000 - 5s/epoch - 33ms/step\n",
      "Epoch 5/100\n",
      "158/158 - 5s - loss: 0.0017 - accuracy: 1.0000 - precision_17: 1.0000 - specificity_at_sensitivity_17: 1.0000 - 5s/epoch - 33ms/step\n",
      "Epoch 6/100\n",
      "158/158 - 5s - loss: 5.4429e-05 - accuracy: 1.0000 - precision_17: 1.0000 - specificity_at_sensitivity_17: 1.0000 - 5s/epoch - 34ms/step\n",
      "Epoch 7/100\n",
      "158/158 - 5s - loss: 3.9085e-05 - accuracy: 1.0000 - precision_17: 1.0000 - specificity_at_sensitivity_17: 1.0000 - 5s/epoch - 33ms/step\n",
      "Epoch 8/100\n",
      "158/158 - 5s - loss: 3.3015e-05 - accuracy: 1.0000 - precision_17: 1.0000 - specificity_at_sensitivity_17: 1.0000 - 5s/epoch - 33ms/step\n",
      "Epoch 9/100\n",
      "158/158 - 5s - loss: 2.8195e-05 - accuracy: 1.0000 - precision_17: 1.0000 - specificity_at_sensitivity_17: 1.0000 - 5s/epoch - 33ms/step\n",
      "Score for fold 6: loss of 1.3159866333007812; accuracy of 77.4193525314331%; precision_17: 0.0\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ...\n",
      "Epoch 1/100\n",
      "162/162 - 6s - loss: 32.5733 - accuracy: 0.6173 - precision_18: 0.3171 - specificity_at_sensitivity_18: 0.0000e+00 - 6s/epoch - 39ms/step\n",
      "Epoch 2/100\n",
      "162/162 - 5s - loss: 0.3321 - accuracy: 0.9321 - precision_18: 0.8913 - specificity_at_sensitivity_18: 0.0174 - 5s/epoch - 34ms/step\n",
      "Epoch 3/100\n",
      "162/162 - 5s - loss: 0.0315 - accuracy: 0.9877 - precision_18: 1.0000 - specificity_at_sensitivity_18: 0.9652 - 5s/epoch - 33ms/step\n",
      "Epoch 4/100\n",
      "162/162 - 5s - loss: 0.0021 - accuracy: 1.0000 - precision_18: 1.0000 - specificity_at_sensitivity_18: 1.0000 - 5s/epoch - 34ms/step\n",
      "Epoch 5/100\n",
      "162/162 - 5s - loss: 0.0014 - accuracy: 1.0000 - precision_18: 1.0000 - specificity_at_sensitivity_18: 1.0000 - 5s/epoch - 33ms/step\n",
      "Epoch 6/100\n",
      "162/162 - 5s - loss: 4.8009e-04 - accuracy: 1.0000 - precision_18: 1.0000 - specificity_at_sensitivity_18: 1.0000 - 5s/epoch - 33ms/step\n",
      "Epoch 7/100\n",
      "162/162 - 5s - loss: 4.3030e-04 - accuracy: 1.0000 - precision_18: 1.0000 - specificity_at_sensitivity_18: 1.0000 - 5s/epoch - 34ms/step\n",
      "Epoch 8/100\n",
      "162/162 - 5s - loss: 3.8937e-04 - accuracy: 1.0000 - precision_18: 1.0000 - specificity_at_sensitivity_18: 1.0000 - 5s/epoch - 34ms/step\n",
      "Epoch 9/100\n",
      "162/162 - 5s - loss: 3.4639e-04 - accuracy: 1.0000 - precision_18: 1.0000 - specificity_at_sensitivity_18: 1.0000 - 5s/epoch - 34ms/step\n",
      "Epoch 10/100\n",
      "162/162 - 5s - loss: 3.1256e-04 - accuracy: 1.0000 - precision_18: 1.0000 - specificity_at_sensitivity_18: 1.0000 - 5s/epoch - 33ms/step\n",
      "Epoch 11/100\n",
      "162/162 - 5s - loss: 2.8381e-04 - accuracy: 1.0000 - precision_18: 1.0000 - specificity_at_sensitivity_18: 1.0000 - 5s/epoch - 34ms/step\n",
      "Epoch 12/100\n",
      "162/162 - 5s - loss: 2.5879e-04 - accuracy: 1.0000 - precision_18: 1.0000 - specificity_at_sensitivity_18: 1.0000 - 5s/epoch - 33ms/step\n",
      "Score for fold 1: loss of 1.5444903373718262; accuracy of 66.66666865348816%; precision_18: 0.0\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ...\n",
      "Epoch 1/100\n",
      "162/162 - 6s - loss: 48.8031 - accuracy: 0.6049 - precision_19: 0.3542 - specificity_at_sensitivity_19: 0.0000e+00 - 6s/epoch - 39ms/step\n",
      "Epoch 2/100\n",
      "162/162 - 5s - loss: 0.3759 - accuracy: 0.8951 - precision_19: 0.8367 - specificity_at_sensitivity_19: 0.0179 - 5s/epoch - 33ms/step\n",
      "Epoch 3/100\n",
      "162/162 - 5s - loss: 0.0348 - accuracy: 0.9938 - precision_19: 1.0000 - specificity_at_sensitivity_19: 0.9107 - 5s/epoch - 33ms/step\n",
      "Epoch 4/100\n",
      "162/162 - 5s - loss: 0.0054 - accuracy: 1.0000 - precision_19: 1.0000 - specificity_at_sensitivity_19: 1.0000 - 5s/epoch - 34ms/step\n",
      "Epoch 5/100\n",
      "162/162 - 5s - loss: 0.0017 - accuracy: 1.0000 - precision_19: 1.0000 - specificity_at_sensitivity_19: 1.0000 - 5s/epoch - 34ms/step\n",
      "Epoch 6/100\n",
      "162/162 - 5s - loss: 0.0023 - accuracy: 1.0000 - precision_19: 1.0000 - specificity_at_sensitivity_19: 1.0000 - 5s/epoch - 33ms/step\n",
      "Epoch 7/100\n",
      "162/162 - 5s - loss: 7.9916e-04 - accuracy: 1.0000 - precision_19: 1.0000 - specificity_at_sensitivity_19: 1.0000 - 5s/epoch - 34ms/step\n",
      "Epoch 8/100\n",
      "162/162 - 5s - loss: 3.2883e-04 - accuracy: 1.0000 - precision_19: 1.0000 - specificity_at_sensitivity_19: 1.0000 - 5s/epoch - 33ms/step\n",
      "Epoch 9/100\n",
      "162/162 - 5s - loss: 1.9365e-04 - accuracy: 1.0000 - precision_19: 1.0000 - specificity_at_sensitivity_19: 1.0000 - 5s/epoch - 34ms/step\n",
      "Epoch 10/100\n",
      "162/162 - 5s - loss: 1.1684e-04 - accuracy: 1.0000 - precision_19: 1.0000 - specificity_at_sensitivity_19: 1.0000 - 5s/epoch - 33ms/step\n",
      "Epoch 11/100\n",
      "162/162 - 5s - loss: 8.9038e-05 - accuracy: 1.0000 - precision_19: 1.0000 - specificity_at_sensitivity_19: 1.0000 - 5s/epoch - 33ms/step\n",
      "Epoch 12/100\n",
      "162/162 - 5s - loss: 6.7921e-05 - accuracy: 1.0000 - precision_19: 1.0000 - specificity_at_sensitivity_19: 1.0000 - 5s/epoch - 33ms/step\n",
      "Epoch 13/100\n",
      "162/162 - 5s - loss: 5.4342e-05 - accuracy: 1.0000 - precision_19: 1.0000 - specificity_at_sensitivity_19: 1.0000 - 5s/epoch - 34ms/step\n",
      "Epoch 14/100\n",
      "162/162 - 5s - loss: 4.4218e-05 - accuracy: 1.0000 - precision_19: 1.0000 - specificity_at_sensitivity_19: 1.0000 - 5s/epoch - 34ms/step\n",
      "Score for fold 2: loss of 1.2969039678573608; accuracy of 70.37037014961243%; precision_19: 0.0\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 3 ...\n",
      "Epoch 1/100\n",
      "162/162 - 6s - loss: 21.0222 - accuracy: 0.5988 - precision_20: 0.3182 - specificity_at_sensitivity_20: 0.0000e+00 - 6s/epoch - 39ms/step\n",
      "Epoch 2/100\n",
      "162/162 - 5s - loss: 0.2086 - accuracy: 0.9259 - precision_20: 0.8776 - specificity_at_sensitivity_20: 0.0088 - 5s/epoch - 34ms/step\n",
      "Epoch 3/100\n",
      "162/162 - 5s - loss: 0.0467 - accuracy: 0.9753 - precision_20: 0.9592 - specificity_at_sensitivity_20: 0.9823 - 5s/epoch - 34ms/step\n",
      "Epoch 4/100\n",
      "162/162 - 5s - loss: 0.0026 - accuracy: 1.0000 - precision_20: 1.0000 - specificity_at_sensitivity_20: 1.0000 - 5s/epoch - 33ms/step\n",
      "Epoch 5/100\n",
      "162/162 - 5s - loss: 9.8781e-04 - accuracy: 1.0000 - precision_20: 1.0000 - specificity_at_sensitivity_20: 1.0000 - 5s/epoch - 34ms/step\n",
      "Epoch 6/100\n",
      "162/162 - 5s - loss: 6.9660e-04 - accuracy: 1.0000 - precision_20: 1.0000 - specificity_at_sensitivity_20: 1.0000 - 5s/epoch - 33ms/step\n",
      "Epoch 7/100\n",
      "162/162 - 6s - loss: 5.4357e-04 - accuracy: 1.0000 - precision_20: 1.0000 - specificity_at_sensitivity_20: 1.0000 - 6s/epoch - 34ms/step\n",
      "Epoch 8/100\n",
      "162/162 - 5s - loss: 4.5766e-04 - accuracy: 1.0000 - precision_20: 1.0000 - specificity_at_sensitivity_20: 1.0000 - 5s/epoch - 34ms/step\n",
      "Epoch 9/100\n",
      "162/162 - 5s - loss: 3.9808e-04 - accuracy: 1.0000 - precision_20: 1.0000 - specificity_at_sensitivity_20: 1.0000 - 5s/epoch - 34ms/step\n",
      "Epoch 10/100\n",
      "162/162 - 5s - loss: 3.3530e-04 - accuracy: 1.0000 - precision_20: 1.0000 - specificity_at_sensitivity_20: 1.0000 - 5s/epoch - 34ms/step\n",
      "Epoch 11/100\n",
      "162/162 - 5s - loss: 2.9495e-04 - accuracy: 1.0000 - precision_20: 1.0000 - specificity_at_sensitivity_20: 1.0000 - 5s/epoch - 33ms/step\n",
      "Epoch 12/100\n",
      "162/162 - 5s - loss: 2.6206e-04 - accuracy: 1.0000 - precision_20: 1.0000 - specificity_at_sensitivity_20: 1.0000 - 5s/epoch - 34ms/step\n",
      "Epoch 13/100\n",
      "162/162 - 5s - loss: 2.3025e-04 - accuracy: 1.0000 - precision_20: 1.0000 - specificity_at_sensitivity_20: 1.0000 - 5s/epoch - 33ms/step\n",
      "Epoch 14/100\n",
      "162/162 - 5s - loss: 2.0397e-04 - accuracy: 1.0000 - precision_20: 1.0000 - specificity_at_sensitivity_20: 1.0000 - 5s/epoch - 33ms/step\n",
      "Score for fold 3: loss of 1.8808192014694214; accuracy of 74.0740716457367%; precision_20: 0.0\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 4 ...\n",
      "Epoch 1/100\n",
      "162/162 - 6s - loss: 42.8952 - accuracy: 0.6296 - precision_21: 0.3409 - specificity_at_sensitivity_21: 0.0000e+00 - 6s/epoch - 39ms/step\n",
      "Epoch 2/100\n",
      "162/162 - 5s - loss: 0.3038 - accuracy: 0.9259 - precision_21: 0.8696 - specificity_at_sensitivity_21: 0.7414 - 5s/epoch - 34ms/step\n",
      "Epoch 3/100\n",
      "162/162 - 5s - loss: 0.0642 - accuracy: 0.9815 - precision_21: 1.0000 - specificity_at_sensitivity_21: 0.8621 - 5s/epoch - 34ms/step\n",
      "Epoch 4/100\n",
      "162/162 - 5s - loss: 0.0031 - accuracy: 1.0000 - precision_21: 1.0000 - specificity_at_sensitivity_21: 1.0000 - 5s/epoch - 33ms/step\n",
      "Epoch 5/100\n",
      "162/162 - 5s - loss: 9.0673e-04 - accuracy: 1.0000 - precision_21: 1.0000 - specificity_at_sensitivity_21: 1.0000 - 5s/epoch - 34ms/step\n",
      "Epoch 6/100\n",
      "162/162 - 5s - loss: 6.6712e-04 - accuracy: 1.0000 - precision_21: 1.0000 - specificity_at_sensitivity_21: 1.0000 - 5s/epoch - 34ms/step\n",
      "Epoch 7/100\n",
      "162/162 - 5s - loss: 5.3939e-04 - accuracy: 1.0000 - precision_21: 1.0000 - specificity_at_sensitivity_21: 1.0000 - 5s/epoch - 33ms/step\n",
      "Epoch 8/100\n",
      "162/162 - 5s - loss: 4.7170e-04 - accuracy: 1.0000 - precision_21: 1.0000 - specificity_at_sensitivity_21: 1.0000 - 5s/epoch - 33ms/step\n",
      "Epoch 9/100\n",
      "162/162 - 5s - loss: 3.9353e-04 - accuracy: 1.0000 - precision_21: 1.0000 - specificity_at_sensitivity_21: 1.0000 - 5s/epoch - 34ms/step\n",
      "Epoch 10/100\n",
      "162/162 - 5s - loss: 3.3666e-04 - accuracy: 1.0000 - precision_21: 1.0000 - specificity_at_sensitivity_21: 1.0000 - 5s/epoch - 34ms/step\n",
      "Epoch 11/100\n",
      "162/162 - 5s - loss: 2.9692e-04 - accuracy: 1.0000 - precision_21: 1.0000 - specificity_at_sensitivity_21: 1.0000 - 5s/epoch - 33ms/step\n",
      "Epoch 12/100\n",
      "162/162 - 5s - loss: 2.6288e-04 - accuracy: 1.0000 - precision_21: 1.0000 - specificity_at_sensitivity_21: 1.0000 - 5s/epoch - 34ms/step\n",
      "Epoch 13/100\n",
      "162/162 - 5s - loss: 2.3281e-04 - accuracy: 1.0000 - precision_21: 1.0000 - specificity_at_sensitivity_21: 1.0000 - 5s/epoch - 33ms/step\n",
      "Epoch 14/100\n",
      "162/162 - 5s - loss: 2.1241e-04 - accuracy: 1.0000 - precision_21: 1.0000 - specificity_at_sensitivity_21: 1.0000 - 5s/epoch - 34ms/step\n",
      "Epoch 15/100\n",
      "162/162 - 5s - loss: 1.8879e-04 - accuracy: 1.0000 - precision_21: 1.0000 - specificity_at_sensitivity_21: 1.0000 - 5s/epoch - 34ms/step\n",
      "Score for fold 4: loss of 2.377596855163574; accuracy of 62.962961196899414%; precision_21: 0.0\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 5 ...\n",
      "Epoch 1/100\n",
      "162/162 - 6s - loss: 29.6035 - accuracy: 0.6235 - precision_22: 0.3469 - specificity_at_sensitivity_22: 0.0000e+00 - 6s/epoch - 39ms/step\n",
      "Epoch 2/100\n",
      "162/162 - 5s - loss: 0.1759 - accuracy: 0.9506 - precision_22: 0.9318 - specificity_at_sensitivity_22: 0.0000e+00 - 5s/epoch - 34ms/step\n",
      "Epoch 3/100\n",
      "162/162 - 6s - loss: 0.0476 - accuracy: 0.9815 - precision_22: 0.9778 - specificity_at_sensitivity_22: 0.9310 - 6s/epoch - 34ms/step\n",
      "Epoch 4/100\n",
      "162/162 - 5s - loss: 0.0377 - accuracy: 0.9877 - precision_22: 0.9783 - specificity_at_sensitivity_22: 0.9828 - 5s/epoch - 34ms/step\n",
      "Epoch 5/100\n",
      "162/162 - 5s - loss: 0.0051 - accuracy: 1.0000 - precision_22: 1.0000 - specificity_at_sensitivity_22: 1.0000 - 5s/epoch - 34ms/step\n",
      "Epoch 6/100\n",
      "162/162 - 5s - loss: 7.6827e-04 - accuracy: 1.0000 - precision_22: 1.0000 - specificity_at_sensitivity_22: 1.0000 - 5s/epoch - 34ms/step\n",
      "Epoch 7/100\n",
      "162/162 - 5s - loss: 5.9042e-04 - accuracy: 1.0000 - precision_22: 1.0000 - specificity_at_sensitivity_22: 1.0000 - 5s/epoch - 34ms/step\n",
      "Epoch 8/100\n",
      "162/162 - 5s - loss: 4.8059e-04 - accuracy: 1.0000 - precision_22: 1.0000 - specificity_at_sensitivity_22: 1.0000 - 5s/epoch - 34ms/step\n",
      "Epoch 9/100\n",
      "162/162 - 5s - loss: 4.0468e-04 - accuracy: 1.0000 - precision_22: 1.0000 - specificity_at_sensitivity_22: 1.0000 - 5s/epoch - 34ms/step\n",
      "Epoch 10/100\n",
      "162/162 - 5s - loss: 3.5167e-04 - accuracy: 1.0000 - precision_22: 1.0000 - specificity_at_sensitivity_22: 1.0000 - 5s/epoch - 34ms/step\n",
      "Epoch 11/100\n",
      "162/162 - 5s - loss: 3.1105e-04 - accuracy: 1.0000 - precision_22: 1.0000 - specificity_at_sensitivity_22: 1.0000 - 5s/epoch - 33ms/step\n",
      "Epoch 12/100\n",
      "162/162 - 5s - loss: 2.7470e-04 - accuracy: 1.0000 - precision_22: 1.0000 - specificity_at_sensitivity_22: 1.0000 - 5s/epoch - 34ms/step\n",
      "Epoch 13/100\n",
      "162/162 - 5s - loss: 2.4554e-04 - accuracy: 1.0000 - precision_22: 1.0000 - specificity_at_sensitivity_22: 1.0000 - 5s/epoch - 34ms/step\n",
      "Epoch 14/100\n",
      "162/162 - 5s - loss: 2.2105e-04 - accuracy: 1.0000 - precision_22: 1.0000 - specificity_at_sensitivity_22: 1.0000 - 5s/epoch - 34ms/step\n",
      "Epoch 15/100\n",
      "162/162 - 5s - loss: 2.0127e-04 - accuracy: 1.0000 - precision_22: 1.0000 - specificity_at_sensitivity_22: 1.0000 - 5s/epoch - 34ms/step\n",
      "Epoch 16/100\n",
      "162/162 - 5s - loss: 1.8131e-04 - accuracy: 1.0000 - precision_22: 1.0000 - specificity_at_sensitivity_22: 1.0000 - 5s/epoch - 34ms/step\n",
      "Score for fold 5: loss of 1.4698143005371094; accuracy of 62.962961196899414%; precision_22: 0.0\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 6 ...\n",
      "Epoch 1/100\n",
      "162/162 - 6s - loss: 13.7296 - accuracy: 0.5741 - precision_23: 0.2857 - specificity_at_sensitivity_23: 0.0000e+00 - 6s/epoch - 39ms/step\n",
      "Epoch 2/100\n",
      "162/162 - 5s - loss: 0.2321 - accuracy: 0.9136 - precision_23: 0.8627 - specificity_at_sensitivity_23: 0.7027 - 5s/epoch - 33ms/step\n",
      "Epoch 3/100\n",
      "162/162 - 5s - loss: 0.0638 - accuracy: 0.9691 - precision_23: 0.9600 - specificity_at_sensitivity_23: 0.8649 - 5s/epoch - 33ms/step\n",
      "Epoch 4/100\n",
      "162/162 - 5s - loss: 0.0211 - accuracy: 0.9877 - precision_23: 0.9804 - specificity_at_sensitivity_23: 0.9910 - 5s/epoch - 33ms/step\n",
      "Epoch 5/100\n",
      "162/162 - 5s - loss: 0.0165 - accuracy: 0.9938 - precision_23: 1.0000 - specificity_at_sensitivity_23: 1.0000 - 5s/epoch - 34ms/step\n",
      "Epoch 6/100\n",
      "162/162 - 5s - loss: 0.0939 - accuracy: 0.9938 - precision_23: 0.9808 - specificity_at_sensitivity_23: 0.9910 - 5s/epoch - 33ms/step\n",
      "Epoch 7/100\n",
      "162/162 - 5s - loss: 0.3047 - accuracy: 0.9753 - precision_23: 0.9608 - specificity_at_sensitivity_23: 0.0450 - 5s/epoch - 34ms/step\n",
      "Epoch 8/100\n",
      "162/162 - 5s - loss: 9.5348e-04 - accuracy: 1.0000 - precision_23: 1.0000 - specificity_at_sensitivity_23: 1.0000 - 5s/epoch - 33ms/step\n",
      "Epoch 9/100\n",
      "162/162 - 5s - loss: 6.9693e-04 - accuracy: 1.0000 - precision_23: 1.0000 - specificity_at_sensitivity_23: 1.0000 - 5s/epoch - 33ms/step\n",
      "Epoch 10/100\n",
      "162/162 - 5s - loss: 5.7997e-04 - accuracy: 1.0000 - precision_23: 1.0000 - specificity_at_sensitivity_23: 1.0000 - 5s/epoch - 34ms/step\n",
      "Epoch 11/100\n",
      "162/162 - 5s - loss: 4.9804e-04 - accuracy: 1.0000 - precision_23: 1.0000 - specificity_at_sensitivity_23: 1.0000 - 5s/epoch - 33ms/step\n",
      "Epoch 12/100\n",
      "162/162 - 5s - loss: 4.3401e-04 - accuracy: 1.0000 - precision_23: 1.0000 - specificity_at_sensitivity_23: 1.0000 - 5s/epoch - 33ms/step\n",
      "Epoch 13/100\n",
      "162/162 - 5s - loss: 3.8221e-04 - accuracy: 1.0000 - precision_23: 1.0000 - specificity_at_sensitivity_23: 1.0000 - 5s/epoch - 33ms/step\n",
      "Epoch 14/100\n",
      "162/162 - 5s - loss: 3.4082e-04 - accuracy: 1.0000 - precision_23: 1.0000 - specificity_at_sensitivity_23: 1.0000 - 5s/epoch - 34ms/step\n",
      "Epoch 15/100\n",
      "162/162 - 5s - loss: 3.0469e-04 - accuracy: 1.0000 - precision_23: 1.0000 - specificity_at_sensitivity_23: 1.0000 - 5s/epoch - 34ms/step\n",
      "Epoch 16/100\n",
      "162/162 - 5s - loss: 2.7468e-04 - accuracy: 1.0000 - precision_23: 1.0000 - specificity_at_sensitivity_23: 1.0000 - 5s/epoch - 33ms/step\n",
      "Epoch 17/100\n",
      "162/162 - 5s - loss: 2.4730e-04 - accuracy: 1.0000 - precision_23: 1.0000 - specificity_at_sensitivity_23: 1.0000 - 5s/epoch - 34ms/step\n",
      "Epoch 18/100\n",
      "162/162 - 5s - loss: 2.2405e-04 - accuracy: 1.0000 - precision_23: 1.0000 - specificity_at_sensitivity_23: 1.0000 - 5s/epoch - 33ms/step\n",
      "Score for fold 6: loss of 1.2444761991500854; accuracy of 77.77777910232544%; precision_23: 0.0\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 7 ...\n",
      "Epoch 1/100\n",
      "162/162 - 6s - loss: 8.3482 - accuracy: 0.6543 - precision_24: 0.3953 - specificity_at_sensitivity_24: 0.0000e+00 - 6s/epoch - 39ms/step\n",
      "Epoch 2/100\n",
      "162/162 - 5s - loss: 0.3786 - accuracy: 0.8951 - precision_24: 0.8261 - specificity_at_sensitivity_24: 0.0087 - 5s/epoch - 34ms/step\n",
      "Epoch 3/100\n",
      "162/162 - 5s - loss: 0.0515 - accuracy: 0.9877 - precision_24: 1.0000 - specificity_at_sensitivity_24: 0.8783 - 5s/epoch - 33ms/step\n",
      "Epoch 4/100\n",
      "162/162 - 5s - loss: 7.3234e-04 - accuracy: 1.0000 - precision_24: 1.0000 - specificity_at_sensitivity_24: 1.0000 - 5s/epoch - 33ms/step\n",
      "Epoch 5/100\n",
      "162/162 - 5s - loss: 3.7816e-04 - accuracy: 1.0000 - precision_24: 1.0000 - specificity_at_sensitivity_24: 1.0000 - 5s/epoch - 34ms/step\n",
      "Epoch 6/100\n",
      "162/162 - 5s - loss: 2.9851e-04 - accuracy: 1.0000 - precision_24: 1.0000 - specificity_at_sensitivity_24: 1.0000 - 5s/epoch - 33ms/step\n",
      "Epoch 7/100\n",
      "162/162 - 5s - loss: 2.4327e-04 - accuracy: 1.0000 - precision_24: 1.0000 - specificity_at_sensitivity_24: 1.0000 - 5s/epoch - 33ms/step\n",
      "Epoch 8/100\n",
      "162/162 - 5s - loss: 2.0171e-04 - accuracy: 1.0000 - precision_24: 1.0000 - specificity_at_sensitivity_24: 1.0000 - 5s/epoch - 34ms/step\n",
      "Epoch 9/100\n",
      "162/162 - 5s - loss: 1.7378e-04 - accuracy: 1.0000 - precision_24: 1.0000 - specificity_at_sensitivity_24: 1.0000 - 5s/epoch - 33ms/step\n",
      "Epoch 10/100\n",
      "162/162 - 5s - loss: 1.5090e-04 - accuracy: 1.0000 - precision_24: 1.0000 - specificity_at_sensitivity_24: 1.0000 - 5s/epoch - 34ms/step\n",
      "Score for fold 7: loss of 2.5978176593780518; accuracy of 66.66666865348816%; precision_24: 0.0\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ...\n",
      "Epoch 1/100\n",
      "165/165 - 6s - loss: 15.1178 - accuracy: 0.5636 - precision_25: 0.2195 - specificity_at_sensitivity_25: 0.0000e+00 - 6s/epoch - 39ms/step\n",
      "Epoch 2/100\n",
      "165/165 - 6s - loss: 0.2161 - accuracy: 0.9212 - precision_25: 0.8750 - specificity_at_sensitivity_25: 0.0259 - 6s/epoch - 34ms/step\n",
      "Epoch 3/100\n",
      "165/165 - 6s - loss: 0.0516 - accuracy: 0.9879 - precision_25: 1.0000 - specificity_at_sensitivity_25: 0.8190 - 6s/epoch - 33ms/step\n",
      "Epoch 4/100\n",
      "165/165 - 6s - loss: 0.0017 - accuracy: 1.0000 - precision_25: 1.0000 - specificity_at_sensitivity_25: 1.0000 - 6s/epoch - 34ms/step\n",
      "Epoch 5/100\n",
      "165/165 - 6s - loss: 6.7847e-04 - accuracy: 1.0000 - precision_25: 1.0000 - specificity_at_sensitivity_25: 1.0000 - 6s/epoch - 33ms/step\n",
      "Epoch 6/100\n",
      "165/165 - 6s - loss: 2.9696e-04 - accuracy: 1.0000 - precision_25: 1.0000 - specificity_at_sensitivity_25: 1.0000 - 6s/epoch - 33ms/step\n",
      "Epoch 7/100\n",
      "165/165 - 6s - loss: 1.9551e-04 - accuracy: 1.0000 - precision_25: 1.0000 - specificity_at_sensitivity_25: 1.0000 - 6s/epoch - 33ms/step\n",
      "Epoch 8/100\n",
      "165/165 - 6s - loss: 1.3345e-04 - accuracy: 1.0000 - precision_25: 1.0000 - specificity_at_sensitivity_25: 1.0000 - 6s/epoch - 34ms/step\n",
      "Epoch 9/100\n",
      "165/165 - 5s - loss: 1.0062e-04 - accuracy: 1.0000 - precision_25: 1.0000 - specificity_at_sensitivity_25: 1.0000 - 5s/epoch - 33ms/step\n",
      "Epoch 10/100\n",
      "165/165 - 6s - loss: 7.8953e-05 - accuracy: 1.0000 - precision_25: 1.0000 - specificity_at_sensitivity_25: 1.0000 - 6s/epoch - 34ms/step\n",
      "Epoch 11/100\n",
      "165/165 - 6s - loss: 6.3704e-05 - accuracy: 1.0000 - precision_25: 1.0000 - specificity_at_sensitivity_25: 1.0000 - 6s/epoch - 33ms/step\n",
      "Epoch 12/100\n",
      "165/165 - 6s - loss: 5.1702e-05 - accuracy: 1.0000 - precision_25: 1.0000 - specificity_at_sensitivity_25: 1.0000 - 6s/epoch - 34ms/step\n",
      "Epoch 13/100\n",
      "165/165 - 6s - loss: 4.3368e-05 - accuracy: 1.0000 - precision_25: 1.0000 - specificity_at_sensitivity_25: 1.0000 - 6s/epoch - 33ms/step\n",
      "Score for fold 1: loss of 1.6204389333724976; accuracy of 66.66666865348816%; precision_25: 0.0\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ...\n",
      "Epoch 1/100\n",
      "165/165 - 7s - loss: 27.9294 - accuracy: 0.6242 - precision_26: 0.3778 - specificity_at_sensitivity_26: 0.0000e+00 - 7s/epoch - 40ms/step\n",
      "Epoch 2/100\n",
      "165/165 - 5s - loss: 0.4018 - accuracy: 0.8970 - precision_26: 0.8542 - specificity_at_sensitivity_26: 0.0175 - 5s/epoch - 33ms/step\n",
      "Epoch 3/100\n",
      "165/165 - 6s - loss: 0.0157 - accuracy: 1.0000 - precision_26: 1.0000 - specificity_at_sensitivity_26: 1.0000 - 6s/epoch - 33ms/step\n",
      "Epoch 4/100\n",
      "165/165 - 5s - loss: 0.0046 - accuracy: 1.0000 - precision_26: 1.0000 - specificity_at_sensitivity_26: 1.0000 - 5s/epoch - 33ms/step\n",
      "Epoch 5/100\n",
      "165/165 - 6s - loss: 0.0016 - accuracy: 1.0000 - precision_26: 1.0000 - specificity_at_sensitivity_26: 1.0000 - 6s/epoch - 34ms/step\n",
      "Epoch 6/100\n",
      "165/165 - 6s - loss: 0.0012 - accuracy: 1.0000 - precision_26: 1.0000 - specificity_at_sensitivity_26: 1.0000 - 6s/epoch - 33ms/step\n",
      "Epoch 7/100\n",
      "165/165 - 6s - loss: 9.7452e-04 - accuracy: 1.0000 - precision_26: 1.0000 - specificity_at_sensitivity_26: 1.0000 - 6s/epoch - 34ms/step\n",
      "Epoch 8/100\n",
      "165/165 - 5s - loss: 8.2294e-04 - accuracy: 1.0000 - precision_26: 1.0000 - specificity_at_sensitivity_26: 1.0000 - 5s/epoch - 33ms/step\n",
      "Epoch 9/100\n",
      "165/165 - 5s - loss: 7.0204e-04 - accuracy: 1.0000 - precision_26: 1.0000 - specificity_at_sensitivity_26: 1.0000 - 5s/epoch - 33ms/step\n",
      "Epoch 10/100\n",
      "165/165 - 5s - loss: 6.1386e-04 - accuracy: 1.0000 - precision_26: 1.0000 - specificity_at_sensitivity_26: 1.0000 - 5s/epoch - 33ms/step\n",
      "Epoch 11/100\n",
      "165/165 - 6s - loss: 5.3196e-04 - accuracy: 1.0000 - precision_26: 1.0000 - specificity_at_sensitivity_26: 1.0000 - 6s/epoch - 34ms/step\n",
      "Epoch 12/100\n",
      "165/165 - 5s - loss: 4.6816e-04 - accuracy: 1.0000 - precision_26: 1.0000 - specificity_at_sensitivity_26: 1.0000 - 5s/epoch - 33ms/step\n",
      "Epoch 13/100\n",
      "165/165 - 6s - loss: 4.1166e-04 - accuracy: 1.0000 - precision_26: 1.0000 - specificity_at_sensitivity_26: 1.0000 - 6s/epoch - 33ms/step\n",
      "Epoch 14/100\n",
      "165/165 - 5s - loss: 3.6671e-04 - accuracy: 1.0000 - precision_26: 1.0000 - specificity_at_sensitivity_26: 1.0000 - 5s/epoch - 33ms/step\n",
      "Epoch 15/100\n",
      "165/165 - 6s - loss: 3.2612e-04 - accuracy: 1.0000 - precision_26: 1.0000 - specificity_at_sensitivity_26: 1.0000 - 6s/epoch - 34ms/step\n",
      "Epoch 16/100\n",
      "165/165 - 6s - loss: 2.9143e-04 - accuracy: 1.0000 - precision_26: 1.0000 - specificity_at_sensitivity_26: 1.0000 - 6s/epoch - 33ms/step\n",
      "Epoch 17/100\n",
      "165/165 - 6s - loss: 2.6199e-04 - accuracy: 1.0000 - precision_26: 1.0000 - specificity_at_sensitivity_26: 1.0000 - 6s/epoch - 34ms/step\n",
      "Epoch 18/100\n",
      "165/165 - 6s - loss: 2.3505e-04 - accuracy: 1.0000 - precision_26: 1.0000 - specificity_at_sensitivity_26: 1.0000 - 6s/epoch - 34ms/step\n",
      "Epoch 19/100\n",
      "165/165 - 6s - loss: 2.1266e-04 - accuracy: 1.0000 - precision_26: 1.0000 - specificity_at_sensitivity_26: 1.0000 - 6s/epoch - 34ms/step\n",
      "Score for fold 2: loss of 1.3019342422485352; accuracy of 79.16666865348816%; precision_26: 0.0\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 3 ...\n",
      "Epoch 1/100\n",
      "165/165 - 6s - loss: 18.7923 - accuracy: 0.6303 - precision_27: 0.3111 - specificity_at_sensitivity_27: 0.0000e+00 - 6s/epoch - 39ms/step\n",
      "Epoch 2/100\n",
      "165/165 - 6s - loss: 0.5032 - accuracy: 0.9091 - precision_27: 0.8372 - specificity_at_sensitivity_27: 0.0083 - 6s/epoch - 34ms/step\n",
      "Epoch 3/100\n",
      "165/165 - 6s - loss: 0.0974 - accuracy: 0.9758 - precision_27: 1.0000 - specificity_at_sensitivity_27: 0.0248 - 6s/epoch - 34ms/step\n",
      "Epoch 4/100\n",
      "165/165 - 5s - loss: 0.0049 - accuracy: 1.0000 - precision_27: 1.0000 - specificity_at_sensitivity_27: 1.0000 - 5s/epoch - 33ms/step\n",
      "Epoch 5/100\n",
      "165/165 - 6s - loss: 2.3413e-04 - accuracy: 1.0000 - precision_27: 1.0000 - specificity_at_sensitivity_27: 1.0000 - 6s/epoch - 34ms/step\n",
      "Epoch 6/100\n",
      "165/165 - 6s - loss: 1.2099e-04 - accuracy: 1.0000 - precision_27: 1.0000 - specificity_at_sensitivity_27: 1.0000 - 6s/epoch - 33ms/step\n",
      "Epoch 7/100\n",
      "165/165 - 6s - loss: 7.9607e-05 - accuracy: 1.0000 - precision_27: 1.0000 - specificity_at_sensitivity_27: 1.0000 - 6s/epoch - 33ms/step\n",
      "Epoch 8/100\n",
      "165/165 - 5s - loss: 5.7831e-05 - accuracy: 1.0000 - precision_27: 1.0000 - specificity_at_sensitivity_27: 1.0000 - 5s/epoch - 33ms/step\n",
      "Epoch 9/100\n",
      "165/165 - 6s - loss: 4.4392e-05 - accuracy: 1.0000 - precision_27: 1.0000 - specificity_at_sensitivity_27: 1.0000 - 6s/epoch - 34ms/step\n",
      "Score for fold 3: loss of 2.6393516063690186; accuracy of 50.0%; precision_27: 0.0\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 4 ...\n",
      "Epoch 1/100\n",
      "165/165 - 6s - loss: 11.6435 - accuracy: 0.5636 - precision_28: 0.2000 - specificity_at_sensitivity_28: 0.0000e+00 - 6s/epoch - 38ms/step\n",
      "Epoch 2/100\n",
      "165/165 - 6s - loss: 0.2720 - accuracy: 0.9394 - precision_28: 0.9070 - specificity_at_sensitivity_28: 0.0000e+00 - 6s/epoch - 33ms/step\n",
      "Epoch 3/100\n",
      "165/165 - 6s - loss: 0.0443 - accuracy: 0.9818 - precision_28: 0.9773 - specificity_at_sensitivity_28: 0.9833 - 6s/epoch - 34ms/step\n",
      "Epoch 4/100\n",
      "165/165 - 6s - loss: 0.0041 - accuracy: 1.0000 - precision_28: 1.0000 - specificity_at_sensitivity_28: 1.0000 - 6s/epoch - 33ms/step\n",
      "Epoch 5/100\n",
      "165/165 - 6s - loss: 3.5750e-04 - accuracy: 1.0000 - precision_28: 1.0000 - specificity_at_sensitivity_28: 1.0000 - 6s/epoch - 34ms/step\n",
      "Epoch 6/100\n",
      "165/165 - 6s - loss: 2.5881e-04 - accuracy: 1.0000 - precision_28: 1.0000 - specificity_at_sensitivity_28: 1.0000 - 6s/epoch - 34ms/step\n",
      "Epoch 7/100\n",
      "165/165 - 6s - loss: 2.2744e-04 - accuracy: 1.0000 - precision_28: 1.0000 - specificity_at_sensitivity_28: 1.0000 - 6s/epoch - 34ms/step\n",
      "Epoch 8/100\n",
      "165/165 - 6s - loss: 2.0713e-04 - accuracy: 1.0000 - precision_28: 1.0000 - specificity_at_sensitivity_28: 1.0000 - 6s/epoch - 34ms/step\n",
      "Epoch 9/100\n",
      "165/165 - 6s - loss: 1.8683e-04 - accuracy: 1.0000 - precision_28: 1.0000 - specificity_at_sensitivity_28: 1.0000 - 6s/epoch - 33ms/step\n",
      "Epoch 10/100\n",
      "165/165 - 6s - loss: 1.6928e-04 - accuracy: 1.0000 - precision_28: 1.0000 - specificity_at_sensitivity_28: 1.0000 - 6s/epoch - 34ms/step\n",
      "Score for fold 4: loss of 2.4904139041900635; accuracy of 54.16666865348816%; precision_28: 0.0\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 5 ...\n",
      "Epoch 1/100\n",
      "165/165 - 6s - loss: 10.8064 - accuracy: 0.5818 - precision_29: 0.2889 - specificity_at_sensitivity_29: 0.0000e+00 - 6s/epoch - 37ms/step\n",
      "Epoch 2/100\n",
      "165/165 - 6s - loss: 0.6392 - accuracy: 0.8485 - precision_29: 0.7358 - specificity_at_sensitivity_29: 0.0174 - 6s/epoch - 34ms/step\n",
      "Epoch 3/100\n",
      "165/165 - 6s - loss: 0.1105 - accuracy: 0.9818 - precision_29: 0.9796 - specificity_at_sensitivity_29: 0.0087 - 6s/epoch - 34ms/step\n",
      "Epoch 4/100\n",
      "165/165 - 6s - loss: 0.0358 - accuracy: 0.9879 - precision_29: 0.9800 - specificity_at_sensitivity_29: 0.9913 - 6s/epoch - 34ms/step\n",
      "Epoch 5/100\n",
      "165/165 - 6s - loss: 5.7773e-04 - accuracy: 1.0000 - precision_29: 1.0000 - specificity_at_sensitivity_29: 1.0000 - 6s/epoch - 34ms/step\n",
      "Epoch 6/100\n",
      "165/165 - 6s - loss: 3.0480e-04 - accuracy: 1.0000 - precision_29: 1.0000 - specificity_at_sensitivity_29: 1.0000 - 6s/epoch - 33ms/step\n",
      "Epoch 7/100\n",
      "165/165 - 6s - loss: 2.4061e-04 - accuracy: 1.0000 - precision_29: 1.0000 - specificity_at_sensitivity_29: 1.0000 - 6s/epoch - 34ms/step\n",
      "Epoch 8/100\n",
      "165/165 - 6s - loss: 2.0180e-04 - accuracy: 1.0000 - precision_29: 1.0000 - specificity_at_sensitivity_29: 1.0000 - 6s/epoch - 33ms/step\n",
      "Epoch 9/100\n",
      "165/165 - 6s - loss: 1.7397e-04 - accuracy: 1.0000 - precision_29: 1.0000 - specificity_at_sensitivity_29: 1.0000 - 6s/epoch - 34ms/step\n",
      "Epoch 10/100\n",
      "165/165 - 6s - loss: 1.5200e-04 - accuracy: 1.0000 - precision_29: 1.0000 - specificity_at_sensitivity_29: 1.0000 - 6s/epoch - 34ms/step\n",
      "Epoch 11/100\n",
      "165/165 - 6s - loss: 1.3536e-04 - accuracy: 1.0000 - precision_29: 1.0000 - specificity_at_sensitivity_29: 1.0000 - 6s/epoch - 34ms/step\n",
      "Score for fold 5: loss of 1.5273076295852661; accuracy of 66.66666865348816%; precision_29: 0.0\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 6 ...\n",
      "Epoch 1/100\n",
      "166/166 - 6s - loss: 10.7916 - accuracy: 0.5060 - precision_30: 0.2115 - specificity_at_sensitivity_30: 0.0000e+00 - 6s/epoch - 39ms/step\n",
      "Epoch 2/100\n",
      "166/166 - 6s - loss: 0.5573 - accuracy: 0.8916 - precision_30: 0.8148 - specificity_at_sensitivity_30: 0.0000e+00 - 6s/epoch - 33ms/step\n",
      "Epoch 3/100\n",
      "166/166 - 6s - loss: 0.0030 - accuracy: 1.0000 - precision_30: 1.0000 - specificity_at_sensitivity_30: 1.0000 - 6s/epoch - 33ms/step\n",
      "Epoch 4/100\n",
      "166/166 - 5s - loss: 0.0010 - accuracy: 1.0000 - precision_30: 1.0000 - specificity_at_sensitivity_30: 1.0000 - 5s/epoch - 33ms/step\n",
      "Epoch 5/100\n",
      "166/166 - 6s - loss: 2.6817e-04 - accuracy: 1.0000 - precision_30: 1.0000 - specificity_at_sensitivity_30: 1.0000 - 6s/epoch - 33ms/step\n",
      "Epoch 6/100\n",
      "166/166 - 6s - loss: 2.3096e-04 - accuracy: 1.0000 - precision_30: 1.0000 - specificity_at_sensitivity_30: 1.0000 - 6s/epoch - 33ms/step\n",
      "Epoch 7/100\n",
      "166/166 - 6s - loss: 2.0046e-04 - accuracy: 1.0000 - precision_30: 1.0000 - specificity_at_sensitivity_30: 1.0000 - 6s/epoch - 34ms/step\n",
      "Epoch 8/100\n",
      "166/166 - 6s - loss: 1.7743e-04 - accuracy: 1.0000 - precision_30: 1.0000 - specificity_at_sensitivity_30: 1.0000 - 6s/epoch - 34ms/step\n",
      "Score for fold 6: loss of 1.02692449092865; accuracy of 82.6086938381195%; precision_30: 0.0\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 7 ...\n",
      "Epoch 1/100\n",
      "166/166 - 6s - loss: 13.1620 - accuracy: 0.5843 - precision_31: 0.3519 - specificity_at_sensitivity_31: 0.0000e+00 - 6s/epoch - 39ms/step\n",
      "Epoch 2/100\n",
      "166/166 - 6s - loss: 0.5341 - accuracy: 0.8916 - precision_31: 0.8571 - specificity_at_sensitivity_31: 0.0088 - 6s/epoch - 33ms/step\n",
      "Epoch 3/100\n",
      "166/166 - 6s - loss: 0.1024 - accuracy: 0.9398 - precision_31: 0.8909 - specificity_at_sensitivity_31: 0.9115 - 6s/epoch - 33ms/step\n",
      "Epoch 4/100\n",
      "166/166 - 5s - loss: 0.0397 - accuracy: 0.9819 - precision_31: 0.9630 - specificity_at_sensitivity_31: 0.9735 - 5s/epoch - 33ms/step\n",
      "Epoch 5/100\n",
      "166/166 - 6s - loss: 0.0025 - accuracy: 1.0000 - precision_31: 1.0000 - specificity_at_sensitivity_31: 1.0000 - 6s/epoch - 33ms/step\n",
      "Epoch 6/100\n",
      "166/166 - 6s - loss: 2.5857e-04 - accuracy: 1.0000 - precision_31: 1.0000 - specificity_at_sensitivity_31: 1.0000 - 6s/epoch - 33ms/step\n",
      "Epoch 7/100\n",
      "166/166 - 6s - loss: 1.8679e-04 - accuracy: 1.0000 - precision_31: 1.0000 - specificity_at_sensitivity_31: 1.0000 - 6s/epoch - 33ms/step\n",
      "Epoch 8/100\n",
      "166/166 - 6s - loss: 1.6502e-04 - accuracy: 1.0000 - precision_31: 1.0000 - specificity_at_sensitivity_31: 1.0000 - 6s/epoch - 33ms/step\n",
      "Epoch 9/100\n",
      "166/166 - 6s - loss: 1.5243e-04 - accuracy: 1.0000 - precision_31: 1.0000 - specificity_at_sensitivity_31: 1.0000 - 6s/epoch - 33ms/step\n",
      "Epoch 10/100\n",
      "166/166 - 6s - loss: 1.4001e-04 - accuracy: 1.0000 - precision_31: 1.0000 - specificity_at_sensitivity_31: 1.0000 - 6s/epoch - 33ms/step\n",
      "Epoch 11/100\n",
      "166/166 - 6s - loss: 1.3043e-04 - accuracy: 1.0000 - precision_31: 1.0000 - specificity_at_sensitivity_31: 1.0000 - 6s/epoch - 33ms/step\n",
      "Epoch 12/100\n",
      "166/166 - 6s - loss: 1.2203e-04 - accuracy: 1.0000 - precision_31: 1.0000 - specificity_at_sensitivity_31: 1.0000 - 6s/epoch - 33ms/step\n",
      "Score for fold 7: loss of 0.9743964076042175; accuracy of 86.95651888847351%; precision_31: 0.0\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 8 ...\n",
      "Epoch 1/100\n",
      "166/166 - 6s - loss: 7.3226 - accuracy: 0.6506 - precision_32: 0.4000 - specificity_at_sensitivity_32: 0.0000e+00 - 6s/epoch - 39ms/step\n",
      "Epoch 2/100\n",
      "166/166 - 6s - loss: 0.3908 - accuracy: 0.8795 - precision_32: 0.8182 - specificity_at_sensitivity_32: 0.0085 - 6s/epoch - 33ms/step\n",
      "Epoch 3/100\n",
      "166/166 - 6s - loss: 0.0333 - accuracy: 0.9819 - precision_32: 0.9787 - specificity_at_sensitivity_32: 0.9831 - 6s/epoch - 33ms/step\n",
      "Epoch 4/100\n",
      "166/166 - 6s - loss: 0.0079 - accuracy: 1.0000 - precision_32: 1.0000 - specificity_at_sensitivity_32: 1.0000 - 6s/epoch - 33ms/step\n",
      "Epoch 5/100\n",
      "166/166 - 6s - loss: 0.0010 - accuracy: 1.0000 - precision_32: 1.0000 - specificity_at_sensitivity_32: 1.0000 - 6s/epoch - 33ms/step\n",
      "Epoch 6/100\n",
      "166/166 - 6s - loss: 7.0290e-04 - accuracy: 1.0000 - precision_32: 1.0000 - specificity_at_sensitivity_32: 1.0000 - 6s/epoch - 33ms/step\n",
      "Epoch 7/100\n",
      "166/166 - 6s - loss: 5.4899e-04 - accuracy: 1.0000 - precision_32: 1.0000 - specificity_at_sensitivity_32: 1.0000 - 6s/epoch - 33ms/step\n",
      "Epoch 8/100\n",
      "166/166 - 6s - loss: 4.4895e-04 - accuracy: 1.0000 - precision_32: 1.0000 - specificity_at_sensitivity_32: 1.0000 - 6s/epoch - 33ms/step\n",
      "Epoch 9/100\n",
      "166/166 - 6s - loss: 3.8483e-04 - accuracy: 1.0000 - precision_32: 1.0000 - specificity_at_sensitivity_32: 1.0000 - 6s/epoch - 33ms/step\n",
      "Epoch 10/100\n",
      "166/166 - 6s - loss: 3.2365e-04 - accuracy: 1.0000 - precision_32: 1.0000 - specificity_at_sensitivity_32: 1.0000 - 6s/epoch - 33ms/step\n",
      "Epoch 11/100\n",
      "166/166 - 6s - loss: 2.8189e-04 - accuracy: 1.0000 - precision_32: 1.0000 - specificity_at_sensitivity_32: 1.0000 - 6s/epoch - 33ms/step\n",
      "Epoch 12/100\n",
      "166/166 - 6s - loss: 2.4521e-04 - accuracy: 1.0000 - precision_32: 1.0000 - specificity_at_sensitivity_32: 1.0000 - 6s/epoch - 33ms/step\n",
      "Epoch 13/100\n",
      "166/166 - 6s - loss: 2.1850e-04 - accuracy: 1.0000 - precision_32: 1.0000 - specificity_at_sensitivity_32: 1.0000 - 6s/epoch - 33ms/step\n",
      "Epoch 14/100\n",
      "166/166 - 6s - loss: 1.9111e-04 - accuracy: 1.0000 - precision_32: 1.0000 - specificity_at_sensitivity_32: 1.0000 - 6s/epoch - 33ms/step\n",
      "Epoch 15/100\n",
      "166/166 - 6s - loss: 1.6988e-04 - accuracy: 1.0000 - precision_32: 1.0000 - specificity_at_sensitivity_32: 1.0000 - 6s/epoch - 33ms/step\n",
      "Epoch 16/100\n",
      "166/166 - 6s - loss: 1.5257e-04 - accuracy: 1.0000 - precision_32: 1.0000 - specificity_at_sensitivity_32: 1.0000 - 6s/epoch - 33ms/step\n",
      "Score for fold 8: loss of 1.7121343612670898; accuracy of 65.21739363670349%; precision_32: 0.0\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ...\n",
      "Epoch 1/100\n",
      "168/168 - 7s - loss: 10.0885 - accuracy: 0.6012 - precision_33: 0.3125 - specificity_at_sensitivity_33: 0.0000e+00 - 7s/epoch - 40ms/step\n",
      "Epoch 2/100\n",
      "168/168 - 6s - loss: 0.3777 - accuracy: 0.9107 - precision_33: 0.8542 - specificity_at_sensitivity_33: 0.0084 - 6s/epoch - 33ms/step\n",
      "Epoch 3/100\n",
      "168/168 - 6s - loss: 0.0086 - accuracy: 1.0000 - precision_33: 1.0000 - specificity_at_sensitivity_33: 1.0000 - 6s/epoch - 33ms/step\n",
      "Epoch 4/100\n",
      "168/168 - 6s - loss: 0.0074 - accuracy: 1.0000 - precision_33: 1.0000 - specificity_at_sensitivity_33: 1.0000 - 6s/epoch - 33ms/step\n",
      "Epoch 5/100\n",
      "168/168 - 6s - loss: 0.0042 - accuracy: 1.0000 - precision_33: 1.0000 - specificity_at_sensitivity_33: 1.0000 - 6s/epoch - 33ms/step\n",
      "Epoch 6/100\n",
      "168/168 - 6s - loss: 2.0793e-04 - accuracy: 1.0000 - precision_33: 1.0000 - specificity_at_sensitivity_33: 1.0000 - 6s/epoch - 33ms/step\n",
      "Epoch 7/100\n",
      "168/168 - 6s - loss: 7.9161e-05 - accuracy: 1.0000 - precision_33: 1.0000 - specificity_at_sensitivity_33: 1.0000 - 6s/epoch - 34ms/step\n",
      "Epoch 8/100\n",
      "168/168 - 6s - loss: 6.3903e-05 - accuracy: 1.0000 - precision_33: 1.0000 - specificity_at_sensitivity_33: 1.0000 - 6s/epoch - 34ms/step\n",
      "Epoch 9/100\n",
      "168/168 - 6s - loss: 5.5872e-05 - accuracy: 1.0000 - precision_33: 1.0000 - specificity_at_sensitivity_33: 1.0000 - 6s/epoch - 33ms/step\n",
      "Epoch 10/100\n",
      "168/168 - 6s - loss: 5.0782e-05 - accuracy: 1.0000 - precision_33: 1.0000 - specificity_at_sensitivity_33: 1.0000 - 6s/epoch - 33ms/step\n",
      "Score for fold 1: loss of 2.331441879272461; accuracy of 66.66666865348816%; precision_33: 0.0\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ...\n",
      "Epoch 1/100\n",
      "168/168 - 6s - loss: 12.9299 - accuracy: 0.6190 - precision_34: 0.2791 - specificity_at_sensitivity_34: 0.0000e+00 - 6s/epoch - 38ms/step\n",
      "Epoch 2/100\n",
      "168/168 - 6s - loss: 0.5508 - accuracy: 0.8929 - precision_34: 0.8000 - specificity_at_sensitivity_34: 0.0000e+00 - 6s/epoch - 33ms/step\n",
      "Epoch 3/100\n",
      "168/168 - 6s - loss: 0.0967 - accuracy: 0.9821 - precision_34: 0.9773 - specificity_at_sensitivity_34: 0.0000e+00 - 6s/epoch - 33ms/step\n",
      "Epoch 4/100\n",
      "168/168 - 6s - loss: 0.0022 - accuracy: 1.0000 - precision_34: 1.0000 - specificity_at_sensitivity_34: 1.0000 - 6s/epoch - 33ms/step\n",
      "Epoch 5/100\n",
      "168/168 - 6s - loss: 6.8426e-04 - accuracy: 1.0000 - precision_34: 1.0000 - specificity_at_sensitivity_34: 1.0000 - 6s/epoch - 33ms/step\n",
      "Epoch 6/100\n",
      "168/168 - 6s - loss: 5.0221e-04 - accuracy: 1.0000 - precision_34: 1.0000 - specificity_at_sensitivity_34: 1.0000 - 6s/epoch - 33ms/step\n",
      "Epoch 7/100\n",
      "168/168 - 6s - loss: 4.0113e-04 - accuracy: 1.0000 - precision_34: 1.0000 - specificity_at_sensitivity_34: 1.0000 - 6s/epoch - 33ms/step\n",
      "Epoch 8/100\n",
      "168/168 - 6s - loss: 3.3687e-04 - accuracy: 1.0000 - precision_34: 1.0000 - specificity_at_sensitivity_34: 1.0000 - 6s/epoch - 33ms/step\n",
      "Epoch 9/100\n",
      "168/168 - 6s - loss: 2.8957e-04 - accuracy: 1.0000 - precision_34: 1.0000 - specificity_at_sensitivity_34: 1.0000 - 6s/epoch - 33ms/step\n",
      "Epoch 10/100\n",
      "168/168 - 6s - loss: 2.5313e-04 - accuracy: 1.0000 - precision_34: 1.0000 - specificity_at_sensitivity_34: 1.0000 - 6s/epoch - 33ms/step\n",
      "Epoch 11/100\n",
      "168/168 - 6s - loss: 2.2405e-04 - accuracy: 1.0000 - precision_34: 1.0000 - specificity_at_sensitivity_34: 1.0000 - 6s/epoch - 33ms/step\n",
      "Epoch 12/100\n",
      "168/168 - 6s - loss: 1.9662e-04 - accuracy: 1.0000 - precision_34: 1.0000 - specificity_at_sensitivity_34: 1.0000 - 6s/epoch - 33ms/step\n",
      "Score for fold 2: loss of 3.1859898567199707; accuracy of 47.61904776096344%; precision_34: 0.0\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 3 ...\n",
      "Epoch 1/100\n",
      "168/168 - 6s - loss: 8.3071 - accuracy: 0.6071 - precision_35: 0.3261 - specificity_at_sensitivity_35: 0.0000e+00 - 6s/epoch - 37ms/step\n",
      "Epoch 2/100\n",
      "168/168 - 6s - loss: 0.3714 - accuracy: 0.8929 - precision_35: 0.8636 - specificity_at_sensitivity_35: 0.0169 - 6s/epoch - 33ms/step\n",
      "Epoch 3/100\n",
      "168/168 - 6s - loss: 0.0191 - accuracy: 0.9940 - precision_35: 1.0000 - specificity_at_sensitivity_35: 0.9831 - 6s/epoch - 34ms/step\n",
      "Epoch 4/100\n",
      "168/168 - 6s - loss: 0.0026 - accuracy: 1.0000 - precision_35: 1.0000 - specificity_at_sensitivity_35: 1.0000 - 6s/epoch - 34ms/step\n",
      "Epoch 5/100\n",
      "168/168 - 6s - loss: 0.0010 - accuracy: 1.0000 - precision_35: 1.0000 - specificity_at_sensitivity_35: 1.0000 - 6s/epoch - 33ms/step\n",
      "Epoch 6/100\n",
      "168/168 - 6s - loss: 7.1223e-04 - accuracy: 1.0000 - precision_35: 1.0000 - specificity_at_sensitivity_35: 1.0000 - 6s/epoch - 33ms/step\n",
      "Epoch 7/100\n",
      "168/168 - 6s - loss: 5.8883e-04 - accuracy: 1.0000 - precision_35: 1.0000 - specificity_at_sensitivity_35: 1.0000 - 6s/epoch - 33ms/step\n",
      "Epoch 8/100\n",
      "168/168 - 6s - loss: 4.8946e-04 - accuracy: 1.0000 - precision_35: 1.0000 - specificity_at_sensitivity_35: 1.0000 - 6s/epoch - 33ms/step\n",
      "Epoch 9/100\n",
      "168/168 - 6s - loss: 4.1920e-04 - accuracy: 1.0000 - precision_35: 1.0000 - specificity_at_sensitivity_35: 1.0000 - 6s/epoch - 33ms/step\n",
      "Epoch 10/100\n",
      "168/168 - 6s - loss: 3.7340e-04 - accuracy: 1.0000 - precision_35: 1.0000 - specificity_at_sensitivity_35: 1.0000 - 6s/epoch - 33ms/step\n",
      "Epoch 11/100\n",
      "168/168 - 6s - loss: 3.1916e-04 - accuracy: 1.0000 - precision_35: 1.0000 - specificity_at_sensitivity_35: 1.0000 - 6s/epoch - 33ms/step\n",
      "Epoch 12/100\n",
      "168/168 - 6s - loss: 2.8498e-04 - accuracy: 1.0000 - precision_35: 1.0000 - specificity_at_sensitivity_35: 1.0000 - 6s/epoch - 33ms/step\n",
      "Epoch 13/100\n",
      "168/168 - 6s - loss: 2.5335e-04 - accuracy: 1.0000 - precision_35: 1.0000 - specificity_at_sensitivity_35: 1.0000 - 6s/epoch - 33ms/step\n",
      "Epoch 14/100\n",
      "168/168 - 6s - loss: 2.2629e-04 - accuracy: 1.0000 - precision_35: 1.0000 - specificity_at_sensitivity_35: 1.0000 - 6s/epoch - 33ms/step\n",
      "Score for fold 3: loss of 1.7330175638198853; accuracy of 76.1904776096344%; precision_35: 1.0\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 4 ...\n",
      "Epoch 1/100\n",
      "168/168 - 6s - loss: 29.1216 - accuracy: 0.5774 - precision_36: 0.3091 - specificity_at_sensitivity_36: 0.0000e+00 - 6s/epoch - 38ms/step\n",
      "Epoch 2/100\n",
      "168/168 - 6s - loss: 0.3264 - accuracy: 0.8988 - precision_36: 0.8511 - specificity_at_sensitivity_36: 0.0000e+00 - 6s/epoch - 33ms/step\n",
      "Epoch 3/100\n",
      "168/168 - 6s - loss: 0.0141 - accuracy: 1.0000 - precision_36: 1.0000 - specificity_at_sensitivity_36: 1.0000 - 6s/epoch - 33ms/step\n",
      "Epoch 4/100\n",
      "168/168 - 6s - loss: 0.0037 - accuracy: 1.0000 - precision_36: 1.0000 - specificity_at_sensitivity_36: 1.0000 - 6s/epoch - 34ms/step\n",
      "Epoch 5/100\n",
      "168/168 - 6s - loss: 0.0024 - accuracy: 1.0000 - precision_36: 1.0000 - specificity_at_sensitivity_36: 1.0000 - 6s/epoch - 33ms/step\n",
      "Epoch 6/100\n",
      "168/168 - 6s - loss: 0.0017 - accuracy: 1.0000 - precision_36: 1.0000 - specificity_at_sensitivity_36: 1.0000 - 6s/epoch - 33ms/step\n",
      "Epoch 7/100\n",
      "168/168 - 6s - loss: 0.0014 - accuracy: 1.0000 - precision_36: 1.0000 - specificity_at_sensitivity_36: 1.0000 - 6s/epoch - 33ms/step\n",
      "Epoch 8/100\n",
      "168/168 - 6s - loss: 0.0011 - accuracy: 1.0000 - precision_36: 1.0000 - specificity_at_sensitivity_36: 1.0000 - 6s/epoch - 33ms/step\n",
      "Epoch 9/100\n",
      "168/168 - 6s - loss: 9.4070e-04 - accuracy: 1.0000 - precision_36: 1.0000 - specificity_at_sensitivity_36: 1.0000 - 6s/epoch - 33ms/step\n",
      "Epoch 10/100\n",
      "168/168 - 6s - loss: 8.1099e-04 - accuracy: 1.0000 - precision_36: 1.0000 - specificity_at_sensitivity_36: 1.0000 - 6s/epoch - 33ms/step\n",
      "Epoch 11/100\n",
      "168/168 - 6s - loss: 6.9917e-04 - accuracy: 1.0000 - precision_36: 1.0000 - specificity_at_sensitivity_36: 1.0000 - 6s/epoch - 33ms/step\n",
      "Epoch 12/100\n",
      "168/168 - 6s - loss: 6.0900e-04 - accuracy: 1.0000 - precision_36: 1.0000 - specificity_at_sensitivity_36: 1.0000 - 6s/epoch - 33ms/step\n",
      "Epoch 13/100\n",
      "168/168 - 6s - loss: 5.4015e-04 - accuracy: 1.0000 - precision_36: 1.0000 - specificity_at_sensitivity_36: 1.0000 - 6s/epoch - 33ms/step\n",
      "Epoch 14/100\n",
      "168/168 - 6s - loss: 4.7076e-04 - accuracy: 1.0000 - precision_36: 1.0000 - specificity_at_sensitivity_36: 1.0000 - 6s/epoch - 33ms/step\n",
      "Epoch 15/100\n",
      "168/168 - 6s - loss: 4.2072e-04 - accuracy: 1.0000 - precision_36: 1.0000 - specificity_at_sensitivity_36: 1.0000 - 6s/epoch - 33ms/step\n",
      "Epoch 16/100\n",
      "168/168 - 6s - loss: 3.7422e-04 - accuracy: 1.0000 - precision_36: 1.0000 - specificity_at_sensitivity_36: 1.0000 - 6s/epoch - 33ms/step\n",
      "Epoch 17/100\n",
      "168/168 - 6s - loss: 3.3439e-04 - accuracy: 1.0000 - precision_36: 1.0000 - specificity_at_sensitivity_36: 1.0000 - 6s/epoch - 34ms/step\n",
      "Epoch 18/100\n",
      "168/168 - 6s - loss: 3.0291e-04 - accuracy: 1.0000 - precision_36: 1.0000 - specificity_at_sensitivity_36: 1.0000 - 6s/epoch - 33ms/step\n",
      "Epoch 19/100\n",
      "168/168 - 6s - loss: 2.7244e-04 - accuracy: 1.0000 - precision_36: 1.0000 - specificity_at_sensitivity_36: 1.0000 - 6s/epoch - 33ms/step\n",
      "Epoch 20/100\n",
      "168/168 - 6s - loss: 2.4363e-04 - accuracy: 1.0000 - precision_36: 1.0000 - specificity_at_sensitivity_36: 1.0000 - 6s/epoch - 33ms/step\n",
      "Epoch 21/100\n",
      "168/168 - 6s - loss: 2.1947e-04 - accuracy: 1.0000 - precision_36: 1.0000 - specificity_at_sensitivity_36: 1.0000 - 6s/epoch - 33ms/step\n",
      "Score for fold 4: loss of 1.510920524597168; accuracy of 71.42857313156128%; precision_36: 0.0\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 5 ...\n",
      "Epoch 1/100\n",
      "168/168 - 7s - loss: 18.8804 - accuracy: 0.5476 - precision_37: 0.2600 - specificity_at_sensitivity_37: 0.0000e+00 - 7s/epoch - 39ms/step\n",
      "Epoch 2/100\n",
      "168/168 - 6s - loss: 0.2300 - accuracy: 0.9226 - precision_37: 0.8980 - specificity_at_sensitivity_37: 0.0517 - 6s/epoch - 33ms/step\n",
      "Epoch 3/100\n",
      "168/168 - 6s - loss: 0.0104 - accuracy: 1.0000 - precision_37: 1.0000 - specificity_at_sensitivity_37: 1.0000 - 6s/epoch - 33ms/step\n",
      "Epoch 4/100\n",
      "168/168 - 6s - loss: 0.0028 - accuracy: 1.0000 - precision_37: 1.0000 - specificity_at_sensitivity_37: 1.0000 - 6s/epoch - 33ms/step\n",
      "Epoch 5/100\n",
      "168/168 - 6s - loss: 0.0012 - accuracy: 1.0000 - precision_37: 1.0000 - specificity_at_sensitivity_37: 1.0000 - 6s/epoch - 33ms/step\n",
      "Epoch 6/100\n",
      "168/168 - 6s - loss: 7.0352e-04 - accuracy: 1.0000 - precision_37: 1.0000 - specificity_at_sensitivity_37: 1.0000 - 6s/epoch - 33ms/step\n",
      "Epoch 7/100\n",
      "168/168 - 6s - loss: 5.8388e-04 - accuracy: 1.0000 - precision_37: 1.0000 - specificity_at_sensitivity_37: 1.0000 - 6s/epoch - 33ms/step\n",
      "Epoch 8/100\n",
      "168/168 - 6s - loss: 4.9400e-04 - accuracy: 1.0000 - precision_37: 1.0000 - specificity_at_sensitivity_37: 1.0000 - 6s/epoch - 33ms/step\n",
      "Epoch 9/100\n",
      "168/168 - 6s - loss: 4.2003e-04 - accuracy: 1.0000 - precision_37: 1.0000 - specificity_at_sensitivity_37: 1.0000 - 6s/epoch - 33ms/step\n",
      "Epoch 10/100\n",
      "168/168 - 6s - loss: 3.6774e-04 - accuracy: 1.0000 - precision_37: 1.0000 - specificity_at_sensitivity_37: 1.0000 - 6s/epoch - 33ms/step\n",
      "Epoch 11/100\n",
      "168/168 - 6s - loss: 3.2532e-04 - accuracy: 1.0000 - precision_37: 1.0000 - specificity_at_sensitivity_37: 1.0000 - 6s/epoch - 33ms/step\n",
      "Epoch 12/100\n",
      "168/168 - 6s - loss: 2.4300e-04 - accuracy: 1.0000 - precision_37: 1.0000 - specificity_at_sensitivity_37: 1.0000 - 6s/epoch - 33ms/step\n",
      "Epoch 13/100\n",
      "168/168 - 6s - loss: 8.1141e-05 - accuracy: 1.0000 - precision_37: 1.0000 - specificity_at_sensitivity_37: 1.0000 - 6s/epoch - 33ms/step\n",
      "Epoch 14/100\n",
      "168/168 - 6s - loss: 5.5153e-05 - accuracy: 1.0000 - precision_37: 1.0000 - specificity_at_sensitivity_37: 1.0000 - 6s/epoch - 33ms/step\n",
      "Epoch 15/100\n",
      "168/168 - 6s - loss: 3.4730e-05 - accuracy: 1.0000 - precision_37: 1.0000 - specificity_at_sensitivity_37: 1.0000 - 6s/epoch - 33ms/step\n",
      "Epoch 16/100\n",
      "168/168 - 6s - loss: 2.6966e-05 - accuracy: 1.0000 - precision_37: 1.0000 - specificity_at_sensitivity_37: 1.0000 - 6s/epoch - 34ms/step\n",
      "Score for fold 5: loss of 1.6342321634292603; accuracy of 80.95238208770752%; precision_37: 0.0\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 6 ...\n",
      "Epoch 1/100\n",
      "168/168 - 7s - loss: 23.1158 - accuracy: 0.6190 - precision_38: 0.3333 - specificity_at_sensitivity_38: 0.0000e+00 - 7s/epoch - 39ms/step\n",
      "Epoch 2/100\n",
      "168/168 - 6s - loss: 0.4421 - accuracy: 0.8869 - precision_38: 0.8163 - specificity_at_sensitivity_38: 0.0085 - 6s/epoch - 33ms/step\n",
      "Epoch 3/100\n",
      "168/168 - 6s - loss: 0.0442 - accuracy: 0.9881 - precision_38: 1.0000 - specificity_at_sensitivity_38: 0.9068 - 6s/epoch - 33ms/step\n",
      "Epoch 4/100\n",
      "168/168 - 6s - loss: 0.0017 - accuracy: 1.0000 - precision_38: 1.0000 - specificity_at_sensitivity_38: 1.0000 - 6s/epoch - 33ms/step\n",
      "Epoch 5/100\n",
      "168/168 - 6s - loss: 9.5449e-04 - accuracy: 1.0000 - precision_38: 1.0000 - specificity_at_sensitivity_38: 1.0000 - 6s/epoch - 33ms/step\n",
      "Epoch 6/100\n",
      "168/168 - 6s - loss: 7.1404e-04 - accuracy: 1.0000 - precision_38: 1.0000 - specificity_at_sensitivity_38: 1.0000 - 6s/epoch - 33ms/step\n",
      "Epoch 7/100\n",
      "168/168 - 6s - loss: 5.7798e-04 - accuracy: 1.0000 - precision_38: 1.0000 - specificity_at_sensitivity_38: 1.0000 - 6s/epoch - 33ms/step\n",
      "Epoch 8/100\n",
      "168/168 - 6s - loss: 4.8079e-04 - accuracy: 1.0000 - precision_38: 1.0000 - specificity_at_sensitivity_38: 1.0000 - 6s/epoch - 33ms/step\n",
      "Epoch 9/100\n",
      "168/168 - 6s - loss: 4.0572e-04 - accuracy: 1.0000 - precision_38: 1.0000 - specificity_at_sensitivity_38: 1.0000 - 6s/epoch - 33ms/step\n",
      "Epoch 10/100\n",
      "168/168 - 6s - loss: 3.5415e-04 - accuracy: 1.0000 - precision_38: 1.0000 - specificity_at_sensitivity_38: 1.0000 - 6s/epoch - 33ms/step\n",
      "Epoch 11/100\n",
      "168/168 - 6s - loss: 3.0544e-04 - accuracy: 1.0000 - precision_38: 1.0000 - specificity_at_sensitivity_38: 1.0000 - 6s/epoch - 33ms/step\n",
      "Epoch 12/100\n",
      "168/168 - 6s - loss: 2.6731e-04 - accuracy: 1.0000 - precision_38: 1.0000 - specificity_at_sensitivity_38: 1.0000 - 6s/epoch - 33ms/step\n",
      "Epoch 13/100\n",
      "168/168 - 6s - loss: 2.3403e-04 - accuracy: 1.0000 - precision_38: 1.0000 - specificity_at_sensitivity_38: 1.0000 - 6s/epoch - 33ms/step\n",
      "Epoch 14/100\n",
      "168/168 - 6s - loss: 2.1026e-04 - accuracy: 1.0000 - precision_38: 1.0000 - specificity_at_sensitivity_38: 1.0000 - 6s/epoch - 33ms/step\n",
      "Score for fold 6: loss of 1.259814739227295; accuracy of 71.42857313156128%; precision_38: 0.0\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 7 ...\n",
      "Epoch 1/100\n",
      "168/168 - 7s - loss: 14.6507 - accuracy: 0.6071 - precision_39: 0.3333 - specificity_at_sensitivity_39: 0.0000e+00 - 7s/epoch - 41ms/step\n",
      "Epoch 2/100\n",
      "168/168 - 6s - loss: 0.3119 - accuracy: 0.9048 - precision_39: 0.8431 - specificity_at_sensitivity_39: 0.0256 - 6s/epoch - 34ms/step\n",
      "Epoch 3/100\n",
      "168/168 - 6s - loss: 0.0465 - accuracy: 0.9821 - precision_39: 0.9800 - specificity_at_sensitivity_39: 0.9316 - 6s/epoch - 34ms/step\n",
      "Epoch 4/100\n",
      "168/168 - 6s - loss: 0.0015 - accuracy: 1.0000 - precision_39: 1.0000 - specificity_at_sensitivity_39: 1.0000 - 6s/epoch - 33ms/step\n",
      "Epoch 5/100\n",
      "168/168 - 6s - loss: 9.7743e-04 - accuracy: 1.0000 - precision_39: 1.0000 - specificity_at_sensitivity_39: 1.0000 - 6s/epoch - 34ms/step\n",
      "Epoch 6/100\n",
      "168/168 - 6s - loss: 7.7003e-04 - accuracy: 1.0000 - precision_39: 1.0000 - specificity_at_sensitivity_39: 1.0000 - 6s/epoch - 34ms/step\n",
      "Epoch 7/100\n",
      "168/168 - 6s - loss: 6.3752e-04 - accuracy: 1.0000 - precision_39: 1.0000 - specificity_at_sensitivity_39: 1.0000 - 6s/epoch - 33ms/step\n",
      "Epoch 8/100\n",
      "168/168 - 6s - loss: 5.3211e-04 - accuracy: 1.0000 - precision_39: 1.0000 - specificity_at_sensitivity_39: 1.0000 - 6s/epoch - 33ms/step\n",
      "Epoch 9/100\n",
      "168/168 - 6s - loss: 4.5669e-04 - accuracy: 1.0000 - precision_39: 1.0000 - specificity_at_sensitivity_39: 1.0000 - 6s/epoch - 34ms/step\n",
      "Epoch 10/100\n",
      "168/168 - 6s - loss: 4.0090e-04 - accuracy: 1.0000 - precision_39: 1.0000 - specificity_at_sensitivity_39: 1.0000 - 6s/epoch - 34ms/step\n",
      "Epoch 11/100\n",
      "168/168 - 6s - loss: 3.4799e-04 - accuracy: 1.0000 - precision_39: 1.0000 - specificity_at_sensitivity_39: 1.0000 - 6s/epoch - 34ms/step\n",
      "Epoch 12/100\n",
      "168/168 - 6s - loss: 3.0647e-04 - accuracy: 1.0000 - precision_39: 1.0000 - specificity_at_sensitivity_39: 1.0000 - 6s/epoch - 34ms/step\n",
      "Epoch 13/100\n",
      "168/168 - 6s - loss: 2.7557e-04 - accuracy: 1.0000 - precision_39: 1.0000 - specificity_at_sensitivity_39: 1.0000 - 6s/epoch - 33ms/step\n",
      "Epoch 14/100\n",
      "168/168 - 6s - loss: 2.4394e-04 - accuracy: 1.0000 - precision_39: 1.0000 - specificity_at_sensitivity_39: 1.0000 - 6s/epoch - 34ms/step\n",
      "Epoch 15/100\n",
      "168/168 - 6s - loss: 2.1909e-04 - accuracy: 1.0000 - precision_39: 1.0000 - specificity_at_sensitivity_39: 1.0000 - 6s/epoch - 33ms/step\n",
      "Epoch 16/100\n",
      "168/168 - 6s - loss: 1.9727e-04 - accuracy: 1.0000 - precision_39: 1.0000 - specificity_at_sensitivity_39: 1.0000 - 6s/epoch - 34ms/step\n",
      "Score for fold 7: loss of 1.1738046407699585; accuracy of 76.1904776096344%; precision_39: 0.0\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 8 ...\n",
      "Epoch 1/100\n",
      "168/168 - 6s - loss: 48.7227 - accuracy: 0.5833 - precision_40: 0.3469 - specificity_at_sensitivity_40: 0.0000e+00 - 6s/epoch - 38ms/step\n",
      "Epoch 2/100\n",
      "168/168 - 6s - loss: 0.2307 - accuracy: 0.9167 - precision_40: 0.8868 - specificity_at_sensitivity_40: 0.0177 - 6s/epoch - 33ms/step\n",
      "Epoch 3/100\n",
      "168/168 - 6s - loss: 0.0150 - accuracy: 1.0000 - precision_40: 1.0000 - specificity_at_sensitivity_40: 1.0000 - 6s/epoch - 33ms/step\n",
      "Epoch 4/100\n",
      "168/168 - 6s - loss: 0.0025 - accuracy: 1.0000 - precision_40: 1.0000 - specificity_at_sensitivity_40: 1.0000 - 6s/epoch - 33ms/step\n",
      "Epoch 5/100\n",
      "168/168 - 6s - loss: 0.0015 - accuracy: 1.0000 - precision_40: 1.0000 - specificity_at_sensitivity_40: 1.0000 - 6s/epoch - 33ms/step\n",
      "Epoch 6/100\n",
      "168/168 - 6s - loss: 0.0011 - accuracy: 1.0000 - precision_40: 1.0000 - specificity_at_sensitivity_40: 1.0000 - 6s/epoch - 33ms/step\n",
      "Epoch 7/100\n",
      "168/168 - 6s - loss: 9.5000e-04 - accuracy: 1.0000 - precision_40: 1.0000 - specificity_at_sensitivity_40: 1.0000 - 6s/epoch - 33ms/step\n",
      "Epoch 8/100\n",
      "168/168 - 6s - loss: 8.1164e-04 - accuracy: 1.0000 - precision_40: 1.0000 - specificity_at_sensitivity_40: 1.0000 - 6s/epoch - 33ms/step\n",
      "Epoch 9/100\n",
      "168/168 - 6s - loss: 7.0633e-04 - accuracy: 1.0000 - precision_40: 1.0000 - specificity_at_sensitivity_40: 1.0000 - 6s/epoch - 33ms/step\n",
      "Epoch 10/100\n",
      "168/168 - 6s - loss: 6.1799e-04 - accuracy: 1.0000 - precision_40: 1.0000 - specificity_at_sensitivity_40: 1.0000 - 6s/epoch - 33ms/step\n",
      "Epoch 11/100\n",
      "168/168 - 6s - loss: 5.4149e-04 - accuracy: 1.0000 - precision_40: 1.0000 - specificity_at_sensitivity_40: 1.0000 - 6s/epoch - 33ms/step\n",
      "Epoch 12/100\n",
      "168/168 - 6s - loss: 4.8272e-04 - accuracy: 1.0000 - precision_40: 1.0000 - specificity_at_sensitivity_40: 1.0000 - 6s/epoch - 33ms/step\n",
      "Epoch 13/100\n",
      "168/168 - 6s - loss: 4.2913e-04 - accuracy: 1.0000 - precision_40: 1.0000 - specificity_at_sensitivity_40: 1.0000 - 6s/epoch - 33ms/step\n",
      "Epoch 14/100\n",
      "168/168 - 6s - loss: 3.8500e-04 - accuracy: 1.0000 - precision_40: 1.0000 - specificity_at_sensitivity_40: 1.0000 - 6s/epoch - 33ms/step\n",
      "Epoch 15/100\n",
      "168/168 - 6s - loss: 3.4891e-04 - accuracy: 1.0000 - precision_40: 1.0000 - specificity_at_sensitivity_40: 1.0000 - 6s/epoch - 33ms/step\n",
      "Epoch 16/100\n",
      "168/168 - 6s - loss: 3.1620e-04 - accuracy: 1.0000 - precision_40: 1.0000 - specificity_at_sensitivity_40: 1.0000 - 6s/epoch - 33ms/step\n",
      "Epoch 17/100\n",
      "168/168 - 6s - loss: 2.8248e-04 - accuracy: 1.0000 - precision_40: 1.0000 - specificity_at_sensitivity_40: 1.0000 - 6s/epoch - 33ms/step\n",
      "Epoch 18/100\n",
      "168/168 - 6s - loss: 2.5902e-04 - accuracy: 1.0000 - precision_40: 1.0000 - specificity_at_sensitivity_40: 1.0000 - 6s/epoch - 33ms/step\n",
      "Epoch 19/100\n",
      "168/168 - 6s - loss: 2.3442e-04 - accuracy: 1.0000 - precision_40: 1.0000 - specificity_at_sensitivity_40: 1.0000 - 6s/epoch - 33ms/step\n",
      "Score for fold 8: loss of 0.3399973511695862; accuracy of 90.47619104385376%; precision_40: 0.0\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 9 ...\n",
      "Epoch 1/100\n",
      "168/168 - 1779s - loss: 23.1880 - accuracy: 0.6310 - precision_41: 0.3000 - specificity_at_sensitivity_41: 0.0000e+00 - 1779s/epoch - 11s/step\n",
      "Epoch 2/100\n",
      "168/168 - 18s - loss: 0.1719 - accuracy: 0.9405 - precision_41: 0.9091 - specificity_at_sensitivity_41: 0.6721 - 18s/epoch - 107ms/step\n",
      "Epoch 3/100\n",
      "168/168 - 16s - loss: 0.0156 - accuracy: 0.9881 - precision_41: 0.9783 - specificity_at_sensitivity_41: 0.9918 - 16s/epoch - 94ms/step\n",
      "Epoch 4/100\n",
      "168/168 - 14s - loss: 0.0025 - accuracy: 1.0000 - precision_41: 1.0000 - specificity_at_sensitivity_41: 1.0000 - 14s/epoch - 81ms/step\n",
      "Epoch 5/100\n",
      "168/168 - 12s - loss: 7.7250e-04 - accuracy: 1.0000 - precision_41: 1.0000 - specificity_at_sensitivity_41: 1.0000 - 12s/epoch - 70ms/step\n",
      "Epoch 6/100\n",
      "168/168 - 12s - loss: 6.1101e-04 - accuracy: 1.0000 - precision_41: 1.0000 - specificity_at_sensitivity_41: 1.0000 - 12s/epoch - 69ms/step\n",
      "Epoch 7/100\n",
      "168/168 - 12s - loss: 4.9712e-04 - accuracy: 1.0000 - precision_41: 1.0000 - specificity_at_sensitivity_41: 1.0000 - 12s/epoch - 70ms/step\n",
      "Epoch 8/100\n",
      "168/168 - 11s - loss: 4.2045e-04 - accuracy: 1.0000 - precision_41: 1.0000 - specificity_at_sensitivity_41: 1.0000 - 11s/epoch - 68ms/step\n",
      "Epoch 9/100\n",
      "168/168 - 12s - loss: 3.6236e-04 - accuracy: 1.0000 - precision_41: 1.0000 - specificity_at_sensitivity_41: 1.0000 - 12s/epoch - 69ms/step\n",
      "Epoch 10/100\n",
      "168/168 - 12s - loss: 3.1759e-04 - accuracy: 1.0000 - precision_41: 1.0000 - specificity_at_sensitivity_41: 1.0000 - 12s/epoch - 71ms/step\n",
      "Epoch 11/100\n",
      "168/168 - 12s - loss: 2.7913e-04 - accuracy: 1.0000 - precision_41: 1.0000 - specificity_at_sensitivity_41: 1.0000 - 12s/epoch - 69ms/step\n",
      "Epoch 12/100\n",
      "168/168 - 12s - loss: 2.4956e-04 - accuracy: 1.0000 - precision_41: 1.0000 - specificity_at_sensitivity_41: 1.0000 - 12s/epoch - 70ms/step\n",
      "Epoch 13/100\n",
      "168/168 - 11s - loss: 2.1969e-04 - accuracy: 1.0000 - precision_41: 1.0000 - specificity_at_sensitivity_41: 1.0000 - 11s/epoch - 68ms/step\n",
      "Epoch 14/100\n",
      "168/168 - 12s - loss: 1.9791e-04 - accuracy: 1.0000 - precision_41: 1.0000 - specificity_at_sensitivity_41: 1.0000 - 12s/epoch - 71ms/step\n",
      "Epoch 15/100\n",
      "168/168 - 11s - loss: 1.7900e-04 - accuracy: 1.0000 - precision_41: 1.0000 - specificity_at_sensitivity_41: 1.0000 - 11s/epoch - 68ms/step\n",
      "Score for fold 9: loss of 2.9799411296844482; accuracy of 52.3809552192688%; precision_41: 0.0\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ...\n",
      "Epoch 1/100\n",
      "170/170 - 13s - loss: 9.0333 - accuracy: 0.5647 - precision_42: 0.2800 - specificity_at_sensitivity_42: 0.0000e+00 - 13s/epoch - 74ms/step\n",
      "Epoch 2/100\n",
      "170/170 - 11s - loss: 0.2875 - accuracy: 0.8882 - precision_42: 0.8511 - specificity_at_sensitivity_42: 0.0085 - 11s/epoch - 67ms/step\n",
      "Epoch 3/100\n",
      "170/170 - 12s - loss: 0.0243 - accuracy: 0.9941 - precision_42: 0.9811 - specificity_at_sensitivity_42: 0.9915 - 12s/epoch - 71ms/step\n",
      "Epoch 4/100\n",
      "170/170 - 12s - loss: 0.0195 - accuracy: 0.9941 - precision_42: 1.0000 - specificity_at_sensitivity_42: 0.9661 - 12s/epoch - 72ms/step\n",
      "Epoch 5/100\n",
      "170/170 - 12s - loss: 8.2541e-04 - accuracy: 1.0000 - precision_42: 1.0000 - specificity_at_sensitivity_42: 1.0000 - 12s/epoch - 69ms/step\n",
      "Epoch 6/100\n",
      "170/170 - 12s - loss: 5.8317e-04 - accuracy: 1.0000 - precision_42: 1.0000 - specificity_at_sensitivity_42: 1.0000 - 12s/epoch - 72ms/step\n",
      "Epoch 7/100\n",
      "170/170 - 12s - loss: 4.8431e-04 - accuracy: 1.0000 - precision_42: 1.0000 - specificity_at_sensitivity_42: 1.0000 - 12s/epoch - 72ms/step\n",
      "Epoch 8/100\n",
      "170/170 - 12s - loss: 4.1411e-04 - accuracy: 1.0000 - precision_42: 1.0000 - specificity_at_sensitivity_42: 1.0000 - 12s/epoch - 72ms/step\n",
      "Epoch 9/100\n",
      "170/170 - 12s - loss: 3.5136e-04 - accuracy: 1.0000 - precision_42: 1.0000 - specificity_at_sensitivity_42: 1.0000 - 12s/epoch - 71ms/step\n",
      "Epoch 10/100\n",
      "170/170 - 12s - loss: 3.0811e-04 - accuracy: 1.0000 - precision_42: 1.0000 - specificity_at_sensitivity_42: 1.0000 - 12s/epoch - 70ms/step\n",
      "Epoch 11/100\n",
      "170/170 - 12s - loss: 2.6994e-04 - accuracy: 1.0000 - precision_42: 1.0000 - specificity_at_sensitivity_42: 1.0000 - 12s/epoch - 69ms/step\n",
      "Epoch 12/100\n",
      "170/170 - 12s - loss: 2.3722e-04 - accuracy: 1.0000 - precision_42: 1.0000 - specificity_at_sensitivity_42: 1.0000 - 12s/epoch - 71ms/step\n",
      "Epoch 13/100\n",
      "170/170 - 12s - loss: 2.1112e-04 - accuracy: 1.0000 - precision_42: 1.0000 - specificity_at_sensitivity_42: 1.0000 - 12s/epoch - 68ms/step\n",
      "Score for fold 1: loss of 1.283052682876587; accuracy of 73.68420958518982%; precision_42: 0.0\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ...\n",
      "Epoch 1/100\n",
      "170/170 - 12s - loss: 6.6208 - accuracy: 0.5529 - precision_43: 0.2245 - specificity_at_sensitivity_43: 0.0000e+00 - 12s/epoch - 73ms/step\n",
      "Epoch 2/100\n",
      "170/170 - 11s - loss: 0.2761 - accuracy: 0.9353 - precision_43: 0.9130 - specificity_at_sensitivity_43: 0.1157 - 11s/epoch - 67ms/step\n",
      "Epoch 3/100\n",
      "170/170 - 12s - loss: 0.0269 - accuracy: 0.9882 - precision_43: 0.9796 - specificity_at_sensitivity_43: 0.9752 - 12s/epoch - 68ms/step\n",
      "Epoch 4/100\n",
      "170/170 - 11s - loss: 3.8702e-04 - accuracy: 1.0000 - precision_43: 1.0000 - specificity_at_sensitivity_43: 1.0000 - 11s/epoch - 67ms/step\n",
      "Epoch 5/100\n",
      "170/170 - 11s - loss: 2.2604e-04 - accuracy: 1.0000 - precision_43: 1.0000 - specificity_at_sensitivity_43: 1.0000 - 11s/epoch - 67ms/step\n",
      "Epoch 6/100\n",
      "170/170 - 12s - loss: 1.8156e-04 - accuracy: 1.0000 - precision_43: 1.0000 - specificity_at_sensitivity_43: 1.0000 - 12s/epoch - 68ms/step\n",
      "Epoch 7/100\n",
      "170/170 - 13s - loss: 1.5075e-04 - accuracy: 1.0000 - precision_43: 1.0000 - specificity_at_sensitivity_43: 1.0000 - 13s/epoch - 75ms/step\n",
      "Epoch 8/100\n",
      "170/170 - 12s - loss: 1.3020e-04 - accuracy: 1.0000 - precision_43: 1.0000 - specificity_at_sensitivity_43: 1.0000 - 12s/epoch - 69ms/step\n",
      "Score for fold 2: loss of 2.3208484649658203; accuracy of 63.15789222717285%; precision_43: 0.0\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 3 ...\n",
      "Epoch 1/100\n",
      "170/170 - 12s - loss: 29.2075 - accuracy: 0.6471 - precision_44: 0.3958 - specificity_at_sensitivity_44: 0.0000e+00 - 12s/epoch - 73ms/step\n",
      "Epoch 2/100\n",
      "170/170 - 12s - loss: 0.4770 - accuracy: 0.8882 - precision_44: 0.8039 - specificity_at_sensitivity_44: 0.1333 - 12s/epoch - 69ms/step\n",
      "Epoch 3/100\n",
      "170/170 - 12s - loss: 0.1237 - accuracy: 0.9529 - precision_44: 0.9375 - specificity_at_sensitivity_44: 0.2000 - 12s/epoch - 70ms/step\n",
      "Epoch 4/100\n",
      "170/170 - 12s - loss: 0.2955 - accuracy: 0.9412 - precision_44: 0.9000 - specificity_at_sensitivity_44: 0.3333 - 12s/epoch - 71ms/step\n",
      "Epoch 5/100\n",
      "170/170 - 12s - loss: 1.5496e-04 - accuracy: 1.0000 - precision_44: 1.0000 - specificity_at_sensitivity_44: 1.0000 - 12s/epoch - 69ms/step\n",
      "Epoch 6/100\n",
      "170/170 - 12s - loss: 6.1924e-05 - accuracy: 1.0000 - precision_44: 1.0000 - specificity_at_sensitivity_44: 1.0000 - 12s/epoch - 68ms/step\n",
      "Epoch 7/100\n",
      "170/170 - 12s - loss: 4.6254e-05 - accuracy: 1.0000 - precision_44: 1.0000 - specificity_at_sensitivity_44: 1.0000 - 12s/epoch - 69ms/step\n",
      "Epoch 8/100\n",
      "170/170 - 12s - loss: 3.8033e-05 - accuracy: 1.0000 - precision_44: 1.0000 - specificity_at_sensitivity_44: 1.0000 - 12s/epoch - 69ms/step\n",
      "Epoch 9/100\n",
      "170/170 - 12s - loss: 3.2880e-05 - accuracy: 1.0000 - precision_44: 1.0000 - specificity_at_sensitivity_44: 1.0000 - 12s/epoch - 70ms/step\n",
      "Epoch 10/100\n",
      "170/170 - 11s - loss: 2.9108e-05 - accuracy: 1.0000 - precision_44: 1.0000 - specificity_at_sensitivity_44: 1.0000 - 11s/epoch - 67ms/step\n",
      "Score for fold 3: loss of 4.2051262855529785; accuracy of 68.42105388641357%; precision_44: 0.0\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 4 ...\n",
      "Epoch 1/100\n",
      "170/170 - 13s - loss: 18.0466 - accuracy: 0.5824 - precision_45: 0.3000 - specificity_at_sensitivity_45: 0.0000e+00 - 13s/epoch - 79ms/step\n",
      "Epoch 2/100\n",
      "170/170 - 12s - loss: 0.2897 - accuracy: 0.9000 - precision_45: 0.8269 - specificity_at_sensitivity_45: 0.0000e+00 - 12s/epoch - 69ms/step\n",
      "Epoch 3/100\n",
      "170/170 - 12s - loss: 0.0329 - accuracy: 0.9882 - precision_45: 1.0000 - specificity_at_sensitivity_45: 0.9664 - 12s/epoch - 69ms/step\n",
      "Epoch 4/100\n",
      "170/170 - 12s - loss: 0.0014 - accuracy: 1.0000 - precision_45: 1.0000 - specificity_at_sensitivity_45: 1.0000 - 12s/epoch - 69ms/step\n",
      "Epoch 5/100\n",
      "170/170 - 12s - loss: 6.0183e-04 - accuracy: 1.0000 - precision_45: 1.0000 - specificity_at_sensitivity_45: 1.0000 - 12s/epoch - 70ms/step\n",
      "Epoch 6/100\n",
      "170/170 - 12s - loss: 4.1841e-04 - accuracy: 1.0000 - precision_45: 1.0000 - specificity_at_sensitivity_45: 1.0000 - 12s/epoch - 69ms/step\n",
      "Epoch 7/100\n",
      "170/170 - 12s - loss: 3.4502e-04 - accuracy: 1.0000 - precision_45: 1.0000 - specificity_at_sensitivity_45: 1.0000 - 12s/epoch - 71ms/step\n",
      "Epoch 8/100\n",
      "170/170 - 12s - loss: 2.9154e-04 - accuracy: 1.0000 - precision_45: 1.0000 - specificity_at_sensitivity_45: 1.0000 - 12s/epoch - 68ms/step\n",
      "Epoch 9/100\n",
      "170/170 - 12s - loss: 2.5468e-04 - accuracy: 1.0000 - precision_45: 1.0000 - specificity_at_sensitivity_45: 1.0000 - 12s/epoch - 72ms/step\n",
      "Epoch 10/100\n",
      "170/170 - 12s - loss: 2.2195e-04 - accuracy: 1.0000 - precision_45: 1.0000 - specificity_at_sensitivity_45: 1.0000 - 12s/epoch - 69ms/step\n",
      "Epoch 11/100\n",
      "170/170 - 12s - loss: 1.9545e-04 - accuracy: 1.0000 - precision_45: 1.0000 - specificity_at_sensitivity_45: 1.0000 - 12s/epoch - 68ms/step\n",
      "Score for fold 4: loss of 1.573012351989746; accuracy of 78.94737124443054%; precision_45: 1.0\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 5 ...\n",
      "Epoch 1/100\n",
      "170/170 - 13s - loss: 10.6106 - accuracy: 0.5294 - precision_46: 0.2157 - specificity_at_sensitivity_46: 0.0000e+00 - 13s/epoch - 76ms/step\n",
      "Epoch 2/100\n",
      "170/170 - 12s - loss: 0.6447 - accuracy: 0.9118 - precision_46: 0.8913 - specificity_at_sensitivity_46: 0.0252 - 12s/epoch - 69ms/step\n",
      "Epoch 3/100\n",
      "170/170 - 12s - loss: 0.0190 - accuracy: 1.0000 - precision_46: 1.0000 - specificity_at_sensitivity_46: 1.0000 - 12s/epoch - 73ms/step\n",
      "Epoch 4/100\n",
      "170/170 - 12s - loss: 0.6558 - accuracy: 0.9882 - precision_46: 0.9804 - specificity_at_sensitivity_46: 0.9916 - 12s/epoch - 68ms/step\n",
      "Epoch 5/100\n",
      "170/170 - 11s - loss: 3.1721e-04 - accuracy: 1.0000 - precision_46: 1.0000 - specificity_at_sensitivity_46: 1.0000 - 11s/epoch - 67ms/step\n",
      "Epoch 6/100\n",
      "170/170 - 12s - loss: 6.2934e-05 - accuracy: 1.0000 - precision_46: 1.0000 - specificity_at_sensitivity_46: 1.0000 - 12s/epoch - 70ms/step\n",
      "Epoch 7/100\n",
      "170/170 - 12s - loss: 4.2064e-05 - accuracy: 1.0000 - precision_46: 1.0000 - specificity_at_sensitivity_46: 1.0000 - 12s/epoch - 68ms/step\n",
      "Epoch 8/100\n",
      "170/170 - 12s - loss: 3.1624e-05 - accuracy: 1.0000 - precision_46: 1.0000 - specificity_at_sensitivity_46: 1.0000 - 12s/epoch - 69ms/step\n",
      "Epoch 9/100\n",
      "170/170 - 12s - loss: 2.4731e-05 - accuracy: 1.0000 - precision_46: 1.0000 - specificity_at_sensitivity_46: 1.0000 - 12s/epoch - 70ms/step\n",
      "Score for fold 5: loss of 2.278238296508789; accuracy of 73.68420958518982%; precision_46: 0.0\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 6 ...\n",
      "Epoch 1/100\n",
      "170/170 - 13s - loss: 46.9188 - accuracy: 0.6294 - precision_47: 0.3860 - specificity_at_sensitivity_47: 0.0000e+00 - 13s/epoch - 75ms/step\n",
      "Epoch 2/100\n",
      "170/170 - 12s - loss: 0.4640 - accuracy: 0.8824 - precision_47: 0.8125 - specificity_at_sensitivity_47: 0.0500 - 12s/epoch - 69ms/step\n",
      "Epoch 3/100\n",
      "170/170 - 12s - loss: 0.0968 - accuracy: 0.9588 - precision_47: 0.9388 - specificity_at_sensitivity_47: 0.9333 - 12s/epoch - 69ms/step\n",
      "Epoch 4/100\n",
      "170/170 - 12s - loss: 0.0101 - accuracy: 0.9941 - precision_47: 1.0000 - specificity_at_sensitivity_47: 1.0000 - 12s/epoch - 69ms/step\n",
      "Epoch 5/100\n",
      "170/170 - 12s - loss: 0.0012 - accuracy: 1.0000 - precision_47: 1.0000 - specificity_at_sensitivity_47: 1.0000 - 12s/epoch - 71ms/step\n",
      "Epoch 6/100\n",
      "170/170 - 12s - loss: 5.6261e-04 - accuracy: 1.0000 - precision_47: 1.0000 - specificity_at_sensitivity_47: 1.0000 - 12s/epoch - 68ms/step\n",
      "Epoch 7/100\n",
      "170/170 - 12s - loss: 4.4828e-04 - accuracy: 1.0000 - precision_47: 1.0000 - specificity_at_sensitivity_47: 1.0000 - 12s/epoch - 73ms/step\n",
      "Epoch 8/100\n",
      "170/170 - 12s - loss: 3.7462e-04 - accuracy: 1.0000 - precision_47: 1.0000 - specificity_at_sensitivity_47: 1.0000 - 12s/epoch - 70ms/step\n",
      "Epoch 9/100\n",
      "170/170 - 12s - loss: 3.2938e-04 - accuracy: 1.0000 - precision_47: 1.0000 - specificity_at_sensitivity_47: 1.0000 - 12s/epoch - 68ms/step\n",
      "Epoch 10/100\n",
      "170/170 - 12s - loss: 2.8843e-04 - accuracy: 1.0000 - precision_47: 1.0000 - specificity_at_sensitivity_47: 1.0000 - 12s/epoch - 68ms/step\n",
      "Epoch 11/100\n",
      "170/170 - 12s - loss: 2.5804e-04 - accuracy: 1.0000 - precision_47: 1.0000 - specificity_at_sensitivity_47: 1.0000 - 12s/epoch - 68ms/step\n",
      "Epoch 12/100\n",
      "170/170 - 12s - loss: 2.2945e-04 - accuracy: 1.0000 - precision_47: 1.0000 - specificity_at_sensitivity_47: 1.0000 - 12s/epoch - 69ms/step\n",
      "Score for fold 6: loss of 1.7187522649765015; accuracy of 68.42105388641357%; precision_47: 0.0\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 7 ...\n",
      "Epoch 1/100\n",
      "170/170 - 13s - loss: 7.7630 - accuracy: 0.6000 - precision_48: 0.3061 - specificity_at_sensitivity_48: 0.0000e+00 - 13s/epoch - 75ms/step\n",
      "Epoch 2/100\n",
      "170/170 - 12s - loss: 0.9340 - accuracy: 0.8706 - precision_48: 0.8140 - specificity_at_sensitivity_48: 0.0165 - 12s/epoch - 69ms/step\n",
      "Epoch 3/100\n",
      "170/170 - 12s - loss: 0.0677 - accuracy: 0.9765 - precision_48: 0.9592 - specificity_at_sensitivity_48: 0.9256 - 12s/epoch - 70ms/step\n",
      "Epoch 4/100\n",
      "170/170 - 12s - loss: 0.0018 - accuracy: 1.0000 - precision_48: 1.0000 - specificity_at_sensitivity_48: 1.0000 - 12s/epoch - 69ms/step\n",
      "Epoch 5/100\n",
      "170/170 - 12s - loss: 0.0011 - accuracy: 1.0000 - precision_48: 1.0000 - specificity_at_sensitivity_48: 1.0000 - 12s/epoch - 68ms/step\n",
      "Epoch 6/100\n",
      "170/170 - 12s - loss: 6.0224e-04 - accuracy: 1.0000 - precision_48: 1.0000 - specificity_at_sensitivity_48: 1.0000 - 12s/epoch - 71ms/step\n",
      "Epoch 7/100\n",
      "170/170 - 12s - loss: 4.9696e-04 - accuracy: 1.0000 - precision_48: 1.0000 - specificity_at_sensitivity_48: 1.0000 - 12s/epoch - 69ms/step\n",
      "Epoch 8/100\n",
      "170/170 - 12s - loss: 4.3967e-04 - accuracy: 1.0000 - precision_48: 1.0000 - specificity_at_sensitivity_48: 1.0000 - 12s/epoch - 71ms/step\n",
      "Epoch 9/100\n",
      "170/170 - 12s - loss: 3.7293e-04 - accuracy: 1.0000 - precision_48: 1.0000 - specificity_at_sensitivity_48: 1.0000 - 12s/epoch - 70ms/step\n",
      "Epoch 10/100\n",
      "170/170 - 12s - loss: 3.2849e-04 - accuracy: 1.0000 - precision_48: 1.0000 - specificity_at_sensitivity_48: 1.0000 - 12s/epoch - 71ms/step\n",
      "Epoch 11/100\n",
      "170/170 - 12s - loss: 2.8595e-04 - accuracy: 1.0000 - precision_48: 1.0000 - specificity_at_sensitivity_48: 1.0000 - 12s/epoch - 69ms/step\n",
      "Epoch 12/100\n",
      "170/170 - 12s - loss: 2.5400e-04 - accuracy: 1.0000 - precision_48: 1.0000 - specificity_at_sensitivity_48: 1.0000 - 12s/epoch - 69ms/step\n",
      "Epoch 13/100\n",
      "170/170 - 12s - loss: 2.2773e-04 - accuracy: 1.0000 - precision_48: 1.0000 - specificity_at_sensitivity_48: 1.0000 - 12s/epoch - 70ms/step\n",
      "Epoch 14/100\n",
      "170/170 - 12s - loss: 2.0324e-04 - accuracy: 1.0000 - precision_48: 1.0000 - specificity_at_sensitivity_48: 1.0000 - 12s/epoch - 69ms/step\n",
      "Epoch 15/100\n",
      "170/170 - 12s - loss: 1.8438e-04 - accuracy: 1.0000 - precision_48: 1.0000 - specificity_at_sensitivity_48: 1.0000 - 12s/epoch - 71ms/step\n",
      "Score for fold 7: loss of 2.0397565364837646; accuracy of 63.15789222717285%; precision_48: 0.0\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 8 ...\n",
      "Epoch 1/100\n",
      "170/170 - 13s - loss: 10.9872 - accuracy: 0.5824 - precision_49: 0.3273 - specificity_at_sensitivity_49: 0.0000e+00 - 13s/epoch - 74ms/step\n",
      "Epoch 2/100\n",
      "170/170 - 12s - loss: 0.6228 - accuracy: 0.8294 - precision_49: 0.7170 - specificity_at_sensitivity_49: 0.0085 - 12s/epoch - 71ms/step\n",
      "Epoch 3/100\n",
      "170/170 - 12s - loss: 0.0155 - accuracy: 0.9941 - precision_49: 0.9811 - specificity_at_sensitivity_49: 0.9915 - 12s/epoch - 68ms/step\n",
      "Epoch 4/100\n",
      "170/170 - 12s - loss: 0.0012 - accuracy: 1.0000 - precision_49: 1.0000 - specificity_at_sensitivity_49: 1.0000 - 12s/epoch - 70ms/step\n",
      "Epoch 5/100\n",
      "170/170 - 12s - loss: 3.7378e-04 - accuracy: 1.0000 - precision_49: 1.0000 - specificity_at_sensitivity_49: 1.0000 - 12s/epoch - 68ms/step\n",
      "Epoch 6/100\n",
      "170/170 - 12s - loss: 2.9110e-04 - accuracy: 1.0000 - precision_49: 1.0000 - specificity_at_sensitivity_49: 1.0000 - 12s/epoch - 71ms/step\n",
      "Epoch 7/100\n",
      "170/170 - 12s - loss: 2.4315e-04 - accuracy: 1.0000 - precision_49: 1.0000 - specificity_at_sensitivity_49: 1.0000 - 12s/epoch - 70ms/step\n",
      "Epoch 8/100\n",
      "170/170 - 12s - loss: 2.0347e-04 - accuracy: 1.0000 - precision_49: 1.0000 - specificity_at_sensitivity_49: 1.0000 - 12s/epoch - 70ms/step\n",
      "Epoch 9/100\n",
      "170/170 - 12s - loss: 1.6880e-04 - accuracy: 1.0000 - precision_49: 1.0000 - specificity_at_sensitivity_49: 1.0000 - 12s/epoch - 70ms/step\n",
      "Epoch 10/100\n",
      "170/170 - 12s - loss: 1.4750e-04 - accuracy: 1.0000 - precision_49: 1.0000 - specificity_at_sensitivity_49: 1.0000 - 12s/epoch - 69ms/step\n",
      "Score for fold 8: loss of 1.1825109720230103; accuracy of 78.94737124443054%; precision_49: 0.0\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 9 ...\n",
      "Epoch 1/100\n",
      "170/170 - 13s - loss: 10.0880 - accuracy: 0.5824 - precision_50: 0.2667 - specificity_at_sensitivity_50: 0.0000e+00 - 13s/epoch - 74ms/step\n",
      "Epoch 2/100\n",
      "170/170 - 12s - loss: 0.4309 - accuracy: 0.8941 - precision_50: 0.8077 - specificity_at_sensitivity_50: 0.0000e+00 - 12s/epoch - 69ms/step\n",
      "Epoch 3/100\n",
      "170/170 - 12s - loss: 0.0590 - accuracy: 0.9765 - precision_50: 0.9792 - specificity_at_sensitivity_50: 0.9417 - 12s/epoch - 70ms/step\n",
      "Epoch 4/100\n",
      "170/170 - 11s - loss: 0.0031 - accuracy: 1.0000 - precision_50: 1.0000 - specificity_at_sensitivity_50: 1.0000 - 11s/epoch - 67ms/step\n",
      "Epoch 5/100\n",
      "170/170 - 12s - loss: 3.7846e-04 - accuracy: 1.0000 - precision_50: 1.0000 - specificity_at_sensitivity_50: 1.0000 - 12s/epoch - 69ms/step\n",
      "Epoch 6/100\n",
      "170/170 - 12s - loss: 2.8610e-04 - accuracy: 1.0000 - precision_50: 1.0000 - specificity_at_sensitivity_50: 1.0000 - 12s/epoch - 68ms/step\n",
      "Epoch 7/100\n",
      "170/170 - 12s - loss: 2.3819e-04 - accuracy: 1.0000 - precision_50: 1.0000 - specificity_at_sensitivity_50: 1.0000 - 12s/epoch - 68ms/step\n",
      "Epoch 8/100\n",
      "170/170 - 12s - loss: 2.0517e-04 - accuracy: 1.0000 - precision_50: 1.0000 - specificity_at_sensitivity_50: 1.0000 - 12s/epoch - 68ms/step\n",
      "Epoch 9/100\n",
      "170/170 - 12s - loss: 1.7651e-04 - accuracy: 1.0000 - precision_50: 1.0000 - specificity_at_sensitivity_50: 1.0000 - 12s/epoch - 69ms/step\n",
      "Epoch 10/100\n",
      "170/170 - 13s - loss: 1.5274e-04 - accuracy: 1.0000 - precision_50: 1.0000 - specificity_at_sensitivity_50: 1.0000 - 13s/epoch - 74ms/step\n",
      "Score for fold 9: loss of 3.1383893489837646; accuracy of 68.42105388641357%; precision_50: 0.0\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 10 ...\n",
      "Epoch 1/100\n",
      "171/171 - 13s - loss: 21.9764 - accuracy: 0.5731 - precision_51: 0.2830 - specificity_at_sensitivity_51: 0.0000e+00 - 13s/epoch - 74ms/step\n",
      "Epoch 2/100\n",
      "171/171 - 12s - loss: 0.3731 - accuracy: 0.8947 - precision_51: 0.8077 - specificity_at_sensitivity_51: 0.0083 - 12s/epoch - 68ms/step\n",
      "Epoch 3/100\n",
      "171/171 - 12s - loss: 0.0757 - accuracy: 0.9649 - precision_51: 0.9583 - specificity_at_sensitivity_51: 0.9174 - 12s/epoch - 70ms/step\n",
      "Epoch 4/100\n",
      "171/171 - 12s - loss: 0.0063 - accuracy: 1.0000 - precision_51: 1.0000 - specificity_at_sensitivity_51: 1.0000 - 12s/epoch - 69ms/step\n",
      "Epoch 5/100\n",
      "171/171 - 12s - loss: 9.4969e-04 - accuracy: 1.0000 - precision_51: 1.0000 - specificity_at_sensitivity_51: 1.0000 - 12s/epoch - 72ms/step\n",
      "Epoch 6/100\n",
      "171/171 - 12s - loss: 7.0987e-04 - accuracy: 1.0000 - precision_51: 1.0000 - specificity_at_sensitivity_51: 1.0000 - 12s/epoch - 68ms/step\n",
      "Epoch 7/100\n",
      "171/171 - 12s - loss: 5.9951e-04 - accuracy: 1.0000 - precision_51: 1.0000 - specificity_at_sensitivity_51: 1.0000 - 12s/epoch - 71ms/step\n",
      "Epoch 8/100\n",
      "171/171 - 11s - loss: 4.9468e-04 - accuracy: 1.0000 - precision_51: 1.0000 - specificity_at_sensitivity_51: 1.0000 - 11s/epoch - 67ms/step\n",
      "Epoch 9/100\n",
      "171/171 - 12s - loss: 4.3209e-04 - accuracy: 1.0000 - precision_51: 1.0000 - specificity_at_sensitivity_51: 1.0000 - 12s/epoch - 68ms/step\n",
      "Epoch 10/100\n",
      "171/171 - 12s - loss: 3.7959e-04 - accuracy: 1.0000 - precision_51: 1.0000 - specificity_at_sensitivity_51: 1.0000 - 12s/epoch - 69ms/step\n",
      "Epoch 11/100\n",
      "171/171 - 12s - loss: 3.4003e-04 - accuracy: 1.0000 - precision_51: 1.0000 - specificity_at_sensitivity_51: 1.0000 - 12s/epoch - 68ms/step\n",
      "Epoch 12/100\n",
      "171/171 - 12s - loss: 3.0036e-04 - accuracy: 1.0000 - precision_51: 1.0000 - specificity_at_sensitivity_51: 1.0000 - 12s/epoch - 68ms/step\n",
      "Epoch 13/100\n",
      "171/171 - 12s - loss: 2.6722e-04 - accuracy: 1.0000 - precision_51: 1.0000 - specificity_at_sensitivity_51: 1.0000 - 12s/epoch - 72ms/step\n",
      "Epoch 14/100\n",
      "171/171 - 12s - loss: 2.4356e-04 - accuracy: 1.0000 - precision_51: 1.0000 - specificity_at_sensitivity_51: 1.0000 - 12s/epoch - 72ms/step\n",
      "Epoch 15/100\n",
      "171/171 - 11s - loss: 2.1952e-04 - accuracy: 1.0000 - precision_51: 1.0000 - specificity_at_sensitivity_51: 1.0000 - 11s/epoch - 67ms/step\n",
      "Epoch 16/100\n",
      "171/171 - 12s - loss: 1.9981e-04 - accuracy: 1.0000 - precision_51: 1.0000 - specificity_at_sensitivity_51: 1.0000 - 12s/epoch - 70ms/step\n",
      "Score for fold 10: loss of 1.7168699502944946; accuracy of 66.66666865348816%; precision_51: 0.0\n"
     ]
    }
   ],
   "source": [
    "for fold_it in range(3,11):\n",
    "  # Vars to get means\n",
    "  acc_per_fold = []\n",
    "  loss_per_fold = []\n",
    "  precision_per_fold = []\n",
    "\n",
    "  # Hiperparameters\n",
    "  batch_size = 1\n",
    "  no_epochs = 100\n",
    "  verbosity = 2 #1\n",
    "  # Define the K-fold Cross Validator\n",
    "  num_folds = fold_it\n",
    "  kfold = KFold(n_splits=num_folds, shuffle=True)\n",
    "\n",
    "  # K-fold Cross Validation model evaluation for prints\n",
    "  fold_no = 1\n",
    "\n",
    "  for train, test in kfold.split(X_train, y_train):\n",
    "    # Define the model architecture\n",
    "    model = Sequential()\n",
    "    # Convolutional block 1\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(32, kernel_size=(3, 3), padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.01))\n",
    "    # Convolutional block 2\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(8, (3, 3), padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.01))\n",
    "    # Convolutional block 3\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(8, (3, 3), padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.01))\n",
    "    # Convolutional block 4\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(8, (3, 3), padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.01))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dense(64))\n",
    "    model.add(LeakyReLU(alpha=0.01))\n",
    "    model.add(Dense(1, activation='sigmoid') )\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', \n",
    "                loss='binary_crossentropy', \n",
    "                metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.SpecificityAtSensitivity(sensitivity=1)])\n",
    "\n",
    "\n",
    "    # Generate a print\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'Training for fold {fold_no} ...')\n",
    "\n",
    "    callback = tf.keras.callbacks.EarlyStopping(monitor='loss',\n",
    "                                                patience=3,\n",
    "                                                min_delta=0.0001)\n",
    "\n",
    "    # Fit data to model\n",
    "    history = model.fit(X_train[train], y_train[train],\n",
    "                batch_size=batch_size,\n",
    "                epochs=no_epochs,\n",
    "                verbose=verbosity,\n",
    "                callbacks=[callback])\n",
    "\n",
    "    # Generate generalization metrics\n",
    "    scores = model.evaluate(X_train[test], y_train[test], verbose=0)\n",
    "    print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%; {model.metrics_names[2]}: {scores[2]}')\n",
    "    acc_per_fold.append(scores[1] * 100)\n",
    "    loss_per_fold.append(scores[0])\n",
    "    precision_per_fold.append(scores[2])\n",
    "    \n",
    "    if(scores[1]*100 > 60):\n",
    "      folder_path = f'd:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/folds/{num_folds}-fold_11_04_2024/'\n",
    "      if not os.path.exists(folder_path):\n",
    "          os.makedirs(folder_path)\n",
    "      \n",
    "      subfolder_path = folder_path + f'{fold_no}-{scores[1]*100}/'\n",
    "      if not os.path.exists(subfolder_path):\n",
    "          os.makedirs(subfolder_path)\n",
    "\n",
    "      #model.save(f'{folder_path}fold_v5_{fold_no}_{scores[1]*100}.h5')\n",
    "      model.save(subfolder_path + f'fold-{fold_no}.h5')  # Replace with your actual path\n",
    "      df_to_save = new_df_train.iloc[train].copy()\n",
    "      df_to_save.drop('Spectrogram',axis=1, inplace=True)\n",
    "      df_to_save.to_csv(subfolder_path + f'train-data-fold-{fold_no}.csv', index=False)\n",
    "    # Increase fold number\n",
    "    fold_no = fold_no + 1\n",
    "\n",
    "  # Get means\n",
    "  mean_acc = sum(acc_per_fold) / len(acc_per_fold)\n",
    "  mean_loss = sum(loss_per_fold) / len(loss_per_fold)\n",
    "  mean_precision = sum(precision_per_fold) / len(precision_per_fold)\n",
    "  # Append to list to graph\n",
    "  mean_acc_per_fold.append(mean_acc)\n",
    "  mean_loss_per_fold.append(mean_loss)\n",
    "  mean_precision_per_fold.append(mean_precision)\n",
    "  kfold_list.append(num_folds)\n",
    "\n",
    "  with open(folder_path+'score.txt', 'w') as file:\n",
    "        # Write 'hello' to the file\n",
    "        file.write(f' accuracy: {mean_acc}. loss: {mean_loss}. precision: {mean_precision}')\n",
    "  # Plot training accuracy per fold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.144150972366333, 0.7027027010917664, 0.3333333432674408, 0.0]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training accuracy and loss per fold in the same figure\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# Plot accuracy\n",
    "plt.plot(kfold_list, mean_acc_per_fold, marker='o', linestyle='-', color='b', label='Accuracy')\n",
    "\n",
    "# Plot loss\n",
    "plt.plot(kfold_list, mean_loss_per_fold, marker='o', linestyle='-', color='r', label='Loss')\n",
    "\n",
    "# plot precision\n",
    "plt.plot(kfold_list, mean_precision_per_fold,marker='o', linestyle='-', color='g', label='Precision')\n",
    "\n",
    "plt.title('Training Accuracy and Loss per Fold')\n",
    "plt.xlabel('Fold')\n",
    "plt.ylabel('Metric Value')\n",
    "plt.xticks(range(1, num_folds + 1))\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normal training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Hiperparameters\n",
    "batch_size = 1\n",
    "no_epochs = 100\n",
    "verbosity = 2 #1\n",
    "# Define the K-fold Cross Validator\n",
    "num_folds = fold_it\n",
    "kfold = KFold(n_splits=num_folds, shuffle=True)\n",
    "\n",
    "# K-fold Cross Validation model evaluation for prints\n",
    "fold_no = 1\n",
    "\n",
    "# Define the model architecture\n",
    "model = Sequential()\n",
    "# Convolutional block 1\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.01))\n",
    "# Convolutional block 2\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(8, (3, 3), padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.01))\n",
    "# Convolutional block 3\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(8, (3, 3), padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.01))\n",
    "# Convolutional block 4\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(8, (3, 3), padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.01))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(64))\n",
    "model.add(LeakyReLU(alpha=0.01))\n",
    "model.add(Dense(1, activation='sigmoid') )\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', \n",
    "            loss='binary_crossentropy', \n",
    "            metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.SpecificityAtSensitivity(sensitivity=1)])\n",
    "\n",
    "\n",
    "# Generate a print\n",
    "print('------------------------------------------------------------------------')\n",
    "print(f'Training for fold {fold_no} ...')\n",
    "\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss',\n",
    "                                            patience=3,\n",
    "                                            min_delta=0.0001)\n",
    "\n",
    "# Fit data to model\n",
    "history = model.fit(X_train[train], y_train[train],\n",
    "            batch_size=batch_size,\n",
    "            epochs=no_epochs,\n",
    "            verbose=verbosity,\n",
    "            callbacks=[callback])\n",
    "\n",
    "# Generate generalization metrics\n",
    "scores = model.evaluate(X_train[test], y_train[test], verbose=0)\n",
    "print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%; {model.metrics_names[2]}: {scores[2]}')\n",
    "acc_per_fold.append(scores[1] * 100)\n",
    "loss_per_fold.append(scores[0])\n",
    "precision_per_fold.append(scores[2])\n",
    "\n",
    "if(scores[1]*100 > 60):\n",
    "    folder_path = f'd:/Sistema/Escritorio/Escritorio/Tesis/DAIC-WOZ/folds/{num_folds}-fold_11_04_2024/'\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "    \n",
    "    subfolder_path = folder_path + f'{fold_no}-{scores[1]*100}/'\n",
    "    if not os.path.exists(subfolder_path):\n",
    "        os.makedirs(subfolder_path)\n",
    "\n",
    "    #model.save(f'{folder_path}fold_v5_{fold_no}_{scores[1]*100}.h5')\n",
    "    model.save(subfolder_path + f'fold-{fold_no}.h5')  # Replace with your actual path\n",
    "    df_to_save = new_df_train.iloc[train].copy()\n",
    "    df_to_save.drop('Spectrogram',axis=1, inplace=True)\n",
    "    df_to_save.to_csv(subfolder_path + f'train-data-fold-{fold_no}.csv', index=False)\n",
    "# Increase fold number\n",
    "fold_no = fold_no + 1\n",
    "\n",
    "# Get means\n",
    "kfold_list.append(num_folds)\n",
    "\n",
    "with open(folder_path+'score.txt', 'w') as file:\n",
    "    # Write 'hello' to the file\n",
    "    file.write(f' accuracy: {mean_acc}. loss: {mean_loss}. precision: {mean_precision}')\n",
    "# Plot training accuracy per fold\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CNNSpectrogram",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
