{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\herna\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_addons\\utils\\tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\herna\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_addons\\utils\\ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.12.0 and strictly below 2.15.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.10.1 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Global imports\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import imageio\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import BatchNormalization, Conv2D, LeakyReLU, MaxPooling2D, Flatten, Dense\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import librosa\n",
    "import tensorflow_addons as tfa\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchaudio\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import BatchNormalization, Conv2D, LeakyReLU, Flatten, Dense\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# import librosa\n",
    "# #print(tf.config.list_physical_devices('GPU'))\n",
    "# # Desactivar GPU y forzar uso de CPU\n",
    "# gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "# if gpus:\n",
    "#     try:\n",
    "#         #tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "#         pass\n",
    "#     except RuntimeError as e:\n",
    "#         print(e)\n",
    "\n",
    "# #tf.config.set_visible_devices([], 'GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # TensorFlow 2.x\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)\n",
    "else:\n",
    "    print(\"No GPU found\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def load_data(filename):\n",
    "    \"\"\"\n",
    "    Load data from a pickle file.\n",
    "\n",
    "    Args:\n",
    "        filename (str): The path to the pickle file.\n",
    "\n",
    "    Returns:\n",
    "        dict: The loaded data dictionary.\n",
    "    \"\"\"\n",
    "    with open(filename, 'rb') as file:\n",
    "        data_dict_loaded = pickle.load(file)\n",
    "    return data_dict_loaded\n",
    "\n",
    "filename = \"../Data/D3TEC.pkl\"\n",
    "data_dict_loaded = load_data(filename)\n",
    "# data_dict_loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_dict_loaded[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting from fold 1 out of 5, epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading audios: 100%|██████████| 62/62 [00:03<00:00, 18.36it/s]\n",
      "Segmenting audios: 100%|██████████| 1674/1674 [00:00<00:00, 47821.29it/s]\n",
      "Generating spectrograms:   0%|          | 0/7564 [00:00<?, ?it/s]c:\\Users\\herna\\anaconda3\\envs\\tf\\lib\\site-packages\\torchaudio\\functional\\functional.py:584: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (201) may be set too low.\n",
      "  warnings.warn(\n",
      "Generating spectrograms: 100%|██████████| 7564/7564 [11:34<00:00, 10.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 1 of 5 folds...\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Training epoch 1/100, steps per epoch: 1211\n",
      "1211/1211 - 67s - loss: 3.0666e-04 - accuracy: 0.9998 - precision: 1.0000 - recall: 0.9998 - auc: 0.0000e+00 - specificity: 0.0000e+00 - f1_score: 0.9999 - 67s/epoch - 55ms/step\n",
      "[Epoch 1 End] Memory usage: 32290.91 MB\n",
      "Training epoch 2/100, steps per epoch: 1211\n",
      "1211/1211 - 80s - loss: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 0.0000e+00 - specificity: 0.0000e+00 - f1_score: 1.0000 - 80s/epoch - 66ms/step\n",
      "[Epoch 2 End] Memory usage: 34870.96 MB\n",
      "Training epoch 3/100, steps per epoch: 1211\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "Graph execution error:\n\nDetected at node 'sequential/batch_normalization/FusedBatchNormV3' defined at (most recent call last):\n    File \"c:\\Users\\herna\\anaconda3\\envs\\tf\\lib\\runpy.py\", line 196, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"c:\\Users\\herna\\anaconda3\\envs\\tf\\lib\\runpy.py\", line 86, in _run_code\n      exec(code, run_globals)\n    File \"c:\\Users\\herna\\anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n      app.launch_new_instance()\n    File \"c:\\Users\\herna\\anaconda3\\envs\\tf\\lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n      app.start()\n    File \"c:\\Users\\herna\\anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n      self.io_loop.start()\n    File \"c:\\Users\\herna\\anaconda3\\envs\\tf\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n      self.asyncio_loop.run_forever()\n    File \"c:\\Users\\herna\\anaconda3\\envs\\tf\\lib\\asyncio\\base_events.py\", line 603, in run_forever\n      self._run_once()\n    File \"c:\\Users\\herna\\anaconda3\\envs\\tf\\lib\\asyncio\\base_events.py\", line 1909, in _run_once\n      handle._run()\n    File \"c:\\Users\\herna\\anaconda3\\envs\\tf\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"c:\\Users\\herna\\anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n      await self.process_one()\n    File \"c:\\Users\\herna\\anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n      await dispatch(*args)\n    File \"c:\\Users\\herna\\anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n      await result\n    File \"c:\\Users\\herna\\anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n      await super().execute_request(stream, ident, parent)\n    File \"c:\\Users\\herna\\anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n      reply_content = await reply_content\n    File \"c:\\Users\\herna\\anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n      res = shell.run_cell(\n    File \"c:\\Users\\herna\\anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"c:\\Users\\herna\\anaconda3\\envs\\tf\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n      result = self._run_cell(\n    File \"c:\\Users\\herna\\anaconda3\\envs\\tf\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n      result = runner(coro)\n    File \"c:\\Users\\herna\\anaconda3\\envs\\tf\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"c:\\Users\\herna\\anaconda3\\envs\\tf\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"c:\\Users\\herna\\anaconda3\\envs\\tf\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"c:\\Users\\herna\\anaconda3\\envs\\tf\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\herna\\AppData\\Local\\Temp\\ipykernel_45784\\3400400702.py\", line 416, in <module>\n      tester.run_kfold_test()\n    File \"C:\\Users\\herna\\AppData\\Local\\Temp\\ipykernel_45784\\3400400702.py\", line 165, in run_kfold_test\n      model.fit(X_train, y_train, batch_size=self.batch_size, epochs=1, verbose=verbosity, callbacks=[checkpoint_callback])\n    File \"c:\\Users\\herna\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\herna\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1564, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"c:\\Users\\herna\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function\n      return step_function(self, iterator)\n    File \"c:\\Users\\herna\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\herna\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step\n      outputs = model.train_step(data)\n    File \"c:\\Users\\herna\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 993, in train_step\n      y_pred = self(x, training=True)\n    File \"c:\\Users\\herna\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\herna\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 557, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"c:\\Users\\herna\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\herna\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"c:\\Users\\herna\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\herna\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\sequential.py\", line 410, in call\n      return super().call(inputs, training=training, mask=mask)\n    File \"c:\\Users\\herna\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\functional.py\", line 510, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"c:\\Users\\herna\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\functional.py\", line 667, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"c:\\Users\\herna\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\herna\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"c:\\Users\\herna\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\herna\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\layers\\normalization\\batch_normalization.py\", line 850, in call\n      outputs = self._fused_batch_norm(inputs, training=training)\n    File \"c:\\Users\\herna\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\layers\\normalization\\batch_normalization.py\", line 660, in _fused_batch_norm\n      output, mean, variance = control_flow_util.smart_cond(\n    File \"c:\\Users\\herna\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\control_flow_util.py\", line 108, in smart_cond\n      return tf.__internal__.smart_cond.smart_cond(\n    File \"c:\\Users\\herna\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\layers\\normalization\\batch_normalization.py\", line 634, in _fused_batch_norm_training\n      return tf.compat.v1.nn.fused_batch_norm(\nNode: 'sequential/batch_normalization/FusedBatchNormV3'\nOOM when allocating tensor with shape[5,30,792,293] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node sequential/batch_normalization/FusedBatchNormV3}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_1885]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 416\u001b[0m\n\u001b[0;32m    413\u001b[0m \u001b[38;5;66;03m# Ejemplo de uso\u001b[39;00m\n\u001b[0;32m    414\u001b[0m \u001b[38;5;66;03m# Supongamos que data_dict_loaded es el diccionario de datos cargado\u001b[39;00m\n\u001b[0;32m    415\u001b[0m tester \u001b[38;5;241m=\u001b[39m KFoldCNNTester(data_dict_loaded, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresults\u001b[39m\u001b[38;5;124m'\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, loading_method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnormal\u001b[39m\u001b[38;5;124m'\u001b[39m, num_folds\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, model_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLuisFelipe\u001b[39m\u001b[38;5;124m'\u001b[39m, image_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m293\u001b[39m, \u001b[38;5;241m792\u001b[39m), max_segment_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m--> 416\u001b[0m \u001b[43mtester\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_kfold_test\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[6], line 165\u001b[0m, in \u001b[0;36mKFoldCNNTester.run_kfold_test\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(start_epoch, no_epochs):\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/100, steps per epoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msteps_per_epoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 165\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbosity\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mcheckpoint_callback\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    167\u001b[0m         model\u001b[38;5;241m.\u001b[39msave_weights(checkpoint_filepath\u001b[38;5;241m.\u001b[39mformat(epoch\u001b[38;5;241m=\u001b[39mepoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\herna\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\herna\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: Graph execution error:\n\nDetected at node 'sequential/batch_normalization/FusedBatchNormV3' defined at (most recent call last):\n    File \"c:\\Users\\herna\\anaconda3\\envs\\tf\\lib\\runpy.py\", line 196, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"c:\\Users\\herna\\anaconda3\\envs\\tf\\lib\\runpy.py\", line 86, in _run_code\n      exec(code, run_globals)\n    File \"c:\\Users\\herna\\anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n      app.launch_new_instance()\n    File \"c:\\Users\\herna\\anaconda3\\envs\\tf\\lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n      app.start()\n    File \"c:\\Users\\herna\\anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n      self.io_loop.start()\n    File \"c:\\Users\\herna\\anaconda3\\envs\\tf\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n      self.asyncio_loop.run_forever()\n    File \"c:\\Users\\herna\\anaconda3\\envs\\tf\\lib\\asyncio\\base_events.py\", line 603, in run_forever\n      self._run_once()\n    File \"c:\\Users\\herna\\anaconda3\\envs\\tf\\lib\\asyncio\\base_events.py\", line 1909, in _run_once\n      handle._run()\n    File \"c:\\Users\\herna\\anaconda3\\envs\\tf\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"c:\\Users\\herna\\anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n      await self.process_one()\n    File \"c:\\Users\\herna\\anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n      await dispatch(*args)\n    File \"c:\\Users\\herna\\anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n      await result\n    File \"c:\\Users\\herna\\anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n      await super().execute_request(stream, ident, parent)\n    File \"c:\\Users\\herna\\anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n      reply_content = await reply_content\n    File \"c:\\Users\\herna\\anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n      res = shell.run_cell(\n    File \"c:\\Users\\herna\\anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"c:\\Users\\herna\\anaconda3\\envs\\tf\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n      result = self._run_cell(\n    File \"c:\\Users\\herna\\anaconda3\\envs\\tf\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n      result = runner(coro)\n    File \"c:\\Users\\herna\\anaconda3\\envs\\tf\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"c:\\Users\\herna\\anaconda3\\envs\\tf\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"c:\\Users\\herna\\anaconda3\\envs\\tf\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"c:\\Users\\herna\\anaconda3\\envs\\tf\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\herna\\AppData\\Local\\Temp\\ipykernel_45784\\3400400702.py\", line 416, in <module>\n      tester.run_kfold_test()\n    File \"C:\\Users\\herna\\AppData\\Local\\Temp\\ipykernel_45784\\3400400702.py\", line 165, in run_kfold_test\n      model.fit(X_train, y_train, batch_size=self.batch_size, epochs=1, verbose=verbosity, callbacks=[checkpoint_callback])\n    File \"c:\\Users\\herna\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\herna\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1564, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"c:\\Users\\herna\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function\n      return step_function(self, iterator)\n    File \"c:\\Users\\herna\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\herna\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step\n      outputs = model.train_step(data)\n    File \"c:\\Users\\herna\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 993, in train_step\n      y_pred = self(x, training=True)\n    File \"c:\\Users\\herna\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\herna\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 557, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"c:\\Users\\herna\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\herna\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"c:\\Users\\herna\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\herna\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\sequential.py\", line 410, in call\n      return super().call(inputs, training=training, mask=mask)\n    File \"c:\\Users\\herna\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\functional.py\", line 510, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"c:\\Users\\herna\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\functional.py\", line 667, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"c:\\Users\\herna\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\herna\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"c:\\Users\\herna\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\herna\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\layers\\normalization\\batch_normalization.py\", line 850, in call\n      outputs = self._fused_batch_norm(inputs, training=training)\n    File \"c:\\Users\\herna\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\layers\\normalization\\batch_normalization.py\", line 660, in _fused_batch_norm\n      output, mean, variance = control_flow_util.smart_cond(\n    File \"c:\\Users\\herna\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\control_flow_util.py\", line 108, in smart_cond\n      return tf.__internal__.smart_cond.smart_cond(\n    File \"c:\\Users\\herna\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\layers\\normalization\\batch_normalization.py\", line 634, in _fused_batch_norm_training\n      return tf.compat.v1.nn.fused_batch_norm(\nNode: 'sequential/batch_normalization/FusedBatchNormV3'\nOOM when allocating tensor with shape[5,30,792,293] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node sequential/batch_normalization/FusedBatchNormV3}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_1885]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import BatchNormalization, Conv2D, LeakyReLU, Flatten, Dense, Input, MaxPooling2D, Dropout, Resizing, MaxPool2D\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import datetime\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import gc\n",
    "import psutil\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import torchaudio\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "\n",
    "# Deshabilitar XLA compilación\n",
    "os.environ['TF_XLA_FLAGS'] = '--tf_xla_auto_jit=0'\n",
    "\n",
    "# Ruta base para los archivos de espectrogramas\n",
    "base_path = \"F:\\\\Github\\\\Improving-deep-neural-networks-to-identify-mental-disorders-using-Neural-Architecture-Search\\\\D3T3C\"\n",
    "\n",
    "def specificity(y_true, y_pred):\n",
    "    true_negatives = tf.reduce_sum(tf.cast((y_pred < 0.5) & (y_true == 0), tf.float32))\n",
    "    possible_negatives = tf.reduce_sum(tf.cast(y_true == 0, tf.float32))\n",
    "    specificity = true_negatives / (possible_negatives + tf.keras.backend.epsilon())\n",
    "    return specificity\n",
    "\n",
    "def log_memory_usage(stage):\n",
    "    process = psutil.Process(os.getpid())\n",
    "    memory_info = process.memory_info()\n",
    "    print(f\"[{stage}] Memory usage: {memory_info.rss / (1024 ** 2):.2f} MB\")\n",
    "\n",
    "class KFoldCNNTester:\n",
    "    def __init__(self, data_dict, filename, batch_size=1, use_dummy_data=False, loading_method='tf_data', model_type='SpectroCNN', num_channels=3, recording_device='sm', num_folds=5, use_gender='All', reset_saves=False, image_size=(792, 293), max_segment_length=5):\n",
    "        self.data_dict = data_dict\n",
    "        self.filename = filename\n",
    "        self.batch_size = batch_size\n",
    "        self.mean_acc_per_fold = []\n",
    "        self.mean_loss_per_fold = []\n",
    "        self.mean_precision_per_fold = []\n",
    "        self.mean_recall_per_fold = []\n",
    "        self.mean_auc_per_fold = []\n",
    "        self.mean_specificity_per_fold = []\n",
    "        self.mean_f1_per_fold = []  # Para guardar los F1 Scores\n",
    "        self.kfold_list = []\n",
    "        self.state_file = f'{self.filename}_state.json'\n",
    "        self.keys = list(data_dict.keys())  # Lista de keys del diccionario\n",
    "        self.use_dummy_data = use_dummy_data\n",
    "        self.loading_method = loading_method  # Método de carga de datos\n",
    "        self.model_type = model_type  # Flag para elegir el modelo a utilizar\n",
    "        self.num_channels = num_channels  # Número de canales para las imágenes\n",
    "        self.recording_device = recording_device  # Tipo de dispositivo de grabación\n",
    "        self.num_folds = num_folds  # Número de K-folds a correr\n",
    "        self.use_gender = use_gender  # Género a utilizar en el K-fold\n",
    "        self.reset_saves = reset_saves  # Flag para reiniciar los guardados\n",
    "        self.image_size = image_size  # Tamaño de las imágenes generadas\n",
    "        self.max_segment_length = max_segment_length  # Longitud máxima de segmento en segundos\n",
    "\n",
    "        if self.reset_saves and os.path.exists(self.state_file):\n",
    "            os.remove(self.state_file)\n",
    "\n",
    "    def save_state(self, fold_no, epoch_no, checkpoint_list, metrics_per_fold):\n",
    "        state = self.load_state() or {}\n",
    "        state.setdefault('model_type', {}).setdefault(self.model_type, {}).setdefault(self.use_gender, {})[str(self.num_folds)] = {\n",
    "            'epoch': epoch_no,\n",
    "            'checkpoints': checkpoint_list,\n",
    "            'fold_no': fold_no,\n",
    "            'metrics_per_fold': metrics_per_fold\n",
    "        }\n",
    "        with open(self.state_file, 'w') as f:\n",
    "            json.dump(state, f)\n",
    "\n",
    "    def load_state(self):\n",
    "        if os.path.exists(self.state_file):\n",
    "            with open(self.state_file, 'r') as f:\n",
    "                state = json.load(f)\n",
    "            return state\n",
    "        return None\n",
    "\n",
    "    def try_load_weights(self, model):\n",
    "        state = self.load_state()\n",
    "        if state is not None:\n",
    "            if str(self.num_folds) not in state['model_type'][self.model_type][self.use_gender]:\n",
    "                return state, 1, 0, []\n",
    "            last_checkpoint_dict = state['model_type'][self.model_type][self.use_gender][str(self.num_folds)]\n",
    "            checkpoints = last_checkpoint_dict['checkpoints']\n",
    "            for checkpoint in reversed(checkpoints):\n",
    "                try:\n",
    "                    path = \"./\" + checkpoint['path']\n",
    "                    model.load_weights(path)\n",
    "                    print('trying to load weights from checkpoint:', path)\n",
    "                    return state, checkpoint['fold_no'], checkpoint['epoch_no'], checkpoints\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading checkpoint {checkpoint['path']}: {e}\")\n",
    "        return state, 1, 0, []\n",
    "\n",
    "    def run_kfold_test(self):\n",
    "        model = self.define_model()\n",
    "        \n",
    "        state, fold_no, start_epoch, checkpoint_list = self.try_load_weights(model)\n",
    "        print(f\"Starting from fold {fold_no} out of {self.num_folds}, epoch {start_epoch}\")\n",
    "        if state is None or str(self.num_folds) not in state['model_type'][self.model_type][self.use_gender]:\n",
    "            fold_no, start_epoch = 1, 0\n",
    "            checkpoint_list = []\n",
    "            metrics_per_fold = {\n",
    "                \"acc_per_fold\": [],\n",
    "                \"loss_per_fold\": [],\n",
    "                \"precision_per_fold\": [],\n",
    "                \"recall_per_fold\": [],\n",
    "                \"auc_per_fold\": [],\n",
    "                \"specificity_per_fold\": [],\n",
    "                \"f1_per_fold\": []  # Para guardar los F1 Scores\n",
    "            }\n",
    "        else:\n",
    "            last_checkpoint_dict = state['model_type'][self.model_type][self.use_gender][str(self.num_folds)]\n",
    "            metrics_per_fold = last_checkpoint_dict['metrics_per_fold']\n",
    "            checkpoint_list = last_checkpoint_dict['checkpoints']\n",
    "\n",
    "            # If all folds are completed, just calculate and display the means\n",
    "            if len(metrics_per_fold[\"acc_per_fold\"]) >= self.num_folds:\n",
    "                self.calculate_means(metrics_per_fold)\n",
    "                return\n",
    "\n",
    "        if not os.path.exists('checkpoints'):\n",
    "            os.makedirs('checkpoints')\n",
    "\n",
    "        no_epochs = 100\n",
    "        verbosity = 2\n",
    "        skf = StratifiedKFold(n_splits=self.num_folds, shuffle=True)\n",
    "\n",
    "        # Preparar datos para K-Fold\n",
    "        X, y_array = self.prepare_data_for_kfold()\n",
    "\n",
    "        # Borrar el diccionario de datos para liberar memoria\n",
    "        del self.data_dict\n",
    "        gc.collect()\n",
    "\n",
    "        for train_indices, test_indices in skf.split(X, y_array):\n",
    "            if fold_no > self.num_folds:\n",
    "                break\n",
    "\n",
    "            # Obtener los datos de entrenamiento y prueba\n",
    "            X_train, X_test = X[train_indices], X[test_indices]\n",
    "            y_train, y_test = y_array[train_indices], y_array[test_indices]\n",
    "\n",
    "            checkpoint_filepath = f'checkpoints/{self.filename}_{self.model_type}_{self.use_gender}_{self.recording_device}_checkpoint_{self.num_folds}_fold_{fold_no}_epoch_{{epoch}}.h5'\n",
    "\n",
    "            self.try_load_weights(model)\n",
    "\n",
    "            print('------------------------------------------------------------------------')\n",
    "            print(f'Training for fold {fold_no} of {self.num_folds} folds...')\n",
    "\n",
    "            checkpoint_callback = ModelCheckpoint(filepath=checkpoint_filepath, save_weights_only=True,\n",
    "                                                  monitor='loss', mode='min', save_best_only=False, period=10)\n",
    "\n",
    "            callback = EarlyStopping(monitor='loss', patience=3, min_delta=0.0001)\n",
    "\n",
    "            steps_per_epoch = np.ceil(len(X_train) / self.batch_size).astype(int)\n",
    "            for epoch in range(start_epoch, no_epochs):\n",
    "                print(f\"Training epoch {epoch+1}/100, steps per epoch: {steps_per_epoch}\")\n",
    "                model.fit(X_train, y_train, batch_size=self.batch_size, epochs=1, verbose=verbosity, callbacks=[checkpoint_callback])\n",
    "                if (epoch + 1) % 10 == 0:\n",
    "                    model.save_weights(checkpoint_filepath.format(epoch=epoch + 1))\n",
    "                    checkpoint_list.append({'fold_no': fold_no, 'epoch_no': epoch + 1, 'path': checkpoint_filepath.format(epoch=epoch + 1)})\n",
    "                self.save_state(fold_no, epoch + 1, checkpoint_list, metrics_per_fold)\n",
    "                log_memory_usage(f\"Epoch {epoch+1} End\")\n",
    "                start_epoch = 0\n",
    "\n",
    "            start_epoch = 0\n",
    "\n",
    "            print(f\"Evaluating fold {fold_no}\")\n",
    "            scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "            y_pred = model.predict(X_test)\n",
    "            y_true = y_test\n",
    "\n",
    "            metrics_per_fold[\"acc_per_fold\"].append(scores[1] * 100)\n",
    "            metrics_per_fold[\"loss_per_fold\"].append(scores[0])\n",
    "            metrics_per_fold[\"precision_per_fold\"].append(scores[2])\n",
    "            metrics_per_fold[\"recall_per_fold\"].append(scores[3])\n",
    "            metrics_per_fold[\"auc_per_fold\"].append(scores[4])\n",
    "            metrics_per_fold[\"specificity_per_fold\"].append(scores[5])\n",
    "\n",
    "            # Calcular F1 Score\n",
    "            y_pred_labels = np.where(np.array(y_pred) > 0.5, 1, 0)\n",
    "            f1_score = tfa.metrics.F1Score(num_classes=1, threshold=0.5)(y_true, y_pred_labels)\n",
    "            metrics_per_fold[\"f1_per_fold\"].append(f1_score)\n",
    "\n",
    "            print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%; {model.metrics_names[2]}: {scores[2]}; {model.metrics_names[3]}: {scores[3]}; {model.metrics_names[4]}: {scores[4]}; {model.metrics_names[5]}: {scores[5]}; F1: {f1_score}')\n",
    "\n",
    "            # Print confusion matrix\n",
    "            cm = confusion_matrix(y_true, y_pred_labels)\n",
    "            print(f'Confusion Matrix for fold {fold_no}:\\n{cm}')\n",
    "\n",
    "            if scores[1] * 100 > 60:\n",
    "                self.save_model_and_data(model, fold_no, scores, train_keys)\n",
    "\n",
    "            self.save_state(fold_no + 1, 0, checkpoint_list, metrics_per_fold)\n",
    "            fold_no += 1\n",
    "\n",
    "        if metrics_per_fold[\"acc_per_fold\"]:\n",
    "            self.calculate_means(metrics_per_fold)\n",
    "\n",
    "    def prepare_data_for_kfold(self):\n",
    "        X = []\n",
    "        y = []\n",
    "\n",
    "        # Cargar audios\n",
    "        for key in tqdm(self.keys, desc=\"Loading audios\"):\n",
    "            info = self.data_dict[key]\n",
    "            for audio_type, audios in info['audios'].items():\n",
    "                for question_number, audio_data in audios.items():\n",
    "                    if isinstance(question_number, int) and audio_type == self.recording_device:\n",
    "                        audio_path = base_path + audio_data['file_path'].replace('..', \"\")\n",
    "                        waveform, sample_rate = torchaudio.load(audio_path)\n",
    "                        X.append((waveform, sample_rate))\n",
    "                        y.append(info['PHQ-Binary'])\n",
    "\n",
    "        # Segmentar audios\n",
    "        segments = []\n",
    "        segment_labels = []\n",
    "        for waveform, sample_rate in tqdm(X, desc=\"Segmenting audios\"):\n",
    "            segmented_audio = self.segment_audio(waveform, sample_rate)\n",
    "            segments.extend((segment, sample_rate) for segment in segmented_audio)\n",
    "            label = info['PHQ-Binary']  # Definir la etiqueta aquí\n",
    "            segment_labels.extend([label] * len(segmented_audio))\n",
    "\n",
    "        # Generar espectrogramas\n",
    "        spectrograms = []\n",
    "        for segment, sample_rate in tqdm(segments, desc=\"Generating spectrograms\"):\n",
    "            spectrograms.append(self.generate_spectrogram_image(segment, sample_rate))\n",
    "\n",
    "        y_array = np.array(segment_labels)\n",
    "        return np.array(spectrograms), y_array\n",
    "\n",
    "    def segment_audio(self, waveform, sample_rate):\n",
    "        num_segments = int(np.ceil(waveform.shape[1] / (self.max_segment_length * sample_rate)))\n",
    "        segments = []\n",
    "        for i in range(num_segments):\n",
    "            start = i * self.max_segment_length * sample_rate\n",
    "            end = (i + 1) * self.max_segment_length * sample_rate\n",
    "            segment = waveform[:, int(start):int(end)]\n",
    "            segments.append(segment)\n",
    "        return segments\n",
    "\n",
    "    def generate_spectrogram_image(self, segment, sample_rate):\n",
    "        spectrogram = torchaudio.transforms.MelSpectrogram(sample_rate=sample_rate)(segment)\n",
    "        spectrogram = torchaudio.transforms.AmplitudeToDB()(spectrogram)\n",
    "\n",
    "        fig = plt.figure(figsize=(self.image_size[1] / 100, self.image_size[0] / 100), dpi=100)\n",
    "        plt.imshow(spectrogram[0].numpy(), cmap='viridis')\n",
    "        plt.axis('off')\n",
    "        plt.tight_layout(pad=0)\n",
    "\n",
    "        buf = io.BytesIO()\n",
    "        plt.savefig(buf, format='png', bbox_inches='tight', pad_inches=0)\n",
    "        plt.close(fig)\n",
    "        buf.seek(0)\n",
    "        img = Image.open(buf)\n",
    "        img = img.convert('RGB')  # Asegúrate de que tenga 3 canales\n",
    "        img = img.resize((self.image_size[1], self.image_size[0]))\n",
    "        img_array = np.array(img)\n",
    "        return img_array\n",
    "\n",
    "    def define_model(self):\n",
    "        if self.model_type == 'SpectroCNN':\n",
    "            return self.define_complete_model()\n",
    "        elif self.model_type == 'LuisFelipe':\n",
    "            return self.define_luisfelipe_model()\n",
    "        elif self.model_type == 'reduced':\n",
    "            return self.define_reduced_model()\n",
    "\n",
    "    def define_reduced_model(self):\n",
    "        model = Sequential([\n",
    "            BatchNormalization(name='batch_normalization_9'),\n",
    "            Conv2D(16, kernel_size=(3, 3), padding='same', input_shape=(*self.image_size, self.num_channels), name='conv2d_6'),\n",
    "            LeakyReLU(alpha=0.01, name='leaky_re_lu_9'),\n",
    "            BatchNormalization(name='batch_normalization_10'),\n",
    "            Conv2D(8, (3, 3), padding='same', name='conv2d_7'),\n",
    "            LeakyReLU(alpha=0.01, name='leaky_re_lu_10'),\n",
    "            BatchNormalization(name='batch_normalization_11'),\n",
    "            Flatten(name='flatten_6'),\n",
    "            Dense(32, name='dense_6'),\n",
    "            LeakyReLU(alpha=0.01, name='leaky_re_lu_11'),\n",
    "            Dense(1, activation='sigmoid', name='dense_7')\n",
    "        ])\n",
    "        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall(), tf.keras.metrics.AUC(), specificity, tfa.metrics.F1Score(num_classes=1, threshold=0.5)])\n",
    "        return model\n",
    "\n",
    "    def define_complete_model(self):\n",
    "        input_shape = (*self.image_size, self.num_channels)\n",
    "        inputs = Input(shape=input_shape)\n",
    "\n",
    "        x = Conv2D(32, kernel_size=(3, 3), padding='same')(inputs)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = LeakyReLU(alpha=0.01)(x)\n",
    "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "        for _ in range(31):\n",
    "            x = Conv2D(32, kernel_size=(3, 3), padding='same')(x)\n",
    "            x = BatchNormalization()(x)\n",
    "            x = LeakyReLU(alpha=0.01)(x)\n",
    "\n",
    "        x = Flatten()(x)\n",
    "        x = Dense(256, activation='relu')(x)\n",
    "        x = Dropout(0.5)(x)\n",
    "        x = Dense(128, activation='relu')(x)\n",
    "        x = Dropout(0.5)(x)\n",
    "\n",
    "        outputs = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "        model = Model(inputs=inputs, outputs=outputs)\n",
    "        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall(), tf.keras.metrics.AUC(), specificity, tfa.metrics.F1Score(num_classes=1, threshold=0.5)])\n",
    "        return model\n",
    "\n",
    "    def define_luisfelipe_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Resizing(self.image_size[1], self.image_size[0], input_shape=(self.image_size[0], self.image_size[1], self.num_channels)))\n",
    "        model.add(Conv2D(30, (3, 3), strides=1, padding=\"same\", activation=\"relu\"))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPool2D((2, 2), strides=2, padding=\"same\"))\n",
    "        model.add(Conv2D(15, (3, 3), strides=1, padding=\"same\", activation=\"relu\"))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPool2D((2, 2), strides=2, padding=\"same\"))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(units=256, activation=\"relu\"))\n",
    "        model.add(Dropout(0.3))\n",
    "        model.add(Dense(1, activation=\"sigmoid\"))\n",
    "        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall(), tf.keras.metrics.AUC(), specificity, tfa.metrics.F1Score(num_classes=1, threshold=0.5)])\n",
    "        return model\n",
    "\n",
    "    def save_model_and_data(self, model, fold_no, scores, train_keys):\n",
    "        current_time = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "        folder_path = f'{self.filename}_{self.model_type}_{self.use_gender}_{self.recording_device}_{current_time}/'\n",
    "        if not os.path.exists(folder_path):\n",
    "            os.makedirs(folder_path)\n",
    "\n",
    "        subfolder_path = folder_path + f'{self.num_folds}-fold_{fold_no}-{scores[1]*100}/'\n",
    "        if not os.path.exists(subfolder_path):\n",
    "            os.makedirs(subfolder_path)\n",
    "\n",
    "        model.save(subfolder_path + f'fold-{fold_no}.h5')\n",
    "\n",
    "    def calculate_means(self, metrics_per_fold):\n",
    "        mean_acc = sum(metrics_per_fold[\"acc_per_fold\"]) / len(metrics_per_fold[\"acc_per_fold\"]) if metrics_per_fold[\"acc_per_fold\"] else 0\n",
    "        mean_loss = sum(metrics_per_fold[\"loss_per_fold\"]) / len(metrics_per_fold[\"loss_per_fold\"]) if metrics_per_fold[\"loss_per_fold\"] else 0\n",
    "        mean_precision = sum(metrics_per_fold[\"precision_per_fold\"]) / len(metrics_per_fold[\"precision_per_fold\"]) if metrics_per_fold[\"precision_per_fold\"] else 0\n",
    "        mean_recall = sum(metrics_per_fold[\"recall_per_fold\"]) / len(metrics_per_fold[\"recall_per_fold\"]) if metrics_per_fold[\"recall_per_fold\"] else 0\n",
    "        mean_auc = sum(metrics_per_fold[\"auc_per_fold\"]) / len(metrics_per_fold[\"auc_per_fold\"]) if metrics_per_fold[\"auc_per_fold\"] else 0\n",
    "        mean_specificity = sum(metrics_per_fold[\"specificity_per_fold\"]) / len(metrics_per_fold[\"specificity_per_fold\"]) if metrics_per_fold[\"specificity_per_fold\"] else 0\n",
    "        mean_f1 = sum(metrics_per_fold[\"f1_per_fold\"]) / len(metrics_per_fold[\"f1_per_fold\"]) if metrics_per_fold[\"f1_per_fold\"] else 0\n",
    "        \n",
    "        current_time = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "        folder_path = f'./results/{self.filename}_{self.model_type}_{self.use_gender}_{self.recording_device}_{current_time}_{self.num_folds}_mean/'\n",
    "        if not os.path.exists(folder_path):\n",
    "            os.makedirs(folder_path)\n",
    "        with open(folder_path + 'score.txt', 'a+') as file:\n",
    "            file.write(f'accuracy: {mean_acc}. loss: {mean_loss}. precision: {mean_precision}. recall: {mean_recall}. auc: {mean_auc}. specificity: {mean_specificity}. F1: {mean_f1}')\n",
    "\n",
    "    def create_dataset(self, keys):\n",
    "        def data_generator():\n",
    "            if self.use_dummy_data:\n",
    "                for _ in range(len(keys)):\n",
    "                    dummy_spectrogram = np.random.rand(*self.image_size, self.num_channels).astype(np.float32)  # Datos dummy ligeros\n",
    "                    dummy_label = np.random.randint(0, 2, dtype=np.int32)  # Etiqueta dummy corregida\n",
    "                    yield dummy_spectrogram, dummy_label\n",
    "            else:\n",
    "                for key in keys:\n",
    "                    info = self.data_dict[key]\n",
    "                    for audio_type, audios in info['audios'].items():\n",
    "                        for question_number, audio_data in audios.items():\n",
    "                            if isinstance(question_number, int) and audio_type == self.recording_device:\n",
    "                                audio_path = base_path + audio_data['file_path'].replace('..', \"\")\n",
    "                                spectrogram_images = self.generate_spectrogram_image(audio_path)\n",
    "                                for spectrogram_image in spectrogram_images:\n",
    "                                    yield spectrogram_image, info['PHQ-Binary']\n",
    "\n",
    "        output_signature = (\n",
    "            tf.TensorSpec(shape=(*self.image_size, self.num_channels), dtype=tf.float32),\n",
    "            tf.TensorSpec(shape=(), dtype=tf.int32)\n",
    "        )\n",
    "\n",
    "        dataset = tf.data.Dataset.from_generator(\n",
    "            data_generator,\n",
    "            output_signature=output_signature\n",
    "        )\n",
    "        return dataset.cache().shuffle(buffer_size=1000).batch(self.batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    def generate_spectrogram_image(self, segment, sample_rate):\n",
    "        spectrogram = torchaudio.transforms.MelSpectrogram(sample_rate=sample_rate)(segment)\n",
    "        spectrogram = torchaudio.transforms.AmplitudeToDB()(spectrogram)\n",
    "\n",
    "        fig = plt.figure(figsize=(self.image_size[1] / 100, self.image_size[0] / 100), dpi=100)\n",
    "        plt.imshow(spectrogram[0].numpy(), cmap='viridis')\n",
    "        plt.axis('off')\n",
    "        plt.tight_layout(pad=0)\n",
    "\n",
    "        buf = io.BytesIO()\n",
    "        plt.savefig(buf, format='png', bbox_inches='tight', pad_inches=0)\n",
    "        plt.close(fig)\n",
    "        buf.seek(0)\n",
    "        img = Image.open(buf)\n",
    "        img = img.convert('RGB')  # Asegúrate de que tenga 3 canales\n",
    "        img = img.resize((self.image_size[1], self.image_size[0]))\n",
    "        img_array = np.array(img)\n",
    "        return img_array\n",
    "\n",
    "# Ejemplo de uso\n",
    "# Supongamos que data_dict_loaded es el diccionario de datos cargado\n",
    "tester = KFoldCNNTester(data_dict_loaded, 'results', batch_size=5, loading_method='normal', num_folds=5, model_type='LuisFelipe', image_size=(293, 792), max_segment_length=5)\n",
    "tester.run_kfold_test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting from fold 1 out of 5, epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading audios: 100%|██████████| 62/62 [00:03<00:00, 18.30it/s]\n",
      "Segmenting audios: 100%|██████████| 1674/1674 [00:00<00:00, 48502.46it/s]\n",
      "Generating spectrograms:   0%|          | 0/7564 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 11\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Ejemplo de uso\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#from KFoldCNNTesterDoc import KFoldCNNTester\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Ejemplo de uso\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Supongamos que data_dict_loaded es el diccionario de datos cargado\u001b[39;00m\n\u001b[0;32m     10\u001b[0m tester \u001b[38;5;241m=\u001b[39m KFoldCNNTester(data_dict_loaded, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresults\u001b[39m\u001b[38;5;124m'\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, loading_method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnormal\u001b[39m\u001b[38;5;124m'\u001b[39m, num_folds\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, model_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLuisFelipe\u001b[39m\u001b[38;5;124m'\u001b[39m, image_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m293\u001b[39m, \u001b[38;5;241m792\u001b[39m), max_segment_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m---> 11\u001b[0m \u001b[43mtester\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_kfold_test\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[28], line 136\u001b[0m, in \u001b[0;36mKFoldCNNTester.run_kfold_test\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    133\u001b[0m skf \u001b[38;5;241m=\u001b[39m StratifiedKFold(n_splits\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_folds, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    135\u001b[0m \u001b[38;5;66;03m# Preparar datos para K-Fold\u001b[39;00m\n\u001b[1;32m--> 136\u001b[0m X, y_array \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare_data_for_kfold\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;66;03m# Borrar el diccionario de datos para liberar memoria\u001b[39;00m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_dict\n",
      "Cell \u001b[1;32mIn[28], line 233\u001b[0m, in \u001b[0;36mKFoldCNNTester.prepare_data_for_kfold\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    231\u001b[0m \u001b[38;5;66;03m# Generar espectrogramas\u001b[39;00m\n\u001b[0;32m    232\u001b[0m spectrograms \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 233\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m segment, sample_rate \u001b[38;5;129;01min\u001b[39;00m tqdm(segments, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerating spectrograms\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    234\u001b[0m     spectrograms\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_spectrogram_image(segment, sample_rate))\n\u001b[0;32m    236\u001b[0m y_array \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(segment_labels)\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "# Ejemplo de uso\n",
    "\n",
    "#from KFoldCNNTesterDoc import KFoldCNNTester\n",
    "#from tensorflow.keras.layers import MaxPool2D\n",
    "# Ejemplo de uso\n",
    "\n",
    "\n",
    "# Ejemplo de uso\n",
    "# Supongamos que data_dict_loaded es el diccionario de datos cargado\n",
    "tester = KFoldCNNTester(data_dict_loaded, 'results', batch_size=1, loading_method='normal', num_folds=5, model_type='LuisFelipe', image_size=(293, 792), max_segment_length=5)\n",
    "tester.run_kfold_test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tester.run_kfold_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tester = KFoldCNNTester(data_dict_loaded, 'results', batch_size=1, use_tf_data=True, num_folds = 10, model_type='LuisFelipe') #32 needs supervision as it stops more\n",
    "\n",
    "# Run the K-Fold test\n",
    "tester.run_kfold_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# self, data_dict, filename, batch_size=1, use_dummy_data=False, use_tf_data=True, model_type='SpectroCNN', num_channels=3, recording_device='sm', num_folds_range=[5, 8, 10], use_gender='All'):\n",
    "tester = KFoldCNNTester(data_dict_loaded, 'results', batch_size=1, use_tf_data=True, model_type='SpectroCNN', num_folds=5, use_gender='Female') #32 needs supervision as it stops more\n",
    "\n",
    "# Run the K-Fold test\n",
    "tester.run_kfold_test()# self, data_dict, filename, batch_size=1, use_dummy_data=False, use_tf_data=True, model_type='SpectroCNN', num_channels=3, recording_device='sm', num_folds_range=[5, 8, 10], use_gender='All'):\n",
    "\n",
    "\n",
    "tester = KFoldCNNTester(data_dict_loaded, 'results', batch_size=1, use_tf_data=True, model_type='SpectroCNN', num_folds=10, use_gender='Female') #32 needs supervision as it stops more\n",
    "\n",
    "# Run the K-Fold test\n",
    "tester.run_kfold_test()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# self, data_dict, filename, batch_size=1, use_dummy_data=False, use_tf_data=True, model_type='SpectroCNN', num_channels=3, recording_device='sm', num_folds_range=[5, 8, 10], use_gender='All'):\n",
    "tester = KFoldCNNTester(data_dict_loaded, 'results', batch_size=1, use_tf_data=True, model_type='SpectroCNN', num_folds=5, use_gender='Male') #32 needs supervision as it stops more\n",
    "\n",
    "# Run the K-Fold test\n",
    "tester.run_kfold_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# self, data_dict, filename, batch_size=1, use_dummy_data=False, use_tf_data=True, model_type='SpectroCNN', num_channels=3, recording_device='sm', num_folds_range=[5, 8, 10], use_gender='All'):\n",
    "tester = KFoldCNNTester(data_dict_loaded, 'results', batch_size=1, use_tf_data=True, model_type='SpectroCNN', num_folds=10, use_gender='Male') #32 needs supervision as it stops more\n",
    "\n",
    "# Run the K-Fold test\n",
    "tester.run_kfold_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
