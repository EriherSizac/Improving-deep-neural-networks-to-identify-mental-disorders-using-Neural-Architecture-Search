{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, BatchNormalization, MaxPooling2D, Flatten, Dense, Dropout, DepthwiseConv2D\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Binary encoding of Conv2D: 00000000\n",
      "Decoded Conv2D: {'type': 'Conv2D', 'filters': 32, 'kernel_size': (3, 3), 'strides': 1, 'padding': 'same', 'activation': 'relu'}\n",
      "\n",
      "Binary encoding of Dropout: 01101000\n",
      "Decoded Dropout: {'type': 'Dropout', 'rate': 0.3}\n",
      "\n",
      "Binary encoding of Dense: 10001000\n",
      "Decoded Dense: {'type': 'Dense', 'units': 128, 'activation': 'relu'}\n",
      "\n",
      "Binary encoding of Dense: 11100000\n",
      "Decoded Dense: {'type': 'DontCare'}\n"
     ]
    }
   ],
   "source": [
    "# Función para convertir binario a Gray code\n",
    "def binary_to_gray(binary_str):\n",
    "    return binary_str[0] + ''.join(str(int(binary_str[i-1]) ^ int(binary_str[i])) for i in range(1, len(binary_str)))\n",
    "\n",
    "# Función para convertir Gray code a binario\n",
    "def gray_to_binary(gray_str):\n",
    "    binary_str = gray_str[0]\n",
    "    for i in range(1, len(gray_str)):\n",
    "        binary_str += str(int(binary_str[i - 1]) ^ int(gray_str[i]))\n",
    "    return binary_str\n",
    "\n",
    "# Diccionarios para codificación/decodificación\n",
    "layer_type_options = {\n",
    "    'Conv2D': '000', \n",
    "    'BatchNorm': '001', \n",
    "    'MaxPooling': '010', \n",
    "    'Dropout': '011', \n",
    "    'Dense': '100', \n",
    "    'Flatten': '101',\n",
    "    'DepthwiseConv2D': '110',  # Mantenemos DepthwiseConv2D\n",
    "    'DontCare': '111'  # Capa \"don't care\"\n",
    "}\n",
    "filter_options = {32: '00', 30: '01', 16: '10', 8: '11'}\n",
    "stride_options = {1: '0', 2: '1'}\n",
    "dropout_options = {0.2: '00', 0.3: '01', 0.4: '10', 0.5: '11'}\n",
    "neuron_options = {256: '00', 128: '01', 32: '10', 1: '11'}\n",
    "activation_options = {'relu': '00', 'leaky_relu': '01', 'sigmoid': '10', 'tanh': '11'}\n",
    "\n",
    "# Función para codificar parámetros de las capas\n",
    "def encode_layer_params(layer_type, filters=None, strides=None, dropout=None, neurons=None, activation=None):\n",
    "    binary_representation = layer_type_options.get(layer_type, '000')\n",
    "    \n",
    "    if layer_type == 'Conv2D':\n",
    "        binary_representation += filter_options.get(filters, '00')\n",
    "        binary_representation += stride_options.get(strides, '0')\n",
    "        binary_representation += activation_options.get(activation, '00')\n",
    "    \n",
    "    if layer_type == 'DepthwiseConv2D':\n",
    "        binary_representation += filter_options.get(filters, '00')\n",
    "        binary_representation += stride_options.get(strides, '0')\n",
    "        binary_representation += activation_options.get(activation, '00')\n",
    "      \n",
    "    \n",
    "    elif layer_type == 'GlobalAveragePooling2D':\n",
    "        return layer_type_options['GlobalAveragePooling2D']\n",
    "    \n",
    "    elif layer_type == 'MaxPooling':\n",
    "        binary_representation += stride_options.get(strides, '0')\n",
    "    \n",
    "    elif layer_type == 'Dropout':\n",
    "        binary_representation += dropout_options.get(dropout, '00')\n",
    "    \n",
    "    elif layer_type == 'Dense':\n",
    "        binary_representation += neuron_options.get(neurons, '00')\n",
    "        binary_representation += activation_options.get(activation, '00')\n",
    "    \n",
    "     # Fill remaining bits to reach 8 bits\n",
    "    binary_representation = binary_representation.ljust(8, '0')\n",
    "    \n",
    "    return binary_representation\n",
    "\n",
    "# Función para decodificar parámetros de las capas\n",
    "def decode_layer_params(encoded_binary):\n",
    "    # Los primeros 3 bits indican el tipo de capa\n",
    "    layer_type = encoded_binary[:3]\n",
    "    \n",
    "    # Decodificar Conv2D\n",
    "    if layer_type == '000':  # Conv2D\n",
    "        filters = decode_value(encoded_binary[3:5], filter_options, 32)\n",
    "        strides = decode_value(encoded_binary[5], stride_options, 1)\n",
    "        activation = decode_value(encoded_binary[6:8], activation_options, 'relu')\n",
    "        return {\n",
    "            'type': 'Conv2D',\n",
    "            'filters': filters,\n",
    "            'kernel_size': (3, 3),  # Constante\n",
    "            'strides': strides,\n",
    "            'padding': 'same',  # Constante\n",
    "            'activation': activation\n",
    "        }\n",
    "    elif layer_type == '001':  # BatchNorm\n",
    "        return {'type': 'BatchNorm'}\n",
    "   \n",
    "    \n",
    "    # Decodificar MaxPooling\n",
    "    elif layer_type == '010':  # MaxPooling\n",
    "        strides = decode_value(encoded_binary[3], stride_options, 1)\n",
    "        return {'type': 'MaxPooling', 'strides': strides}\n",
    "    \n",
    "    # Decodificar Dropout\n",
    "    elif layer_type == '011':  # Dropout\n",
    "        rate = decode_value(encoded_binary[3:5], dropout_options, 0.2)\n",
    "        return {'type': 'Dropout', 'rate': rate}\n",
    "    \n",
    "    # Decodificar Dense\n",
    "    elif layer_type == '100':  # Dense\n",
    "        neurons = decode_value(encoded_binary[3:5], neuron_options, 256)\n",
    "        activation = decode_value(encoded_binary[5:7], activation_options, 'relu')\n",
    "        return {'type': 'Dense', 'units': neurons, 'activation': activation}\n",
    "   \n",
    "    elif layer_type == '101':  # Flatten\n",
    "        return {'type': 'Flatten'}\n",
    "    \n",
    "     \n",
    "    # Decodificar DepthwiseConv2D\n",
    "    elif layer_type == '110':  # DepthwiseConv2D\n",
    "        filters = decode_value(encoded_binary[3:5], filter_options, 32)\n",
    "        strides = decode_value(encoded_binary[5], stride_options, 1)\n",
    "        activation = decode_value(encoded_binary[6:8], activation_options, 'relu')\n",
    "        return {\n",
    "            'type': 'DepthwiseConv2D',\n",
    "            'filters': filters,\n",
    "            'kernel_size': (3, 3),  # Constante\n",
    "            'strides': strides,\n",
    "            'padding': 'same',  # Constante\n",
    "            'activation': activation\n",
    "        }\n",
    "        \n",
    "    # Caso de \"don't care\"\n",
    "    elif layer_type == '111':  # Don't care (relleno)\n",
    "        return {'type': \"DontCare\"}  # Se omite\n",
    "    \n",
    "    # Si es un tipo no especificado, regresamos None\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Función auxiliar para decodificar valores en función de los diccionarios\n",
    "def decode_value(bits, options_dict, default_value):\n",
    "    # Buscar los bits en los valores del diccionario y devolver la clave correspondiente\n",
    "    for key, value in options_dict.items():\n",
    "        if value == bits:\n",
    "            return key\n",
    "    # Si no se encuentra, devolver el valor por defecto\n",
    "    return default_value\n",
    "\n",
    "\n",
    "\n",
    "# Ejemplo de uso optimizado\n",
    "binary_conv2d = encode_layer_params('Conv2D', filters=32, strides=1, activation='relu')\n",
    "decoded_conv2d = decode_layer_params(binary_conv2d)\n",
    "print(f\"\\nBinary encoding of Conv2D: {binary_conv2d}\")\n",
    "print(f\"Decoded Conv2D: {decoded_conv2d}\")\n",
    "\n",
    "binary_dropout = encode_layer_params('Dropout', dropout=0.3)\n",
    "decoded_dropout = decode_layer_params(binary_dropout)\n",
    "print(f\"\\nBinary encoding of Dropout: {binary_dropout}\")\n",
    "print(f\"Decoded Dropout: {decoded_dropout}\")\n",
    "\n",
    "binary_dense = encode_layer_params('Dense', neurons=128, activation='relu')\n",
    "decoded_dense = decode_layer_params(binary_dense)\n",
    "print(f\"\\nBinary encoding of Dense: {binary_dense}\")\n",
    "print(f\"Decoded Dense: {decoded_dense}\")\n",
    "\n",
    "\n",
    "binary_dense = encode_layer_params('DontCare')\n",
    "decoded_dense = decode_layer_params(binary_dense)\n",
    "print(f\"\\nBinary encoding of Dense: {binary_dense}\")\n",
    "print(f\"Decoded Dense: {decoded_dense}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Encoding of Building Blocks for Neural Network Architectures\n",
    "\n",
    "This module implements the **binary encoding** of various building blocks that make up the architecture of a neural network. Each building block (layers like Conv2D, MaxPooling, Dense, etc.) is represented in binary format based on its key parameters (number of filters, kernel size, number of neurons, activations, etc.). This encoding is useful for optimization processes like **NSGA-III** and later conversion to **Gray code**.\n",
    "\n",
    "### Function `encode_layer_params`\n",
    "\n",
    "This function generates the binary representation of the parameters of a given layer, depending on the type of layer and its specific parameters.\n",
    "\n",
    "### Function Parameters\n",
    "\n",
    "- `layer_type`: (str) The type of layer. It can be one of the following:\n",
    "  - `Conv2D`\n",
    "  - `BatchNorm`\n",
    "  - `MaxPooling`\n",
    "  - `Dropout`\n",
    "  - `Dense`\n",
    "  - `Flatten`\n",
    "  \n",
    "- `filters`: (int) [Optional] Number of filters for `Conv2D` layers. Valid options: `32`, `30`, `16`, `8`.\n",
    "\n",
    "- `kernel_size`: (tuple) [Optional] Kernel size for `Conv2D`. Currently, only `(3, 3)` is supported.\n",
    "\n",
    "- `strides`: (int) [Optional] Stride used in `Conv2D` or `MaxPooling`. Valid options: `1` and `2`.\n",
    "\n",
    "- `padding`: (str) [Optional] Padding type in `Conv2D`. Currently, only `'same'` is supported.\n",
    "\n",
    "- `dropout`: (float) [Optional] Dropout rate for `Dropout` layers. Valid options: `0.2`, `0.3`, `0.5`.\n",
    "\n",
    "- `neurons`: (int) [Optional] Number of neurons in `Dense` layers. Valid options: `256`, `128`, `32`.\n",
    "\n",
    "- `activation`: (str) [Optional] Activation function for `Conv2D` or `Dense` layers. Valid options: `'ReLU'`, `'LeakyReLU'`, `'Sigmoid'`.\n",
    "\n",
    "### Return\n",
    "\n",
    "- `binary_representation`: (str) A string of bits representing the layer parameters in binary format.\n",
    "\n",
    "## Layer Type Encoding\n",
    "\n",
    "Each layer type is represented by a specific sequence of bits. The layer types and their corresponding encodings are:\n",
    "\n",
    "- **Conv2D**: `'000'`\n",
    "- **BatchNorm**: `'001'`\n",
    "- **MaxPooling**: `'010'`\n",
    "- **Dropout**: `'011'`\n",
    "- **Dense**: `'100'`\n",
    "- **Flatten**: `'101'`\n",
    "\n",
    "## Parameters Encoded by Layer Type\n",
    "\n",
    "### Conv2D\n",
    "The `Conv2D` layer includes the following encoded parameters:\n",
    "\n",
    "- **Number of filters**:\n",
    "  - `32`: `'00'`\n",
    "  - `30`: `'01'`\n",
    "  - `16`: `'10'`\n",
    "  - `8`: `'11'`\n",
    "\n",
    "- **Kernel size**:\n",
    "  - `(3, 3)`: `'0'` (only this size is supported for now)\n",
    "\n",
    "- **Stride**:\n",
    "  - `1`: `'0'`\n",
    "  - `2`: `'1'`\n",
    "\n",
    "- **Padding**:\n",
    "  - `'same'`: `'0'`\n",
    "\n",
    "- **Activation function**:\n",
    "  - `ReLU`: `'00'`\n",
    "  - `LeakyReLU`: `'01'`\n",
    "\n",
    "### BatchNorm\n",
    "The `BatchNorm` layer has no additional parameters, only its layer type encoding (`'001'`).\n",
    "\n",
    "### MaxPooling\n",
    "The `MaxPooling` layer has a single encoded parameter:\n",
    "\n",
    "- **Stride**:\n",
    "  - `1`: `'0'`\n",
    "  - `2`: `'1'`\n",
    "\n",
    "### Dropout\n",
    "The `Dropout` layer has the following encoded dropout rates:\n",
    "\n",
    "- **Dropout Rate**:\n",
    "  - `0.2`: `'00'`\n",
    "  - `0.3`: `'01'`\n",
    "  - `0.5`: `'10'`\n",
    "\n",
    "### Dense\n",
    "The `Dense` layer has the following encoded parameters:\n",
    "\n",
    "- **Number of neurons**:\n",
    "  - `256`: `'00'`\n",
    "  - `128`: `'01'`\n",
    "  - `32`: `'10'`\n",
    "\n",
    "- **Activation function**:\n",
    "  - `ReLU`: `'00'`\n",
    "  - `LeakyReLU`: `'01'`\n",
    "  - `Sigmoid`: `'10'`\n",
    "\n",
    "### Flatten\n",
    "The `Flatten` layer has no additional parameters, only its layer type encoding (`'101'`).\n",
    "\n",
    "## Gray Code Conversion\n",
    "\n",
    "In addition to binary encoding, this module supports **Gray code** conversion for efficient optimization, where only one bit changes between consecutive values.\n",
    "\n",
    "### Function `binary_to_gray`\n",
    "\n",
    "This function converts a binary string to Gray code using the following logic:\n",
    "- The first bit in Gray code is the same as the first bit in binary.\n",
    "- For each subsequent bit, the Gray code bit is the XOR of the current binary bit and the previous binary bit.\n",
    "\n",
    "### Function Parameters\n",
    "\n",
    "- `binary_str`: (str) A string representing a binary number.\n",
    "\n",
    "### Return\n",
    "\n",
    "- `gray_str`: (str) A string representing the corresponding Gray code.\n",
    "\n",
    "### Function `gray_to_binary`\n",
    "\n",
    "This function converts a Gray code string back to its binary equivalent using the following logic:\n",
    "- The first bit of the binary string is the same as the first bit of the Gray code.\n",
    "- For each subsequent bit, the binary bit is the XOR of the previous binary bit and the current Gray code bit.\n",
    "\n",
    "### Function Parameters\n",
    "\n",
    "- `gray_str`: (str) A string representing a Gray code number.\n",
    "\n",
    "### Return\n",
    "\n",
    "- `binary_str`: (str) A string representing the corresponding binary number.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: {'type': 'Conv2D', 'filters': 32, 'strides': 1, 'activation': 'relu'} -> Binary Encoding: 00000000\n",
      "Layer: {'type': 'BatchNorm'} -> Binary Encoding: 00100000\n",
      "Layer: {'type': 'MaxPooling', 'strides': 2} -> Binary Encoding: 01010000\n",
      "Layer: {'type': 'Flatten'} -> Binary Encoding: 10100000\n",
      "Layer: {'type': 'Dense', 'units': 256, 'activation': 'relu'} -> Binary Encoding: 10000000\n",
      "Layer: {'type': 'Dropout', 'rate': 0.5} -> Binary Encoding: 01111000\n",
      "Layer: {'type': 'Dense', 'units': 1, 'activation': 'sigmoid'} -> Binary Encoding: 10011100\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Final Binary Encoding: 000000000010000001010000101000001000000001111000100111001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000\n",
      "Modelo original y decodificado son iguales.\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Función para eliminar campos adicionales como 'padding' y 'kernel_size'\n",
    "def clean_decoded_model(model_dict):\n",
    "    cleaned_layers = []\n",
    "    \n",
    "    # Recorrer cada capa del modelo decodificado\n",
    "    for layer in model_dict['layers']:\n",
    "        # Eliminar las capas 'DontCare'\n",
    "        if layer['type'] == 'DontCare':\n",
    "            continue  # No incluir la capa 'DontCare'\n",
    "        \n",
    "        # Remover 'kernel_size' y 'padding' si están presentes\n",
    "        if 'kernel_size' in layer:\n",
    "            del layer['kernel_size']\n",
    "        if 'padding' in layer:\n",
    "            del layer['padding']\n",
    "        \n",
    "        # Añadir la capa limpia a la lista\n",
    "        cleaned_layers.append(layer)\n",
    "    \n",
    "    # Devolver el modelo con las capas limpias\n",
    "    return {'layers': cleaned_layers}\n",
    "\n",
    "# Función para decodificar la arquitectura del modelo a partir de codificación binaria con longitud fija de 8 bits por gen\n",
    "def decode_model_architecture(encoded_string_binary, gene_length=8):\n",
    "    model_dict = {'layers': []}\n",
    "    index = 0\n",
    "    \n",
    "    # Mientras queden genes por decodificar\n",
    "    while index < len(encoded_string_binary):\n",
    "        # Extraer los próximos 8 bits de la codificación binaria\n",
    "        binary_code_for_layer = encoded_string_binary[index:index+gene_length]\n",
    "       \n",
    "        \n",
    "        # Decodificar una capa completa usando decode_layer_params\n",
    "        decoded_layer = decode_layer_params(binary_code_for_layer)\n",
    "       \n",
    "        \n",
    "        # Agregar la capa decodificada al modelo\n",
    "        model_dict['layers'].append(decoded_layer)\n",
    "        \n",
    "        # Avanzar el índice por el tamaño del gen (8 bits)\n",
    "        index += gene_length\n",
    "    return model_dict\n",
    "\n",
    "\n",
    "# Función para codificar la arquitectura del modelo con relleno hasta 75 alelos\n",
    "def encode_model_architecture(model_dict, max_alleles=600):\n",
    "    encoded_layers_bin = []\n",
    "    total_alleles = 0\n",
    "    \n",
    "    # Codificar cada capa individualmente\n",
    "    for layer in model_dict['layers']:\n",
    "        binary_encoding = encode_layer_params(\n",
    "            layer_type=layer['type'],\n",
    "            filters=layer.get('filters'),\n",
    "            strides=layer.get('strides'),\n",
    "            dropout=layer.get('rate'),\n",
    "            neurons=layer.get('units'),\n",
    "            activation=layer.get('activation', '').lower()\n",
    "        )\n",
    "        \n",
    "        # Contar el número de alelos actuales\n",
    "        total_alleles += len(binary_encoding)\n",
    "        encoded_layers_bin.append(binary_encoding)\n",
    "        print(f\"Layer: {layer} -> Binary Encoding: {binary_encoding}\")\n",
    "    \n",
    "    # Verificar si se necesitan capas DontCare\n",
    "    while total_alleles < max_alleles:\n",
    "        # Codificar una capa DontCare (relleno)\n",
    "        dont_care_encoding = encode_layer_params('DontCare')\n",
    "        encoded_layers_bin.append(dont_care_encoding)\n",
    "        \n",
    "        # Aumentar el número de alelos\n",
    "        total_alleles += len(dont_care_encoding)\n",
    "        print(f\"Added DontCare Layer -> Binary Encoding: {dont_care_encoding}\")\n",
    "    \n",
    "    # Concatenar todas las codificaciones en una sola cadena\n",
    "    final_encoding_bin = ''.join(encoded_layers_bin[:max_alleles])  # Asegurarse de no exceder el número máximo de alelos\n",
    "    print(f\"Final Binary Encoding: {final_encoding_bin}\")\n",
    "\n",
    "    return final_encoding_bin\n",
    "\n",
    "# Función de verificación\n",
    "def verify_model_architecture(original_model):\n",
    "    # Codificar el modelo original\n",
    "    encoded_model = encode_model_architecture(original_model)\n",
    "    \n",
    "    # Decodificar el modelo desde el encoding binario\n",
    "    decoded_model = decode_model_architecture(encoded_model)\n",
    "    \n",
    "    # Limpiar el modelo decodificado para quitar 'kernel_size', 'padding' y capas DontCare\n",
    "    decoded_model_cleaned = clean_decoded_model(decoded_model)\n",
    "    \n",
    "    # Verificar si el modelo original y decodificado son iguales\n",
    "    if original_model == decoded_model_cleaned:\n",
    "        print(\"Modelo original y decodificado son iguales.\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"Original Model:\")\n",
    "        print(original_model)\n",
    "        print(\"\\nDecoded Model (cleaned):\")\n",
    "        print(decoded_model_cleaned)\n",
    "        return False\n",
    "\n",
    "# Ejemplo de uso\n",
    "model_example = {\n",
    "    \"layers\": [\n",
    "        {\"type\": \"Conv2D\", \"filters\": 32, \"strides\": 1, \"activation\": \"relu\"},\n",
    "        {\"type\": \"BatchNorm\"},\n",
    "        {\"type\": \"MaxPooling\", \"strides\": 2},\n",
    "        {\"type\": \"Flatten\"},\n",
    "        {\"type\": \"Dense\", \"units\": 256, \"activation\": \"relu\"},\n",
    "        {\"type\": \"Dropout\", \"rate\": 0.5},\n",
    "        {\"type\": \"Dense\", \"units\": 1, \"activation\": \"sigmoid\"}\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Verificar el modelo\n",
    "print(verify_model_architecture(model_example))\n",
    "\n",
    "# Construcción del modelo en TensorFlow desde el diccionario decodificado\n",
    "\n",
    "class DontCareLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(DontCareLayer, self).__init__()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Devuelve los inputs sin ninguna modificación (identidad)\n",
    "        return inputs\n",
    "\n",
    "# Construcción del modelo en TensorFlow desde el diccionario decodificado\n",
    "def build_tf_model_from_dict(model_dict, input_shape=(28, 28, 3)):\n",
    "    model = Sequential()\n",
    "    \n",
    "    for layer in model_dict['layers']:\n",
    "        if layer['type'] == 'Conv2D':\n",
    "            model.add(Conv2D(filters=layer['filters'], \n",
    "                             kernel_size=(3, 3), \n",
    "                             strides=layer['strides'], \n",
    "                             padding='same', \n",
    "                             activation=layer['activation']))\n",
    "                             \n",
    "        elif layer['type'] == 'DepthwiseConv2D':\n",
    "            model.add(DepthwiseConv2D(kernel_size=(3, 3), \n",
    "                                      strides=layer['strides'], \n",
    "                                      padding='same', \n",
    "                                      activation=layer['activation']))\n",
    "\n",
    "        elif layer['type'] == 'BatchNorm':\n",
    "            model.add(BatchNormalization())\n",
    "            \n",
    "        elif layer['type'] == 'MaxPooling':\n",
    "            model.add(MaxPooling2D(pool_size=(2, 2), \n",
    "                                   strides=layer['strides'], \n",
    "                                   padding='same'))\n",
    "                                   \n",
    "        elif layer['type'] == 'Flatten':\n",
    "            model.add(Flatten())\n",
    "            \n",
    "        elif layer['type'] == 'Dense':\n",
    "            model.add(Dense(units=layer['units'], \n",
    "                            activation=layer['activation']))\n",
    "                            \n",
    "        elif layer['type'] == 'Dropout':\n",
    "            model.add(Dropout(rate=layer['rate']))\n",
    "            \n",
    "        elif layer['type'] == 'DontCare':\n",
    "            # Añadir la capa 'DontCareLayer' como parte del modelo\n",
    "            model.add(DontCareLayer())\n",
    "    \n",
    "    # Construir el modelo especificando el input_shape\n",
    "    model.build(input_shape=(None,) + input_shape)  # Ajusta esto al tamaño de entrada\n",
    "\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo 1: build_CNN_LF_model\n",
    "model_CNN_LF = {\n",
    "    \"layers\": [\n",
    "        {\"type\": \"Conv2D\", \"filters\": 30, \"strides\": 1, \"activation\": \"relu\"},\n",
    "        {\"type\": \"Dropout\", \"rate\": 0.2},\n",
    "        {\"type\": \"BatchNorm\"},\n",
    "        {\"type\": \"MaxPooling\", \"strides\": 2},\n",
    "        {\"type\": \"Conv2D\", \"filters\": 16, \"strides\": 1, \"activation\": \"relu\"},  # Revisar 'filters'\n",
    "        {\"type\": \"Dropout\", \"rate\": 0.2},\n",
    "        {\"type\": \"BatchNorm\"},\n",
    "        {\"type\": \"MaxPooling\", \"strides\": 2},\n",
    "        {\"type\": \"Flatten\"},\n",
    "        {\"type\": \"Dense\", \"units\": 256, \"activation\": \"relu\"},\n",
    "        {\"type\": \"Dropout\", \"rate\": 0.3},\n",
    "        {\"type\": \"Dense\", \"units\": 1, \"activation\": \"sigmoid\"}  # Revisar 'units'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Ejemplo 2: build_reduced_model\n",
    "model_reduced = {\n",
    "    \"layers\": [\n",
    "        {\"type\": \"BatchNorm\"},\n",
    "        {\"type\": \"Conv2D\", \"filters\": 16, \"strides\": 1, \"activation\": \"leaky_relu\"},\n",
    "        {\"type\": \"BatchNorm\"},\n",
    "        {\"type\": \"Conv2D\", \"filters\": 8, \"strides\": 1, \"activation\": \"leaky_relu\"},\n",
    "        {\"type\": \"BatchNorm\"},\n",
    "        {\"type\": \"Flatten\"},\n",
    "        {\"type\": \"Dense\", \"units\": 32, \"activation\": \"leaky_relu\"},\n",
    "        {\"type\": \"Dense\", \"units\": 1, \"activation\": \"sigmoid\"}  # Revisar 'units'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Ejemplo 3: build_Spectro_CNN_model\n",
    "model_spectro_CNN = {\n",
    "    \"layers\": [\n",
    "        {\"type\": \"Conv2D\", \"filters\": 32, \"strides\": 1, \"activation\": \"leaky_relu\"},\n",
    "        {\"type\": \"BatchNorm\"},\n",
    "        {\"type\": \"Conv2D\", \"filters\": 32, \"strides\": 1, \"activation\": \"leaky_relu\"},\n",
    "        {\"type\": \"MaxPooling\", \"strides\": 2}, \n",
    "         \n",
    "         \n",
    "        {\"type\": \"Conv2D\", \"filters\": 32, \"strides\": 1, \"activation\": \"leaky_relu\"},\n",
    "        {\"type\": \"BatchNorm\"},\n",
    "        {\"type\": \"Conv2D\", \"filters\": 32, \"strides\": 1, \"activation\": \"leaky_relu\"},\n",
    "        \n",
    "        {\"type\": \"Conv2D\", \"filters\": 32, \"strides\": 1, \"activation\": \"leaky_relu\"},\n",
    "        {\"type\": \"BatchNorm\"},\n",
    "        {\"type\": \"Conv2D\", \"filters\": 32, \"strides\": 1, \"activation\": \"leaky_relu\"},\n",
    "        \n",
    "        {\"type\": \"Conv2D\", \"filters\": 32, \"strides\": 1, \"activation\": \"leaky_relu\"},\n",
    "        {\"type\": \"BatchNorm\"},\n",
    "        {\"type\": \"Conv2D\", \"filters\": 32, \"strides\": 1, \"activation\": \"leaky_relu\"},\n",
    "        \n",
    "        {\"type\": \"Conv2D\", \"filters\": 32, \"strides\": 1, \"activation\": \"leaky_relu\"},\n",
    "        {\"type\": \"BatchNorm\"},\n",
    "        {\"type\": \"Conv2D\", \"filters\": 32, \"strides\": 1, \"activation\": \"leaky_relu\"},\n",
    "        \n",
    "        {\"type\": \"Conv2D\", \"filters\": 32, \"strides\": 1, \"activation\": \"leaky_relu\"},\n",
    "        {\"type\": \"BatchNorm\"},\n",
    "        {\"type\": \"Conv2D\", \"filters\": 32, \"strides\": 1, \"activation\": \"leaky_relu\"},\n",
    "        \n",
    "        {\"type\": \"Conv2D\", \"filters\": 32, \"strides\": 1, \"activation\": \"leaky_relu\"},\n",
    "        {\"type\": \"BatchNorm\"},\n",
    "        {\"type\": \"Conv2D\", \"filters\": 32, \"strides\": 1, \"activation\": \"leaky_relu\"},\n",
    "        \n",
    "        {\"type\": \"Conv2D\", \"filters\": 32, \"strides\": 1, \"activation\": \"leaky_relu\"},\n",
    "        {\"type\": \"BatchNorm\"},\n",
    "        {\"type\": \"Conv2D\", \"filters\": 32, \"strides\": 1, \"activation\": \"leaky_relu\"},\n",
    "        \n",
    "        {\"type\": \"Conv2D\", \"filters\": 32, \"strides\": 1, \"activation\": \"leaky_relu\"},\n",
    "        {\"type\": \"BatchNorm\"},\n",
    "        {\"type\": \"Conv2D\", \"filters\": 32, \"strides\": 1, \"activation\": \"leaky_relu\"},\n",
    "        \n",
    "        {\"type\": \"Conv2D\", \"filters\": 32, \"strides\": 1, \"activation\": \"leaky_relu\"},\n",
    "        {\"type\": \"BatchNorm\"},\n",
    "        {\"type\": \"Conv2D\", \"filters\": 32, \"strides\": 1, \"activation\": \"leaky_relu\"},\n",
    "        \n",
    "        {\"type\": \"Conv2D\", \"filters\": 32, \"strides\": 1, \"activation\": \"leaky_relu\"},\n",
    "        {\"type\": \"BatchNorm\"},\n",
    "        {\"type\": \"Conv2D\", \"filters\": 32, \"strides\": 1, \"activation\": \"leaky_relu\"},\n",
    "        \n",
    "         {\"type\": \"Conv2D\", \"filters\": 32, \"strides\": 1, \"activation\": \"leaky_relu\"},\n",
    "        {\"type\": \"BatchNorm\"},\n",
    "        {\"type\": \"Conv2D\", \"filters\": 32, \"strides\": 1, \"activation\": \"leaky_relu\"},\n",
    "        \n",
    "        {\"type\": \"Conv2D\", \"filters\": 32, \"strides\": 1, \"activation\": \"leaky_relu\"},\n",
    "        {\"type\": \"BatchNorm\"},\n",
    "        {\"type\": \"Conv2D\", \"filters\": 32, \"strides\": 1, \"activation\": \"leaky_relu\"},\n",
    "        \n",
    "        {\"type\": \"Conv2D\", \"filters\": 32, \"strides\": 1, \"activation\": \"leaky_relu\"},\n",
    "        {\"type\": \"BatchNorm\"},\n",
    "        {\"type\": \"Conv2D\", \"filters\": 32, \"strides\": 1, \"activation\": \"leaky_relu\"},\n",
    "        \n",
    "        {\"type\": \"Conv2D\", \"filters\": 32, \"strides\": 1, \"activation\": \"leaky_relu\"},\n",
    "        {\"type\": \"BatchNorm\"},\n",
    "        {\"type\": \"Conv2D\", \"filters\": 32, \"strides\": 1, \"activation\": \"leaky_relu\"},\n",
    "        \n",
    "        {\"type\": \"Conv2D\", \"filters\": 32, \"strides\": 1, \"activation\": \"leaky_relu\"},\n",
    "        {\"type\": \"BatchNorm\"},\n",
    "        {\"type\": \"Conv2D\", \"filters\": 32, \"strides\": 1, \"activation\": \"leaky_relu\"},\n",
    "        \n",
    "        {\"type\": \"Conv2D\", \"filters\": 32, \"strides\": 1, \"activation\": \"leaky_relu\"},\n",
    "        {\"type\": \"BatchNorm\"},\n",
    "        {\"type\": \"Conv2D\", \"filters\": 32, \"strides\": 1, \"activation\": \"leaky_relu\"},\n",
    "        \n",
    "        {\"type\": \"Conv2D\", \"filters\": 32, \"strides\": 1, \"activation\": \"leaky_relu\"},\n",
    "        {\"type\": \"BatchNorm\"},\n",
    "        {\"type\": \"Conv2D\", \"filters\": 32, \"strides\": 1, \"activation\": \"leaky_relu\"},\n",
    "        \n",
    "        {\"type\": \"Conv2D\", \"filters\": 32, \"strides\": 1, \"activation\": \"leaky_relu\"},\n",
    "        {\"type\": \"BatchNorm\"},\n",
    "        {\"type\": \"Conv2D\", \"filters\": 32, \"strides\": 1, \"activation\": \"leaky_relu\"},\n",
    "        \n",
    "        {\"type\": \"Conv2D\", \"filters\": 32, \"strides\": 1, \"activation\": \"leaky_relu\"},\n",
    "        {\"type\": \"BatchNorm\"},\n",
    "        {\"type\": \"Conv2D\", \"filters\": 32, \"strides\": 1, \"activation\": \"leaky_relu\"},\n",
    "        \n",
    "        {\"type\": \"Conv2D\", \"filters\": 32, \"strides\": 1, \"activation\": \"leaky_relu\"},\n",
    "        {\"type\": \"BatchNorm\"},\n",
    "        {\"type\": \"Conv2D\", \"filters\": 32, \"strides\": 1, \"activation\": \"leaky_relu\"},\n",
    "        \n",
    "         {\"type\": \"Conv2D\", \"filters\": 32, \"strides\": 1, \"activation\": \"leaky_relu\"},\n",
    "        {\"type\": \"BatchNorm\"},\n",
    "        {\"type\": \"Conv2D\", \"filters\": 32, \"strides\": 1, \"activation\": \"leaky_relu\"},\n",
    "        \n",
    "        {\"type\": \"Conv2D\", \"filters\": 32, \"strides\": 1, \"activation\": \"leaky_relu\"},\n",
    "        {\"type\": \"BatchNorm\"},\n",
    "        {\"type\": \"Conv2D\", \"filters\": 32, \"strides\": 1, \"activation\": \"leaky_relu\"},\n",
    "        \n",
    "        {\"type\": \"Conv2D\", \"filters\": 32, \"strides\": 1, \"activation\": \"leaky_relu\"},\n",
    "        {\"type\": \"BatchNorm\"},\n",
    "        {\"type\": \"Conv2D\", \"filters\": 32, \"strides\": 1, \"activation\": \"leaky_relu\"},\n",
    "        \n",
    "        {\"type\": \"Conv2D\", \"filters\": 32, \"strides\": 1, \"activation\": \"leaky_relu\"},\n",
    "        {\"type\": \"BatchNorm\"},\n",
    "        {\"type\": \"Conv2D\", \"filters\": 32, \"strides\": 1, \"activation\": \"leaky_relu\"},\n",
    "        \n",
    "        {\"type\": \"Conv2D\", \"filters\": 32, \"strides\": 1, \"activation\": \"leaky_relu\"},\n",
    "        {\"type\": \"BatchNorm\"},\n",
    "        {\"type\": \"Conv2D\", \"filters\": 32, \"strides\": 1, \"activation\": \"leaky_relu\"},\n",
    "        \n",
    "        {\"type\": \"Conv2D\", \"filters\": 32, \"strides\": 1, \"activation\": \"leaky_relu\"},\n",
    "        {\"type\": \"BatchNorm\"},\n",
    "        {\"type\": \"Conv2D\", \"filters\": 32, \"strides\": 1, \"activation\": \"leaky_relu\"},\n",
    "        \n",
    "        {\"type\": \"Conv2D\", \"filters\": 32, \"strides\": 1, \"activation\": \"leaky_relu\"},\n",
    "        {\"type\": \"BatchNorm\"},\n",
    "        {\"type\": \"Conv2D\", \"filters\": 32, \"strides\": 1, \"activation\": \"leaky_relu\"},\n",
    "        \n",
    "        {\"type\": \"Conv2D\", \"filters\": 32, \"strides\": 1, \"activation\": \"leaky_relu\"},\n",
    "        {\"type\": \"BatchNorm\"},\n",
    "        {\"type\": \"Conv2D\", \"filters\": 32, \"strides\": 1, \"activation\": \"leaky_relu\"},\n",
    "        \n",
    "        {\"type\": \"Conv2D\", \"filters\": 32, \"strides\": 1, \"activation\": \"leaky_relu\"},\n",
    "        {\"type\": \"BatchNorm\"},\n",
    "        {\"type\": \"Conv2D\", \"filters\": 32, \"strides\": 1, \"activation\": \"leaky_relu\"},\n",
    "        \n",
    "        {\"type\": \"Conv2D\", \"filters\": 32, \"strides\": 1, \"activation\": \"leaky_relu\"},\n",
    "        {\"type\": \"BatchNorm\"},\n",
    "        {\"type\": \"Conv2D\", \"filters\": 32, \"strides\": 1, \"activation\": \"leaky_relu\"},\n",
    "        \n",
    "                \n",
    "        {\"type\": \"Dense\", \"units\": 1, \"activation\": \"sigmoid\"}  # Revisar 'units'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Ejemplo 4: Simple Conv2D model\n",
    "model_simple_conv = {\n",
    "    \"layers\": [\n",
    "        {\"type\": \"Conv2D\", \"filters\": 16, \"strides\": 1, \"activation\": \"relu\"},\n",
    "        {\"type\": \"Flatten\"},\n",
    "        {\"type\": \"Dense\", \"units\": 128, \"activation\": \"relu\"},\n",
    "        {\"type\": \"Dense\", \"units\": 10, \"activation\": \"sigmoid\"}  # Revisar 'units'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Ejemplo 5: Simple Dense model\n",
    "model_dense_only = {\n",
    "    \"layers\": [\n",
    "        {\"type\": \"Flatten\"},\n",
    "        {\"type\": \"Dense\", \"units\": 256, \"activation\": \"relu\"},\n",
    "        {\"type\": \"Dense\", \"units\": 128, \"activation\": \"relu\"},\n",
    "        {\"type\": \"Dense\", \"units\": 256, \"activation\": \"sigmoid\"}\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Ejemplo 6: Small CNN model with Dropout\n",
    "model_small_CNN = {\n",
    "    \"layers\": [\n",
    "        {\"type\": \"Conv2D\", \"filters\": 8, \"strides\": 1, \"activation\": \"relu\"},\n",
    "        {\"type\": \"Dropout\", \"rate\": 0.2},\n",
    "        {\"type\": \"MaxPooling\", \"strides\": 2},\n",
    "        {\"type\": \"Flatten\"},\n",
    "        {\"type\": \"Dense\", \"units\": 32, \"activation\": \"relu\"},\n",
    "        {\"type\": \"Dense\", \"units\": 1, \"activation\": \"sigmoid\"}  # Revisar 'units'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Ejemplo 7: Deep CNN model\n",
    "model_deep_CNN = {\n",
    "    \"layers\": [\n",
    "        {\"type\": \"Conv2D\", \"filters\": 32, \"strides\": 1, \"activation\": \"relu\"},\n",
    "        {\"type\": \"Conv2D\", \"filters\": 32, \"strides\": 1, \"activation\": \"relu\"},\n",
    "        {\"type\": \"MaxPooling\", \"strides\": 2},\n",
    "        {\"type\": \"Flatten\"},\n",
    "        {\"type\": \"Dense\", \"units\": 32, \"activation\": \"relu\"},\n",
    "        {\"type\": \"Dense\", \"units\": 10, \"activation\": \"sigmoid\"}  # Revisar 'units'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Ejemplo 8: Basic Dense with Dropout\n",
    "model_dense_dropout = {\n",
    "    \"layers\": [\n",
    "        {\"type\": \"Flatten\"},\n",
    "        {\"type\": \"Dense\", \"units\": 128, \"activation\": \"relu\"},\n",
    "        {\"type\": \"Dropout\", \"rate\": 0.5},\n",
    "        {\"type\": \"Dense\", \"units\": 32, \"activation\": \"tanh\"}\n",
    "    ]\n",
    "}\n",
    "\n",
    "model_with_depthwise = {\n",
    "    \"layers\": [\n",
    "        {\"type\": \"DepthwiseConv2D\", \"filters\": 16, \"strides\": 1, \"activation\": \"relu\"},\n",
    "        {\"type\": \"BatchNorm\"},\n",
    "        {\"type\": \"MaxPooling\", \"strides\": 2},\n",
    "        {\"type\": \"Dense\", \"units\": 128, \"activation\": \"sigmoid\"},\n",
    "        {\"type\": \"Dense\", \"units\": 1, \"activation\": \"tanh\"}\n",
    "    ]\n",
    "}\n",
    "model_with_global_avg_pooling = {\n",
    "    \"layers\": [\n",
    "        {\"type\": \"Conv2D\", \"filters\": 32, \"strides\": 1, \"activation\": \"relu\"},\n",
    "        {\"type\": \"BatchNorm\"},\n",
    "        {\"type\": \"GlobalAveragePooling2D\"},\n",
    "        {\"type\": \"Dense\", \"units\": 32, \"activation\": \"relu\"},\n",
    "        {\"type\": \"Dense\", \"units\": 1, \"activation\": \"sigmoid\"}\n",
    "    ]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para validar si los valores del modelo son válidos\n",
    "def validate_model_layers(model):\n",
    "    for layer in model['layers']:\n",
    "        layer_type = layer['type']\n",
    "        \n",
    "        if layer_type == 'Conv2D':\n",
    "            filters = layer.get('filters')\n",
    "            strides = layer.get('strides')\n",
    "            activation = layer.get('activation')\n",
    "\n",
    "            if filters not in filter_options:\n",
    "                print(f\"Invalid filters value in Conv2D layer: {filters}\")\n",
    "                return False\n",
    "            if strides not in stride_options:\n",
    "                print(f\"Invalid strides value in Conv2D layer: {strides}\")\n",
    "                return False\n",
    "            if activation not in activation_options:\n",
    "                print(f\"Invalid activation value in Conv2D layer: {activation}\")\n",
    "                return False\n",
    "        \n",
    "        elif layer_type == 'Dense':\n",
    "            units = layer.get('units')\n",
    "            activation = layer.get('activation')\n",
    "\n",
    "            if units not in neuron_options:\n",
    "                print(f\"Invalid units value in Dense layer: {units}\")\n",
    "                return False\n",
    "            if activation not in activation_options:\n",
    "                print(f\"Invalid activation value in Dense layer: {activation}\")\n",
    "                return False\n",
    "\n",
    "        elif layer_type == 'Dropout':\n",
    "            rate = layer.get('rate')\n",
    "            if rate not in dropout_options:\n",
    "                print(f\"Invalid dropout rate value: {rate}\")\n",
    "                return False\n",
    "        \n",
    "        elif layer_type == 'MaxPooling':\n",
    "            strides = layer.get('strides')\n",
    "            if strides not in stride_options:\n",
    "                print(f\"Invalid strides value in MaxPooling layer: {strides}\")\n",
    "                return False\n",
    "        \n",
    "        elif layer_type == 'Flatten':\n",
    "            # No se necesitan validaciones adicionales para Flatten\n",
    "            pass\n",
    "\n",
    "        elif layer_type == 'BatchNorm':\n",
    "            # No se necesitan validaciones adicionales para BatchNorm\n",
    "            pass\n",
    "\n",
    "        elif layer_type == 'DepthwiseConv2D':\n",
    "            filters = layer.get('filters')\n",
    "            strides = layer.get('strides')\n",
    "            activation = layer.get('activation')\n",
    "\n",
    "            if filters not in filter_options:\n",
    "                print(f\"Invalid filters value in DepthwiseConv2D layer: {filters}\")\n",
    "                return False\n",
    "            if strides not in stride_options:\n",
    "                print(f\"Invalid strides value in DepthwiseConv2D layer: {strides}\")\n",
    "                return False\n",
    "            if activation not in activation_options:\n",
    "                print(f\"Invalid activation value in DepthwiseConv2D layer: {activation}\")\n",
    "                return False\n",
    "\n",
    "        elif layer_type == 'DontCare':\n",
    "            # La capa 'DontCare' no tiene parámetros, por lo tanto no requiere validación adicional\n",
    "            pass\n",
    "        \n",
    "        layer_type = layer['type']\n",
    "        if layer_type not in layer_type_options:\n",
    "            print(f\"Invalid layer type: {layer_type}\")\n",
    "            return False\n",
    "        \n",
    "    return True\n",
    "\n",
    "\n",
    "# Función para verificar y construir el modelo\n",
    "def run_verification_and_build(model):\n",
    "    if validate_model_layers(model):\n",
    "        print(\"\\nModel is valid. Running verification and building the model:\")\n",
    "        # Verificación del modelo\n",
    "        if verify_model_architecture(model):\n",
    "            print(\"Model verification passed.\")\n",
    "        else:\n",
    "            print(\"Model verification failed.\")\n",
    "        \n",
    "        # Construcción del modelo en TensorFlow desde el diccionario decodificado\n",
    "        decoded_model_dict = decode_model_architecture(encode_model_architecture(model))\n",
    "        tf_model = build_tf_model_from_dict(decoded_model_dict)\n",
    "\n",
    "        # Mostrar la estructura del modelo de TensorFlow\n",
    "        try:\n",
    "            tf_model.summary()\n",
    "        except Exception as e:\n",
    "            print(f\"Error building model: {e}\")\n",
    "    else:\n",
    "        print(\"Model is invalid. Skipping verification and build.\")\n",
    "    print(\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verifications:\n",
      "\n",
      "Model 1: CNN_LF\n",
      "\n",
      "Model is valid. Running verification and building the model:\n",
      "Layer: {'type': 'Conv2D', 'filters': 30, 'strides': 1, 'activation': 'relu'} -> Binary Encoding: 00001000\n",
      "Layer: {'type': 'Dropout', 'rate': 0.2} -> Binary Encoding: 01100000\n",
      "Layer: {'type': 'BatchNorm'} -> Binary Encoding: 00100000\n",
      "Layer: {'type': 'MaxPooling', 'strides': 2} -> Binary Encoding: 01010000\n",
      "Layer: {'type': 'Conv2D', 'filters': 16, 'strides': 1, 'activation': 'relu'} -> Binary Encoding: 00010000\n",
      "Layer: {'type': 'Dropout', 'rate': 0.2} -> Binary Encoding: 01100000\n",
      "Layer: {'type': 'BatchNorm'} -> Binary Encoding: 00100000\n",
      "Layer: {'type': 'MaxPooling', 'strides': 2} -> Binary Encoding: 01010000\n",
      "Layer: {'type': 'Flatten'} -> Binary Encoding: 10100000\n",
      "Layer: {'type': 'Dense', 'units': 256, 'activation': 'relu'} -> Binary Encoding: 10000000\n",
      "Layer: {'type': 'Dropout', 'rate': 0.3} -> Binary Encoding: 01101000\n",
      "Layer: {'type': 'Dense', 'units': 1, 'activation': 'sigmoid'} -> Binary Encoding: 10011100\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Final Binary Encoding: 000010000110000000100000010100000001000001100000001000000101000010100000100000000110100010011100111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000\n",
      "Modelo original y decodificado son iguales.\n",
      "Model verification passed.\n",
      "Layer: {'type': 'Conv2D', 'filters': 30, 'strides': 1, 'activation': 'relu'} -> Binary Encoding: 00001000\n",
      "Layer: {'type': 'Dropout', 'rate': 0.2} -> Binary Encoding: 01100000\n",
      "Layer: {'type': 'BatchNorm'} -> Binary Encoding: 00100000\n",
      "Layer: {'type': 'MaxPooling', 'strides': 2} -> Binary Encoding: 01010000\n",
      "Layer: {'type': 'Conv2D', 'filters': 16, 'strides': 1, 'activation': 'relu'} -> Binary Encoding: 00010000\n",
      "Layer: {'type': 'Dropout', 'rate': 0.2} -> Binary Encoding: 01100000\n",
      "Layer: {'type': 'BatchNorm'} -> Binary Encoding: 00100000\n",
      "Layer: {'type': 'MaxPooling', 'strides': 2} -> Binary Encoding: 01010000\n",
      "Layer: {'type': 'Flatten'} -> Binary Encoding: 10100000\n",
      "Layer: {'type': 'Dense', 'units': 256, 'activation': 'relu'} -> Binary Encoding: 10000000\n",
      "Layer: {'type': 'Dropout', 'rate': 0.3} -> Binary Encoding: 01101000\n",
      "Layer: {'type': 'Dense', 'units': 1, 'activation': 'sigmoid'} -> Binary Encoding: 10011100\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Final Binary Encoding: 000010000110000000100000010100000001000001100000001000000101000010100000100000000110100010011100111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000\n",
      "Model: \"sequential_94\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_861 (Conv2D)         (None, 28, 28, 30)        840       \n",
      "                                                                 \n",
      " dropout_65 (Dropout)        (None, 28, 28, 30)        0         \n",
      "                                                                 \n",
      " batch_normalization_478 (Ba  (None, 28, 28, 30)       120       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " max_pooling2d_65 (MaxPoolin  (None, 14, 14, 30)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_862 (Conv2D)         (None, 14, 14, 16)        4336      \n",
      "                                                                 \n",
      " dropout_66 (Dropout)        (None, 14, 14, 16)        0         \n",
      "                                                                 \n",
      " batch_normalization_479 (Ba  (None, 14, 14, 16)       64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " max_pooling2d_66 (MaxPoolin  (None, 7, 7, 16)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_65 (Flatten)        (None, 784)               0         \n",
      "                                                                 \n",
      " dense_187 (Dense)           (None, 256)               200960    \n",
      "                                                                 \n",
      " dropout_67 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_188 (Dense)           (None, 1)                 257       \n",
      "                                                                 \n",
      " dont_care_layer_434 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_435 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_436 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_437 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_438 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_439 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_440 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_441 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_442 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_443 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_444 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_445 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_446 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_447 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_448 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_449 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_450 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_451 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_452 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_453 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_454 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_455 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_456 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_457 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_458 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_459 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_460 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_461 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_462 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_463 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_464 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_465 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_466 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_467 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_468 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_469 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_470 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_471 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_472 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_473 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_474 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_475 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_476 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_477 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_478 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_479 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_480 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_481 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_482 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_483 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_484 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_485 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_486 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_487 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_488 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_489 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_490 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_491 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_492 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_493 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_494 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_495 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_496 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 206,577\n",
      "Trainable params: 206,485\n",
      "Non-trainable params: 92\n",
      "_________________________________________________________________\n",
      "\n",
      "\n",
      "\n",
      "Model 2: Reduced\n",
      "\n",
      "Model is valid. Running verification and building the model:\n",
      "Layer: {'type': 'BatchNorm'} -> Binary Encoding: 00100000\n",
      "Layer: {'type': 'Conv2D', 'filters': 16, 'strides': 1, 'activation': 'leaky_relu'} -> Binary Encoding: 00010001\n",
      "Layer: {'type': 'BatchNorm'} -> Binary Encoding: 00100000\n",
      "Layer: {'type': 'Conv2D', 'filters': 8, 'strides': 1, 'activation': 'leaky_relu'} -> Binary Encoding: 00011001\n",
      "Layer: {'type': 'BatchNorm'} -> Binary Encoding: 00100000\n",
      "Layer: {'type': 'Flatten'} -> Binary Encoding: 10100000\n",
      "Layer: {'type': 'Dense', 'units': 32, 'activation': 'leaky_relu'} -> Binary Encoding: 10010010\n",
      "Layer: {'type': 'Dense', 'units': 1, 'activation': 'sigmoid'} -> Binary Encoding: 10011100\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Final Binary Encoding: 001000000001000100100000000110010010000010100000100100101001110011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000\n",
      "Modelo original y decodificado son iguales.\n",
      "Model verification passed.\n",
      "Layer: {'type': 'BatchNorm'} -> Binary Encoding: 00100000\n",
      "Layer: {'type': 'Conv2D', 'filters': 16, 'strides': 1, 'activation': 'leaky_relu'} -> Binary Encoding: 00010001\n",
      "Layer: {'type': 'BatchNorm'} -> Binary Encoding: 00100000\n",
      "Layer: {'type': 'Conv2D', 'filters': 8, 'strides': 1, 'activation': 'leaky_relu'} -> Binary Encoding: 00011001\n",
      "Layer: {'type': 'BatchNorm'} -> Binary Encoding: 00100000\n",
      "Layer: {'type': 'Flatten'} -> Binary Encoding: 10100000\n",
      "Layer: {'type': 'Dense', 'units': 32, 'activation': 'leaky_relu'} -> Binary Encoding: 10010010\n",
      "Layer: {'type': 'Dense', 'units': 1, 'activation': 'sigmoid'} -> Binary Encoding: 10011100\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Final Binary Encoding: 001000000001000100100000000110010010000010100000100100101001110011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000\n",
      "Model: \"sequential_95\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization_480 (Ba  (None, 28, 28, 3)        12        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " conv2d_863 (Conv2D)         (None, 28, 28, 16)        448       \n",
      "                                                                 \n",
      " batch_normalization_481 (Ba  (None, 28, 28, 16)       64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " conv2d_864 (Conv2D)         (None, 28, 28, 8)         1160      \n",
      "                                                                 \n",
      " batch_normalization_482 (Ba  (None, 28, 28, 8)        32        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " flatten_66 (Flatten)        (None, 6272)              0         \n",
      "                                                                 \n",
      " dense_189 (Dense)           (None, 32)                200736    \n",
      "                                                                 \n",
      " dense_190 (Dense)           (None, 1)                 33        \n",
      "                                                                 \n",
      " dont_care_layer_497 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_498 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_499 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_500 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_501 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_502 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_503 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_504 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_505 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_506 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_507 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_508 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_509 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_510 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_511 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_512 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_513 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_514 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_515 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_516 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_517 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_518 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_519 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_520 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_521 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_522 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_523 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_524 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_525 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_526 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_527 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_528 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_529 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_530 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_531 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_532 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_533 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_534 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_535 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_536 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_537 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_538 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_539 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_540 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_541 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_542 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_543 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_544 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_545 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_546 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_547 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_548 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_549 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_550 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_551 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_552 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_553 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_554 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_555 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_556 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_557 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_558 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_559 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_560 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_561 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_562 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_563 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 202,485\n",
      "Trainable params: 202,431\n",
      "Non-trainable params: 54\n",
      "_________________________________________________________________\n",
      "\n",
      "\n",
      "\n",
      "Model 3: Spectro CNN\n",
      "\n",
      "Model is valid. Running verification and building the model:\n",
      "Layer: {'type': 'Conv2D', 'filters': 32, 'strides': 1, 'activation': 'leaky_relu'} -> Binary Encoding: 00000001\n",
      "Layer: {'type': 'BatchNorm'} -> Binary Encoding: 00100000\n",
      "Layer: {'type': 'Conv2D', 'filters': 32, 'strides': 1, 'activation': 'leaky_relu'} -> Binary Encoding: 00000001\n",
      "Layer: {'type': 'MaxPooling', 'strides': 2} -> Binary Encoding: 01010000\n",
      "Layer: {'type': 'Conv2D', 'filters': 32, 'strides': 1, 'activation': 'leaky_relu'} -> Binary Encoding: 00000001\n",
      "Layer: {'type': 'BatchNorm'} -> Binary Encoding: 00100000\n",
      "Layer: {'type': 'Conv2D', 'filters': 32, 'strides': 1, 'activation': 'leaky_relu'} -> Binary Encoding: 00000001\n",
      "Layer: {'type': 'Conv2D', 'filters': 32, 'strides': 1, 'activation': 'leaky_relu'} -> Binary Encoding: 00000001\n",
      "Layer: {'type': 'BatchNorm'} -> Binary Encoding: 00100000\n",
      "Layer: {'type': 'Conv2D', 'filters': 32, 'strides': 1, 'activation': 'leaky_relu'} -> Binary Encoding: 00000001\n",
      "Layer: {'type': 'Conv2D', 'filters': 32, 'strides': 1, 'activation': 'leaky_relu'} -> Binary Encoding: 00000001\n",
      "Layer: {'type': 'BatchNorm'} -> Binary Encoding: 00100000\n",
      "Layer: {'type': 'Conv2D', 'filters': 32, 'strides': 1, 'activation': 'leaky_relu'} -> Binary Encoding: 00000001\n",
      "Layer: {'type': 'Conv2D', 'filters': 32, 'strides': 1, 'activation': 'leaky_relu'} -> Binary Encoding: 00000001\n",
      "Layer: {'type': 'BatchNorm'} -> Binary Encoding: 00100000\n",
      "Layer: {'type': 'Conv2D', 'filters': 32, 'strides': 1, 'activation': 'leaky_relu'} -> Binary Encoding: 00000001\n",
      "Layer: {'type': 'Conv2D', 'filters': 32, 'strides': 1, 'activation': 'leaky_relu'} -> Binary Encoding: 00000001\n",
      "Layer: {'type': 'BatchNorm'} -> Binary Encoding: 00100000\n",
      "Layer: {'type': 'Conv2D', 'filters': 32, 'strides': 1, 'activation': 'leaky_relu'} -> Binary Encoding: 00000001\n",
      "Layer: {'type': 'Conv2D', 'filters': 32, 'strides': 1, 'activation': 'leaky_relu'} -> Binary Encoding: 00000001\n",
      "Layer: {'type': 'BatchNorm'} -> Binary Encoding: 00100000\n",
      "Layer: {'type': 'Conv2D', 'filters': 32, 'strides': 1, 'activation': 'leaky_relu'} -> Binary Encoding: 00000001\n",
      "Layer: {'type': 'Conv2D', 'filters': 32, 'strides': 1, 'activation': 'leaky_relu'} -> Binary Encoding: 00000001\n",
      "Layer: {'type': 'BatchNorm'} -> Binary Encoding: 00100000\n",
      "Layer: {'type': 'Conv2D', 'filters': 32, 'strides': 1, 'activation': 'leaky_relu'} -> Binary Encoding: 00000001\n",
      "Layer: {'type': 'Conv2D', 'filters': 32, 'strides': 1, 'activation': 'leaky_relu'} -> Binary Encoding: 00000001\n",
      "Layer: {'type': 'BatchNorm'} -> Binary Encoding: 00100000\n",
      "Layer: {'type': 'Conv2D', 'filters': 32, 'strides': 1, 'activation': 'leaky_relu'} -> Binary Encoding: 00000001\n",
      "Layer: {'type': 'Conv2D', 'filters': 32, 'strides': 1, 'activation': 'leaky_relu'} -> Binary Encoding: 00000001\n",
      "Layer: {'type': 'BatchNorm'} -> Binary Encoding: 00100000\n",
      "Layer: {'type': 'Conv2D', 'filters': 32, 'strides': 1, 'activation': 'leaky_relu'} -> Binary Encoding: 00000001\n",
      "Layer: {'type': 'Conv2D', 'filters': 32, 'strides': 1, 'activation': 'leaky_relu'} -> Binary Encoding: 00000001\n",
      "Layer: {'type': 'BatchNorm'} -> Binary Encoding: 00100000\n",
      "Layer: {'type': 'Conv2D', 'filters': 32, 'strides': 1, 'activation': 'leaky_relu'} -> Binary Encoding: 00000001\n",
      "Layer: {'type': 'Conv2D', 'filters': 32, 'strides': 1, 'activation': 'leaky_relu'} -> Binary Encoding: 00000001\n",
      "Layer: {'type': 'BatchNorm'} -> Binary Encoding: 00100000\n",
      "Layer: {'type': 'Conv2D', 'filters': 32, 'strides': 1, 'activation': 'leaky_relu'} -> Binary Encoding: 00000001\n",
      "Layer: {'type': 'Conv2D', 'filters': 32, 'strides': 1, 'activation': 'leaky_relu'} -> Binary Encoding: 00000001\n",
      "Layer: {'type': 'BatchNorm'} -> Binary Encoding: 00100000\n",
      "Layer: {'type': 'Conv2D', 'filters': 32, 'strides': 1, 'activation': 'leaky_relu'} -> Binary Encoding: 00000001\n",
      "Layer: {'type': 'Conv2D', 'filters': 32, 'strides': 1, 'activation': 'leaky_relu'} -> Binary Encoding: 00000001\n",
      "Layer: {'type': 'BatchNorm'} -> Binary Encoding: 00100000\n",
      "Layer: {'type': 'Conv2D', 'filters': 32, 'strides': 1, 'activation': 'leaky_relu'} -> Binary Encoding: 00000001\n",
      "Layer: {'type': 'Conv2D', 'filters': 32, 'strides': 1, 'activation': 'leaky_relu'} -> Binary Encoding: 00000001\n",
      "Layer: {'type': 'BatchNorm'} -> Binary Encoding: 00100000\n",
      "Layer: {'type': 'Conv2D', 'filters': 32, 'strides': 1, 'activation': 'leaky_relu'} -> Binary Encoding: 00000001\n",
      "Layer: {'type': 'Conv2D', 'filters': 32, 'strides': 1, 'activation': 'leaky_relu'} -> Binary Encoding: 00000001\n",
      "Layer: {'type': 'BatchNorm'} -> Binary Encoding: 00100000\n",
      "Layer: {'type': 'Conv2D', 'filters': 32, 'strides': 1, 'activation': 'leaky_relu'} -> Binary Encoding: 00000001\n",
      "Layer: {'type': 'Conv2D', 'filters': 32, 'strides': 1, 'activation': 'leaky_relu'} -> Binary Encoding: 00000001\n",
      "Layer: {'type': 'BatchNorm'} -> Binary Encoding: 00100000\n",
      "Layer: {'type': 'Conv2D', 'filters': 32, 'strides': 1, 'activation': 'leaky_relu'} -> Binary Encoding: 00000001\n",
      "Layer: {'type': 'Conv2D', 'filters': 32, 'strides': 1, 'activation': 'leaky_relu'} -> Binary Encoding: 00000001\n",
      "Layer: {'type': 'BatchNorm'} -> Binary Encoding: 00100000\n",
      "Layer: {'type': 'Conv2D', 'filters': 32, 'strides': 1, 'activation': 'leaky_relu'} -> Binary Encoding: 00000001\n",
      "Layer: {'type': 'Conv2D', 'filters': 32, 'strides': 1, 'activation': 'leaky_relu'} -> Binary Encoding: 00000001\n",
      "Layer: {'type': 'BatchNorm'} -> Binary Encoding: 00100000\n",
      "Layer: {'type': 'Conv2D', 'filters': 32, 'strides': 1, 'activation': 'leaky_relu'} -> Binary Encoding: 00000001\n",
      "Layer: {'type': 'Conv2D', 'filters': 32, 'strides': 1, 'activation': 'leaky_relu'} -> Binary Encoding: 00000001\n",
      "Layer: {'type': 'BatchNorm'} -> Binary Encoding: 00100000\n",
      "Layer: {'type': 'Conv2D', 'filters': 32, 'strides': 1, 'activation': 'leaky_relu'} -> Binary Encoding: 00000001\n",
      "Layer: {'type': 'Conv2D', 'filters': 32, 'strides': 1, 'activation': 'leaky_relu'} -> Binary Encoding: 00000001\n",
      "Layer: {'type': 'BatchNorm'} -> Binary Encoding: 00100000\n",
      "Layer: {'type': 'Conv2D', 'filters': 32, 'strides': 1, 'activation': 'leaky_relu'} -> Binary Encoding: 00000001\n",
      "Layer: {'type': 'Conv2D', 'filters': 32, 'strides': 1, 'activation': 'leaky_relu'} -> Binary Encoding: 00000001\n",
      "Layer: {'type': 'BatchNorm'} -> Binary Encoding: 00100000\n",
      "Layer: {'type': 'Conv2D', 'filters': 32, 'strides': 1, 'activation': 'leaky_relu'} -> Binary Encoding: 00000001\n",
      "Layer: {'type': 'Conv2D', 'filters': 32, 'strides': 1, 'activation': 'leaky_relu'} -> Binary Encoding: 00000001\n",
      "Layer: {'type': 'BatchNorm'} -> Binary Encoding: 00100000\n",
      "Layer: {'type': 'Conv2D', 'filters': 32, 'strides': 1, 'activation': 'leaky_relu'} -> Binary Encoding: 00000001\n",
      "Layer: {'type': 'Conv2D', 'filters': 32, 'strides': 1, 'activation': 'leaky_relu'} -> Binary Encoding: 00000001\n",
      "Layer: {'type': 'BatchNorm'} -> Binary Encoding: 00100000\n",
      "Layer: {'type': 'Conv2D', 'filters': 32, 'strides': 1, 'activation': 'leaky_relu'} -> Binary Encoding: 00000001\n",
      "Layer: {'type': 'Conv2D', 'filters': 32, 'strides': 1, 'activation': 'leaky_relu'} -> Binary Encoding: 00000001\n",
      "Layer: {'type': 'BatchNorm'} -> Binary Encoding: 00100000\n",
      "Layer: {'type': 'Conv2D', 'filters': 32, 'strides': 1, 'activation': 'leaky_relu'} -> Binary Encoding: 00000001\n",
      "Layer: {'type': 'Conv2D', 'filters': 32, 'strides': 1, 'activation': 'leaky_relu'} -> Binary Encoding: 00000001\n",
      "Layer: {'type': 'BatchNorm'} -> Binary Encoding: 00100000\n",
      "Layer: {'type': 'Conv2D', 'filters': 32, 'strides': 1, 'activation': 'leaky_relu'} -> Binary Encoding: 00000001\n",
      "Layer: {'type': 'Conv2D', 'filters': 32, 'strides': 1, 'activation': 'leaky_relu'} -> Binary Encoding: 00000001\n",
      "Layer: {'type': 'BatchNorm'} -> Binary Encoding: 00100000\n",
      "Layer: {'type': 'Conv2D', 'filters': 32, 'strides': 1, 'activation': 'leaky_relu'} -> Binary Encoding: 00000001\n",
      "Layer: {'type': 'Conv2D', 'filters': 32, 'strides': 1, 'activation': 'leaky_relu'} -> Binary Encoding: 00000001\n",
      "Layer: {'type': 'BatchNorm'} -> Binary Encoding: 00100000\n",
      "Layer: {'type': 'Conv2D', 'filters': 32, 'strides': 1, 'activation': 'leaky_relu'} -> Binary Encoding: 00000001\n",
      "Layer: {'type': 'Conv2D', 'filters': 32, 'strides': 1, 'activation': 'leaky_relu'} -> Binary Encoding: 00000001\n",
      "Layer: {'type': 'BatchNorm'} -> Binary Encoding: 00100000\n",
      "Layer: {'type': 'Conv2D', 'filters': 32, 'strides': 1, 'activation': 'leaky_relu'} -> Binary Encoding: 00000001\n",
      "Layer: {'type': 'Conv2D', 'filters': 32, 'strides': 1, 'activation': 'leaky_relu'} -> Binary Encoding: 00000001\n",
      "Layer: {'type': 'BatchNorm'} -> Binary Encoding: 00100000\n",
      "Layer: {'type': 'Conv2D', 'filters': 32, 'strides': 1, 'activation': 'leaky_relu'} -> Binary Encoding: 00000001\n",
      "Layer: {'type': 'Conv2D', 'filters': 32, 'strides': 1, 'activation': 'leaky_relu'} -> Binary Encoding: 00000001\n",
      "Layer: {'type': 'BatchNorm'} -> Binary Encoding: 00100000\n",
      "Layer: {'type': 'Conv2D', 'filters': 32, 'strides': 1, 'activation': 'leaky_relu'} -> Binary Encoding: 00000001\n",
      "Layer: {'type': 'Dense', 'units': 1, 'activation': 'sigmoid'} -> Binary Encoding: 10011100\n",
      "Final Binary Encoding: 0000000100100000000000010101000000000001001000000000000100000001001000000000000100000001001000000000000100000001001000000000000100000001001000000000000100000001001000000000000100000001001000000000000100000001001000000000000100000001001000000000000100000001001000000000000100000001001000000000000100000001001000000000000100000001001000000000000100000001001000000000000100000001001000000000000100000001001000000000000100000001001000000000000100000001001000000000000100000001001000000000000100000001001000000000000100000001001000000000000100000001001000000000000100000001001000000000000100000001001000000000000100000001001000000000000100000001001000000000000100000001001000000000000100000001001000000000000100000001001000000000000100000001001000000000000110011100\n",
      "Modelo original y decodificado son iguales.\n",
      "Model verification passed.\n",
      "Layer: {'type': 'Conv2D', 'filters': 32, 'strides': 1, 'activation': 'leaky_relu'} -> Binary Encoding: 00000001\n",
      "Layer: {'type': 'BatchNorm'} -> Binary Encoding: 00100000\n",
      "Layer: {'type': 'Conv2D', 'filters': 32, 'strides': 1, 'activation': 'leaky_relu'} -> Binary Encoding: 00000001\n",
      "Layer: {'type': 'MaxPooling', 'strides': 2} -> Binary Encoding: 01010000\n",
      "Layer: {'type': 'Conv2D', 'filters': 32, 'strides': 1, 'activation': 'leaky_relu'} -> Binary Encoding: 00000001\n",
      "Layer: {'type': 'BatchNorm'} -> Binary Encoding: 00100000\n",
      "Layer: {'type': 'Conv2D', 'filters': 32, 'strides': 1, 'activation': 'leaky_relu'} -> Binary Encoding: 00000001\n",
      "Layer: {'type': 'Conv2D', 'filters': 32, 'strides': 1, 'activation': 'leaky_relu'} -> Binary Encoding: 00000001\n",
      "Layer: {'type': 'BatchNorm'} -> Binary Encoding: 00100000\n",
      "Layer: {'type': 'Conv2D', 'filters': 32, 'strides': 1, 'activation': 'leaky_relu'} -> Binary Encoding: 00000001\n",
      "Layer: {'type': 'Conv2D', 'filters': 32, 'strides': 1, 'activation': 'leaky_relu'} -> Binary Encoding: 00000001\n",
      "Layer: {'type': 'BatchNorm'} -> Binary Encoding: 00100000\n",
      "Layer: {'type': 'Conv2D', 'filters': 32, 'strides': 1, 'activation': 'leaky_relu'} -> Binary Encoding: 00000001\n",
      "Layer: {'type': 'Conv2D', 'filters': 32, 'strides': 1, 'activation': 'leaky_relu'} -> Binary Encoding: 00000001\n",
      "Layer: {'type': 'BatchNorm'} -> Binary Encoding: 00100000\n",
      "Layer: {'type': 'Conv2D', 'filters': 32, 'strides': 1, 'activation': 'leaky_relu'} -> Binary Encoding: 00000001\n",
      "Layer: {'type': 'Conv2D', 'filters': 32, 'strides': 1, 'activation': 'leaky_relu'} -> Binary Encoding: 00000001\n",
      "Layer: {'type': 'BatchNorm'} -> Binary Encoding: 00100000\n",
      "Layer: {'type': 'Conv2D', 'filters': 32, 'strides': 1, 'activation': 'leaky_relu'} -> Binary Encoding: 00000001\n",
      "Layer: {'type': 'Conv2D', 'filters': 32, 'strides': 1, 'activation': 'leaky_relu'} -> Binary Encoding: 00000001\n",
      "Layer: {'type': 'BatchNorm'} -> Binary Encoding: 00100000\n",
      "Layer: {'type': 'Conv2D', 'filters': 32, 'strides': 1, 'activation': 'leaky_relu'} -> Binary Encoding: 00000001\n",
      "Layer: {'type': 'Conv2D', 'filters': 32, 'strides': 1, 'activation': 'leaky_relu'} -> Binary Encoding: 00000001\n",
      "Layer: {'type': 'BatchNorm'} -> Binary Encoding: 00100000\n",
      "Layer: {'type': 'Conv2D', 'filters': 32, 'strides': 1, 'activation': 'leaky_relu'} -> Binary Encoding: 00000001\n",
      "Layer: {'type': 'Conv2D', 'filters': 32, 'strides': 1, 'activation': 'leaky_relu'} -> Binary Encoding: 00000001\n",
      "Layer: {'type': 'BatchNorm'} -> Binary Encoding: 00100000\n",
      "Layer: {'type': 'Conv2D', 'filters': 32, 'strides': 1, 'activation': 'leaky_relu'} -> Binary Encoding: 00000001\n",
      "Layer: {'type': 'Conv2D', 'filters': 32, 'strides': 1, 'activation': 'leaky_relu'} -> Binary Encoding: 00000001\n",
      "Layer: {'type': 'BatchNorm'} -> Binary Encoding: 00100000\n",
      "Layer: {'type': 'Conv2D', 'filters': 32, 'strides': 1, 'activation': 'leaky_relu'} -> Binary Encoding: 00000001\n",
      "Layer: {'type': 'Conv2D', 'filters': 32, 'strides': 1, 'activation': 'leaky_relu'} -> Binary Encoding: 00000001\n",
      "Layer: {'type': 'BatchNorm'} -> Binary Encoding: 00100000\n",
      "Layer: {'type': 'Conv2D', 'filters': 32, 'strides': 1, 'activation': 'leaky_relu'} -> Binary Encoding: 00000001\n",
      "Layer: {'type': 'Conv2D', 'filters': 32, 'strides': 1, 'activation': 'leaky_relu'} -> Binary Encoding: 00000001\n",
      "Layer: {'type': 'BatchNorm'} -> Binary Encoding: 00100000\n",
      "Layer: {'type': 'Conv2D', 'filters': 32, 'strides': 1, 'activation': 'leaky_relu'} -> Binary Encoding: 00000001\n",
      "Layer: {'type': 'Conv2D', 'filters': 32, 'strides': 1, 'activation': 'leaky_relu'} -> Binary Encoding: 00000001\n",
      "Layer: {'type': 'BatchNorm'} -> Binary Encoding: 00100000\n",
      "Layer: {'type': 'Conv2D', 'filters': 32, 'strides': 1, 'activation': 'leaky_relu'} -> Binary Encoding: 00000001\n",
      "Layer: {'type': 'Conv2D', 'filters': 32, 'strides': 1, 'activation': 'leaky_relu'} -> Binary Encoding: 00000001\n",
      "Layer: {'type': 'BatchNorm'} -> Binary Encoding: 00100000\n",
      "Layer: {'type': 'Conv2D', 'filters': 32, 'strides': 1, 'activation': 'leaky_relu'} -> Binary Encoding: 00000001\n",
      "Layer: {'type': 'Conv2D', 'filters': 32, 'strides': 1, 'activation': 'leaky_relu'} -> Binary Encoding: 00000001\n",
      "Layer: {'type': 'BatchNorm'} -> Binary Encoding: 00100000\n",
      "Layer: {'type': 'Conv2D', 'filters': 32, 'strides': 1, 'activation': 'leaky_relu'} -> Binary Encoding: 00000001\n",
      "Layer: {'type': 'Conv2D', 'filters': 32, 'strides': 1, 'activation': 'leaky_relu'} -> Binary Encoding: 00000001\n",
      "Layer: {'type': 'BatchNorm'} -> Binary Encoding: 00100000\n",
      "Layer: {'type': 'Conv2D', 'filters': 32, 'strides': 1, 'activation': 'leaky_relu'} -> Binary Encoding: 00000001\n",
      "Layer: {'type': 'Conv2D', 'filters': 32, 'strides': 1, 'activation': 'leaky_relu'} -> Binary Encoding: 00000001\n",
      "Layer: {'type': 'BatchNorm'} -> Binary Encoding: 00100000\n",
      "Layer: {'type': 'Conv2D', 'filters': 32, 'strides': 1, 'activation': 'leaky_relu'} -> Binary Encoding: 00000001\n",
      "Layer: {'type': 'Conv2D', 'filters': 32, 'strides': 1, 'activation': 'leaky_relu'} -> Binary Encoding: 00000001\n",
      "Layer: {'type': 'BatchNorm'} -> Binary Encoding: 00100000\n",
      "Layer: {'type': 'Conv2D', 'filters': 32, 'strides': 1, 'activation': 'leaky_relu'} -> Binary Encoding: 00000001\n",
      "Layer: {'type': 'Conv2D', 'filters': 32, 'strides': 1, 'activation': 'leaky_relu'} -> Binary Encoding: 00000001\n",
      "Layer: {'type': 'BatchNorm'} -> Binary Encoding: 00100000\n",
      "Layer: {'type': 'Conv2D', 'filters': 32, 'strides': 1, 'activation': 'leaky_relu'} -> Binary Encoding: 00000001\n",
      "Layer: {'type': 'Conv2D', 'filters': 32, 'strides': 1, 'activation': 'leaky_relu'} -> Binary Encoding: 00000001\n",
      "Layer: {'type': 'BatchNorm'} -> Binary Encoding: 00100000\n",
      "Layer: {'type': 'Conv2D', 'filters': 32, 'strides': 1, 'activation': 'leaky_relu'} -> Binary Encoding: 00000001\n",
      "Layer: {'type': 'Conv2D', 'filters': 32, 'strides': 1, 'activation': 'leaky_relu'} -> Binary Encoding: 00000001\n",
      "Layer: {'type': 'BatchNorm'} -> Binary Encoding: 00100000\n",
      "Layer: {'type': 'Conv2D', 'filters': 32, 'strides': 1, 'activation': 'leaky_relu'} -> Binary Encoding: 00000001\n",
      "Layer: {'type': 'Conv2D', 'filters': 32, 'strides': 1, 'activation': 'leaky_relu'} -> Binary Encoding: 00000001\n",
      "Layer: {'type': 'BatchNorm'} -> Binary Encoding: 00100000\n",
      "Layer: {'type': 'Conv2D', 'filters': 32, 'strides': 1, 'activation': 'leaky_relu'} -> Binary Encoding: 00000001\n",
      "Layer: {'type': 'Conv2D', 'filters': 32, 'strides': 1, 'activation': 'leaky_relu'} -> Binary Encoding: 00000001\n",
      "Layer: {'type': 'BatchNorm'} -> Binary Encoding: 00100000\n",
      "Layer: {'type': 'Conv2D', 'filters': 32, 'strides': 1, 'activation': 'leaky_relu'} -> Binary Encoding: 00000001\n",
      "Layer: {'type': 'Conv2D', 'filters': 32, 'strides': 1, 'activation': 'leaky_relu'} -> Binary Encoding: 00000001\n",
      "Layer: {'type': 'BatchNorm'} -> Binary Encoding: 00100000\n",
      "Layer: {'type': 'Conv2D', 'filters': 32, 'strides': 1, 'activation': 'leaky_relu'} -> Binary Encoding: 00000001\n",
      "Layer: {'type': 'Conv2D', 'filters': 32, 'strides': 1, 'activation': 'leaky_relu'} -> Binary Encoding: 00000001\n",
      "Layer: {'type': 'BatchNorm'} -> Binary Encoding: 00100000\n",
      "Layer: {'type': 'Conv2D', 'filters': 32, 'strides': 1, 'activation': 'leaky_relu'} -> Binary Encoding: 00000001\n",
      "Layer: {'type': 'Conv2D', 'filters': 32, 'strides': 1, 'activation': 'leaky_relu'} -> Binary Encoding: 00000001\n",
      "Layer: {'type': 'BatchNorm'} -> Binary Encoding: 00100000\n",
      "Layer: {'type': 'Conv2D', 'filters': 32, 'strides': 1, 'activation': 'leaky_relu'} -> Binary Encoding: 00000001\n",
      "Layer: {'type': 'Conv2D', 'filters': 32, 'strides': 1, 'activation': 'leaky_relu'} -> Binary Encoding: 00000001\n",
      "Layer: {'type': 'BatchNorm'} -> Binary Encoding: 00100000\n",
      "Layer: {'type': 'Conv2D', 'filters': 32, 'strides': 1, 'activation': 'leaky_relu'} -> Binary Encoding: 00000001\n",
      "Layer: {'type': 'Conv2D', 'filters': 32, 'strides': 1, 'activation': 'leaky_relu'} -> Binary Encoding: 00000001\n",
      "Layer: {'type': 'BatchNorm'} -> Binary Encoding: 00100000\n",
      "Layer: {'type': 'Conv2D', 'filters': 32, 'strides': 1, 'activation': 'leaky_relu'} -> Binary Encoding: 00000001\n",
      "Layer: {'type': 'Conv2D', 'filters': 32, 'strides': 1, 'activation': 'leaky_relu'} -> Binary Encoding: 00000001\n",
      "Layer: {'type': 'BatchNorm'} -> Binary Encoding: 00100000\n",
      "Layer: {'type': 'Conv2D', 'filters': 32, 'strides': 1, 'activation': 'leaky_relu'} -> Binary Encoding: 00000001\n",
      "Layer: {'type': 'Conv2D', 'filters': 32, 'strides': 1, 'activation': 'leaky_relu'} -> Binary Encoding: 00000001\n",
      "Layer: {'type': 'BatchNorm'} -> Binary Encoding: 00100000\n",
      "Layer: {'type': 'Conv2D', 'filters': 32, 'strides': 1, 'activation': 'leaky_relu'} -> Binary Encoding: 00000001\n",
      "Layer: {'type': 'Conv2D', 'filters': 32, 'strides': 1, 'activation': 'leaky_relu'} -> Binary Encoding: 00000001\n",
      "Layer: {'type': 'BatchNorm'} -> Binary Encoding: 00100000\n",
      "Layer: {'type': 'Conv2D', 'filters': 32, 'strides': 1, 'activation': 'leaky_relu'} -> Binary Encoding: 00000001\n",
      "Layer: {'type': 'Dense', 'units': 1, 'activation': 'sigmoid'} -> Binary Encoding: 10011100\n",
      "Final Binary Encoding: 0000000100100000000000010101000000000001001000000000000100000001001000000000000100000001001000000000000100000001001000000000000100000001001000000000000100000001001000000000000100000001001000000000000100000001001000000000000100000001001000000000000100000001001000000000000100000001001000000000000100000001001000000000000100000001001000000000000100000001001000000000000100000001001000000000000100000001001000000000000100000001001000000000000100000001001000000000000100000001001000000000000100000001001000000000000100000001001000000000000100000001001000000000000100000001001000000000000100000001001000000000000100000001001000000000000100000001001000000000000100000001001000000000000100000001001000000000000100000001001000000000000100000001001000000000000110011100\n",
      "Model: \"sequential_96\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_865 (Conv2D)         (None, 28, 28, 32)        896       \n",
      "                                                                 \n",
      " batch_normalization_483 (Ba  (None, 28, 28, 32)       128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " conv2d_866 (Conv2D)         (None, 28, 28, 32)        9248      \n",
      "                                                                 \n",
      " max_pooling2d_67 (MaxPoolin  (None, 14, 14, 32)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_867 (Conv2D)         (None, 14, 14, 32)        9248      \n",
      "                                                                 \n",
      " batch_normalization_484 (Ba  (None, 14, 14, 32)       128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " conv2d_868 (Conv2D)         (None, 14, 14, 32)        9248      \n",
      "                                                                 \n",
      " conv2d_869 (Conv2D)         (None, 14, 14, 32)        9248      \n",
      "                                                                 \n",
      " batch_normalization_485 (Ba  (None, 14, 14, 32)       128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " conv2d_870 (Conv2D)         (None, 14, 14, 32)        9248      \n",
      "                                                                 \n",
      " conv2d_871 (Conv2D)         (None, 14, 14, 32)        9248      \n",
      "                                                                 \n",
      " batch_normalization_486 (Ba  (None, 14, 14, 32)       128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " conv2d_872 (Conv2D)         (None, 14, 14, 32)        9248      \n",
      "                                                                 \n",
      " conv2d_873 (Conv2D)         (None, 14, 14, 32)        9248      \n",
      "                                                                 \n",
      " batch_normalization_487 (Ba  (None, 14, 14, 32)       128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " conv2d_874 (Conv2D)         (None, 14, 14, 32)        9248      \n",
      "                                                                 \n",
      " conv2d_875 (Conv2D)         (None, 14, 14, 32)        9248      \n",
      "                                                                 \n",
      " batch_normalization_488 (Ba  (None, 14, 14, 32)       128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " conv2d_876 (Conv2D)         (None, 14, 14, 32)        9248      \n",
      "                                                                 \n",
      " conv2d_877 (Conv2D)         (None, 14, 14, 32)        9248      \n",
      "                                                                 \n",
      " batch_normalization_489 (Ba  (None, 14, 14, 32)       128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " conv2d_878 (Conv2D)         (None, 14, 14, 32)        9248      \n",
      "                                                                 \n",
      " conv2d_879 (Conv2D)         (None, 14, 14, 32)        9248      \n",
      "                                                                 \n",
      " batch_normalization_490 (Ba  (None, 14, 14, 32)       128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " conv2d_880 (Conv2D)         (None, 14, 14, 32)        9248      \n",
      "                                                                 \n",
      " conv2d_881 (Conv2D)         (None, 14, 14, 32)        9248      \n",
      "                                                                 \n",
      " batch_normalization_491 (Ba  (None, 14, 14, 32)       128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " conv2d_882 (Conv2D)         (None, 14, 14, 32)        9248      \n",
      "                                                                 \n",
      " conv2d_883 (Conv2D)         (None, 14, 14, 32)        9248      \n",
      "                                                                 \n",
      " batch_normalization_492 (Ba  (None, 14, 14, 32)       128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " conv2d_884 (Conv2D)         (None, 14, 14, 32)        9248      \n",
      "                                                                 \n",
      " conv2d_885 (Conv2D)         (None, 14, 14, 32)        9248      \n",
      "                                                                 \n",
      " batch_normalization_493 (Ba  (None, 14, 14, 32)       128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " conv2d_886 (Conv2D)         (None, 14, 14, 32)        9248      \n",
      "                                                                 \n",
      " conv2d_887 (Conv2D)         (None, 14, 14, 32)        9248      \n",
      "                                                                 \n",
      " batch_normalization_494 (Ba  (None, 14, 14, 32)       128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " conv2d_888 (Conv2D)         (None, 14, 14, 32)        9248      \n",
      "                                                                 \n",
      " conv2d_889 (Conv2D)         (None, 14, 14, 32)        9248      \n",
      "                                                                 \n",
      " batch_normalization_495 (Ba  (None, 14, 14, 32)       128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " conv2d_890 (Conv2D)         (None, 14, 14, 32)        9248      \n",
      "                                                                 \n",
      " conv2d_891 (Conv2D)         (None, 14, 14, 32)        9248      \n",
      "                                                                 \n",
      " batch_normalization_496 (Ba  (None, 14, 14, 32)       128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " conv2d_892 (Conv2D)         (None, 14, 14, 32)        9248      \n",
      "                                                                 \n",
      " conv2d_893 (Conv2D)         (None, 14, 14, 32)        9248      \n",
      "                                                                 \n",
      " batch_normalization_497 (Ba  (None, 14, 14, 32)       128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " conv2d_894 (Conv2D)         (None, 14, 14, 32)        9248      \n",
      "                                                                 \n",
      " conv2d_895 (Conv2D)         (None, 14, 14, 32)        9248      \n",
      "                                                                 \n",
      " batch_normalization_498 (Ba  (None, 14, 14, 32)       128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " conv2d_896 (Conv2D)         (None, 14, 14, 32)        9248      \n",
      "                                                                 \n",
      " conv2d_897 (Conv2D)         (None, 14, 14, 32)        9248      \n",
      "                                                                 \n",
      " batch_normalization_499 (Ba  (None, 14, 14, 32)       128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " conv2d_898 (Conv2D)         (None, 14, 14, 32)        9248      \n",
      "                                                                 \n",
      " conv2d_899 (Conv2D)         (None, 14, 14, 32)        9248      \n",
      "                                                                 \n",
      " batch_normalization_500 (Ba  (None, 14, 14, 32)       128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " conv2d_900 (Conv2D)         (None, 14, 14, 32)        9248      \n",
      "                                                                 \n",
      " conv2d_901 (Conv2D)         (None, 14, 14, 32)        9248      \n",
      "                                                                 \n",
      " batch_normalization_501 (Ba  (None, 14, 14, 32)       128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " conv2d_902 (Conv2D)         (None, 14, 14, 32)        9248      \n",
      "                                                                 \n",
      " conv2d_903 (Conv2D)         (None, 14, 14, 32)        9248      \n",
      "                                                                 \n",
      " batch_normalization_502 (Ba  (None, 14, 14, 32)       128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " conv2d_904 (Conv2D)         (None, 14, 14, 32)        9248      \n",
      "                                                                 \n",
      " conv2d_905 (Conv2D)         (None, 14, 14, 32)        9248      \n",
      "                                                                 \n",
      " batch_normalization_503 (Ba  (None, 14, 14, 32)       128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " conv2d_906 (Conv2D)         (None, 14, 14, 32)        9248      \n",
      "                                                                 \n",
      " conv2d_907 (Conv2D)         (None, 14, 14, 32)        9248      \n",
      "                                                                 \n",
      " batch_normalization_504 (Ba  (None, 14, 14, 32)       128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " conv2d_908 (Conv2D)         (None, 14, 14, 32)        9248      \n",
      "                                                                 \n",
      " conv2d_909 (Conv2D)         (None, 14, 14, 32)        9248      \n",
      "                                                                 \n",
      " batch_normalization_505 (Ba  (None, 14, 14, 32)       128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " conv2d_910 (Conv2D)         (None, 14, 14, 32)        9248      \n",
      "                                                                 \n",
      " conv2d_911 (Conv2D)         (None, 14, 14, 32)        9248      \n",
      "                                                                 \n",
      " batch_normalization_506 (Ba  (None, 14, 14, 32)       128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " conv2d_912 (Conv2D)         (None, 14, 14, 32)        9248      \n",
      "                                                                 \n",
      " conv2d_913 (Conv2D)         (None, 14, 14, 32)        9248      \n",
      "                                                                 \n",
      " batch_normalization_507 (Ba  (None, 14, 14, 32)       128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " conv2d_914 (Conv2D)         (None, 14, 14, 32)        9248      \n",
      "                                                                 \n",
      " conv2d_915 (Conv2D)         (None, 14, 14, 32)        9248      \n",
      "                                                                 \n",
      " batch_normalization_508 (Ba  (None, 14, 14, 32)       128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " conv2d_916 (Conv2D)         (None, 14, 14, 32)        9248      \n",
      "                                                                 \n",
      " conv2d_917 (Conv2D)         (None, 14, 14, 32)        9248      \n",
      "                                                                 \n",
      " batch_normalization_509 (Ba  (None, 14, 14, 32)       128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " conv2d_918 (Conv2D)         (None, 14, 14, 32)        9248      \n",
      "                                                                 \n",
      " conv2d_919 (Conv2D)         (None, 14, 14, 32)        9248      \n",
      "                                                                 \n",
      " batch_normalization_510 (Ba  (None, 14, 14, 32)       128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " conv2d_920 (Conv2D)         (None, 14, 14, 32)        9248      \n",
      "                                                                 \n",
      " conv2d_921 (Conv2D)         (None, 14, 14, 32)        9248      \n",
      "                                                                 \n",
      " batch_normalization_511 (Ba  (None, 14, 14, 32)       128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " conv2d_922 (Conv2D)         (None, 14, 14, 32)        9248      \n",
      "                                                                 \n",
      " conv2d_923 (Conv2D)         (None, 14, 14, 32)        9248      \n",
      "                                                                 \n",
      " batch_normalization_512 (Ba  (None, 14, 14, 32)       128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " conv2d_924 (Conv2D)         (None, 14, 14, 32)        9248      \n",
      "                                                                 \n",
      " conv2d_925 (Conv2D)         (None, 14, 14, 32)        9248      \n",
      "                                                                 \n",
      " batch_normalization_513 (Ba  (None, 14, 14, 32)       128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " conv2d_926 (Conv2D)         (None, 14, 14, 32)        9248      \n",
      "                                                                 \n",
      " dense_191 (Dense)           (None, 14, 14, 1)         33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 569,025\n",
      "Trainable params: 567,041\n",
      "Non-trainable params: 1,984\n",
      "_________________________________________________________________\n",
      "\n",
      "\n",
      "\n",
      "Model 4: Simple Conv2D\n",
      "Invalid units value in Dense layer: 10\n",
      "Model is invalid. Skipping verification and build.\n",
      "\n",
      "\n",
      "\n",
      "Model 5: Dense Only\n",
      "\n",
      "Model is valid. Running verification and building the model:\n",
      "Layer: {'type': 'Flatten'} -> Binary Encoding: 10100000\n",
      "Layer: {'type': 'Dense', 'units': 256, 'activation': 'relu'} -> Binary Encoding: 10000000\n",
      "Layer: {'type': 'Dense', 'units': 128, 'activation': 'relu'} -> Binary Encoding: 10001000\n",
      "Layer: {'type': 'Dense', 'units': 256, 'activation': 'sigmoid'} -> Binary Encoding: 10000100\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Final Binary Encoding: 101000001000000010001000100001001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000\n",
      "Modelo original y decodificado son iguales.\n",
      "Model verification passed.\n",
      "Layer: {'type': 'Flatten'} -> Binary Encoding: 10100000\n",
      "Layer: {'type': 'Dense', 'units': 256, 'activation': 'relu'} -> Binary Encoding: 10000000\n",
      "Layer: {'type': 'Dense', 'units': 128, 'activation': 'relu'} -> Binary Encoding: 10001000\n",
      "Layer: {'type': 'Dense', 'units': 256, 'activation': 'sigmoid'} -> Binary Encoding: 10000100\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Final Binary Encoding: 101000001000000010001000100001001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000\n",
      "Model: \"sequential_97\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_67 (Flatten)        (None, 2352)              0         \n",
      "                                                                 \n",
      " dense_192 (Dense)           (None, 256)               602368    \n",
      "                                                                 \n",
      " dense_193 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_194 (Dense)           (None, 256)               33024     \n",
      "                                                                 \n",
      " dont_care_layer_564 (DontCa  (None, 256)              0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_565 (DontCa  (None, 256)              0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_566 (DontCa  (None, 256)              0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_567 (DontCa  (None, 256)              0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_568 (DontCa  (None, 256)              0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_569 (DontCa  (None, 256)              0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_570 (DontCa  (None, 256)              0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_571 (DontCa  (None, 256)              0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_572 (DontCa  (None, 256)              0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_573 (DontCa  (None, 256)              0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_574 (DontCa  (None, 256)              0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_575 (DontCa  (None, 256)              0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_576 (DontCa  (None, 256)              0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_577 (DontCa  (None, 256)              0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_578 (DontCa  (None, 256)              0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_579 (DontCa  (None, 256)              0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_580 (DontCa  (None, 256)              0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_581 (DontCa  (None, 256)              0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_582 (DontCa  (None, 256)              0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_583 (DontCa  (None, 256)              0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_584 (DontCa  (None, 256)              0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_585 (DontCa  (None, 256)              0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_586 (DontCa  (None, 256)              0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_587 (DontCa  (None, 256)              0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_588 (DontCa  (None, 256)              0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_589 (DontCa  (None, 256)              0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_590 (DontCa  (None, 256)              0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_591 (DontCa  (None, 256)              0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_592 (DontCa  (None, 256)              0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_593 (DontCa  (None, 256)              0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_594 (DontCa  (None, 256)              0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_595 (DontCa  (None, 256)              0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_596 (DontCa  (None, 256)              0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_597 (DontCa  (None, 256)              0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_598 (DontCa  (None, 256)              0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_599 (DontCa  (None, 256)              0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_600 (DontCa  (None, 256)              0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_601 (DontCa  (None, 256)              0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_602 (DontCa  (None, 256)              0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_603 (DontCa  (None, 256)              0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_604 (DontCa  (None, 256)              0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_605 (DontCa  (None, 256)              0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_606 (DontCa  (None, 256)              0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_607 (DontCa  (None, 256)              0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_608 (DontCa  (None, 256)              0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_609 (DontCa  (None, 256)              0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_610 (DontCa  (None, 256)              0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_611 (DontCa  (None, 256)              0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_612 (DontCa  (None, 256)              0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_613 (DontCa  (None, 256)              0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_614 (DontCa  (None, 256)              0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_615 (DontCa  (None, 256)              0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_616 (DontCa  (None, 256)              0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_617 (DontCa  (None, 256)              0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_618 (DontCa  (None, 256)              0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_619 (DontCa  (None, 256)              0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_620 (DontCa  (None, 256)              0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_621 (DontCa  (None, 256)              0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_622 (DontCa  (None, 256)              0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_623 (DontCa  (None, 256)              0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_624 (DontCa  (None, 256)              0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_625 (DontCa  (None, 256)              0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_626 (DontCa  (None, 256)              0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_627 (DontCa  (None, 256)              0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_628 (DontCa  (None, 256)              0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_629 (DontCa  (None, 256)              0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_630 (DontCa  (None, 256)              0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_631 (DontCa  (None, 256)              0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_632 (DontCa  (None, 256)              0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_633 (DontCa  (None, 256)              0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_634 (DontCa  (None, 256)              0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 668,288\n",
      "Trainable params: 668,288\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "\n",
      "\n",
      "Model 6: Small CNN\n",
      "\n",
      "Model is valid. Running verification and building the model:\n",
      "Layer: {'type': 'Conv2D', 'filters': 8, 'strides': 1, 'activation': 'relu'} -> Binary Encoding: 00011000\n",
      "Layer: {'type': 'Dropout', 'rate': 0.2} -> Binary Encoding: 01100000\n",
      "Layer: {'type': 'MaxPooling', 'strides': 2} -> Binary Encoding: 01010000\n",
      "Layer: {'type': 'Flatten'} -> Binary Encoding: 10100000\n",
      "Layer: {'type': 'Dense', 'units': 32, 'activation': 'relu'} -> Binary Encoding: 10010000\n",
      "Layer: {'type': 'Dense', 'units': 1, 'activation': 'sigmoid'} -> Binary Encoding: 10011100\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Final Binary Encoding: 000110000110000001010000101000001001000010011100111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000\n",
      "Modelo original y decodificado son iguales.\n",
      "Model verification passed.\n",
      "Layer: {'type': 'Conv2D', 'filters': 8, 'strides': 1, 'activation': 'relu'} -> Binary Encoding: 00011000\n",
      "Layer: {'type': 'Dropout', 'rate': 0.2} -> Binary Encoding: 01100000\n",
      "Layer: {'type': 'MaxPooling', 'strides': 2} -> Binary Encoding: 01010000\n",
      "Layer: {'type': 'Flatten'} -> Binary Encoding: 10100000\n",
      "Layer: {'type': 'Dense', 'units': 32, 'activation': 'relu'} -> Binary Encoding: 10010000\n",
      "Layer: {'type': 'Dense', 'units': 1, 'activation': 'sigmoid'} -> Binary Encoding: 10011100\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Final Binary Encoding: 000110000110000001010000101000001001000010011100111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000\n",
      "Model: \"sequential_98\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_927 (Conv2D)         (None, 28, 28, 8)         224       \n",
      "                                                                 \n",
      " dropout_68 (Dropout)        (None, 28, 28, 8)         0         \n",
      "                                                                 \n",
      " max_pooling2d_68 (MaxPoolin  (None, 14, 14, 8)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_68 (Flatten)        (None, 1568)              0         \n",
      "                                                                 \n",
      " dense_195 (Dense)           (None, 32)                50208     \n",
      "                                                                 \n",
      " dense_196 (Dense)           (None, 1)                 33        \n",
      "                                                                 \n",
      " dont_care_layer_635 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_636 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_637 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_638 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_639 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_640 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_641 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_642 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_643 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_644 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_645 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_646 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_647 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_648 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_649 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_650 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_651 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_652 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_653 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_654 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_655 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_656 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_657 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_658 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_659 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_660 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_661 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_662 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_663 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_664 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_665 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_666 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_667 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_668 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_669 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_670 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_671 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_672 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_673 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_674 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_675 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_676 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_677 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_678 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_679 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_680 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_681 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_682 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_683 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_684 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_685 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_686 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_687 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_688 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_689 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_690 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_691 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_692 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_693 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_694 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_695 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_696 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_697 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_698 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_699 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_700 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_701 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_702 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_703 (DontCa  (None, 1)                0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 50,465\n",
      "Trainable params: 50,465\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "\n",
      "\n",
      "Model 7: Deep CNN\n",
      "Invalid units value in Dense layer: 10\n",
      "Model is invalid. Skipping verification and build.\n",
      "\n",
      "\n",
      "\n",
      "Model 8: Dense with Dropout\n",
      "\n",
      "Model is valid. Running verification and building the model:\n",
      "Layer: {'type': 'Flatten'} -> Binary Encoding: 10100000\n",
      "Layer: {'type': 'Dense', 'units': 128, 'activation': 'relu'} -> Binary Encoding: 10001000\n",
      "Layer: {'type': 'Dropout', 'rate': 0.5} -> Binary Encoding: 01111000\n",
      "Layer: {'type': 'Dense', 'units': 32, 'activation': 'tanh'} -> Binary Encoding: 10010110\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Final Binary Encoding: 101000001000100001111000100101101110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000\n",
      "Modelo original y decodificado son iguales.\n",
      "Model verification passed.\n",
      "Layer: {'type': 'Flatten'} -> Binary Encoding: 10100000\n",
      "Layer: {'type': 'Dense', 'units': 128, 'activation': 'relu'} -> Binary Encoding: 10001000\n",
      "Layer: {'type': 'Dropout', 'rate': 0.5} -> Binary Encoding: 01111000\n",
      "Layer: {'type': 'Dense', 'units': 32, 'activation': 'tanh'} -> Binary Encoding: 10010110\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Final Binary Encoding: 101000001000100001111000100101101110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000\n",
      "Model: \"sequential_99\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_69 (Flatten)        (None, 2352)              0         \n",
      "                                                                 \n",
      " dense_197 (Dense)           (None, 128)               301184    \n",
      "                                                                 \n",
      " dropout_69 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_198 (Dense)           (None, 32)                4128      \n",
      "                                                                 \n",
      " dont_care_layer_704 (DontCa  (None, 32)               0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_705 (DontCa  (None, 32)               0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_706 (DontCa  (None, 32)               0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_707 (DontCa  (None, 32)               0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_708 (DontCa  (None, 32)               0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_709 (DontCa  (None, 32)               0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_710 (DontCa  (None, 32)               0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_711 (DontCa  (None, 32)               0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_712 (DontCa  (None, 32)               0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_713 (DontCa  (None, 32)               0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_714 (DontCa  (None, 32)               0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_715 (DontCa  (None, 32)               0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_716 (DontCa  (None, 32)               0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_717 (DontCa  (None, 32)               0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_718 (DontCa  (None, 32)               0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_719 (DontCa  (None, 32)               0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_720 (DontCa  (None, 32)               0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_721 (DontCa  (None, 32)               0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_722 (DontCa  (None, 32)               0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_723 (DontCa  (None, 32)               0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_724 (DontCa  (None, 32)               0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_725 (DontCa  (None, 32)               0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_726 (DontCa  (None, 32)               0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_727 (DontCa  (None, 32)               0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_728 (DontCa  (None, 32)               0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_729 (DontCa  (None, 32)               0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_730 (DontCa  (None, 32)               0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_731 (DontCa  (None, 32)               0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_732 (DontCa  (None, 32)               0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_733 (DontCa  (None, 32)               0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_734 (DontCa  (None, 32)               0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_735 (DontCa  (None, 32)               0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_736 (DontCa  (None, 32)               0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_737 (DontCa  (None, 32)               0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_738 (DontCa  (None, 32)               0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_739 (DontCa  (None, 32)               0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_740 (DontCa  (None, 32)               0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_741 (DontCa  (None, 32)               0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_742 (DontCa  (None, 32)               0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_743 (DontCa  (None, 32)               0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_744 (DontCa  (None, 32)               0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_745 (DontCa  (None, 32)               0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_746 (DontCa  (None, 32)               0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_747 (DontCa  (None, 32)               0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_748 (DontCa  (None, 32)               0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_749 (DontCa  (None, 32)               0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_750 (DontCa  (None, 32)               0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_751 (DontCa  (None, 32)               0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_752 (DontCa  (None, 32)               0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_753 (DontCa  (None, 32)               0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_754 (DontCa  (None, 32)               0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_755 (DontCa  (None, 32)               0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_756 (DontCa  (None, 32)               0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_757 (DontCa  (None, 32)               0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_758 (DontCa  (None, 32)               0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_759 (DontCa  (None, 32)               0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_760 (DontCa  (None, 32)               0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_761 (DontCa  (None, 32)               0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_762 (DontCa  (None, 32)               0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_763 (DontCa  (None, 32)               0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_764 (DontCa  (None, 32)               0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_765 (DontCa  (None, 32)               0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_766 (DontCa  (None, 32)               0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_767 (DontCa  (None, 32)               0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_768 (DontCa  (None, 32)               0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_769 (DontCa  (None, 32)               0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_770 (DontCa  (None, 32)               0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_771 (DontCa  (None, 32)               0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_772 (DontCa  (None, 32)               0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_773 (DontCa  (None, 32)               0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_774 (DontCa  (None, 32)               0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 305,312\n",
      "Trainable params: 305,312\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "\n",
      "\n",
      "Model with DepthwiseConv2D\n",
      "\n",
      "Model is valid. Running verification and building the model:\n",
      "Layer: {'type': 'DepthwiseConv2D', 'filters': 16, 'strides': 1, 'activation': 'relu'} -> Binary Encoding: 11010000\n",
      "Layer: {'type': 'BatchNorm'} -> Binary Encoding: 00100000\n",
      "Layer: {'type': 'MaxPooling', 'strides': 2} -> Binary Encoding: 01010000\n",
      "Layer: {'type': 'Dense', 'units': 128, 'activation': 'sigmoid'} -> Binary Encoding: 10001100\n",
      "Layer: {'type': 'Dense', 'units': 1, 'activation': 'tanh'} -> Binary Encoding: 10011110\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Final Binary Encoding: 110100000010000001010000100011001001111011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000\n",
      "Modelo original y decodificado son iguales.\n",
      "Model verification passed.\n",
      "Layer: {'type': 'DepthwiseConv2D', 'filters': 16, 'strides': 1, 'activation': 'relu'} -> Binary Encoding: 11010000\n",
      "Layer: {'type': 'BatchNorm'} -> Binary Encoding: 00100000\n",
      "Layer: {'type': 'MaxPooling', 'strides': 2} -> Binary Encoding: 01010000\n",
      "Layer: {'type': 'Dense', 'units': 128, 'activation': 'sigmoid'} -> Binary Encoding: 10001100\n",
      "Layer: {'type': 'Dense', 'units': 1, 'activation': 'tanh'} -> Binary Encoding: 10011110\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Added DontCare Layer -> Binary Encoding: 11100000\n",
      "Final Binary Encoding: 110100000010000001010000100011001001111011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000111000001110000011100000\n",
      "Model: \"sequential_100\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " depthwise_conv2d_9 (Depthwi  (None, 28, 28, 3)        30        \n",
      " seConv2D)                                                       \n",
      "                                                                 \n",
      " batch_normalization_514 (Ba  (None, 28, 28, 3)        12        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " max_pooling2d_69 (MaxPoolin  (None, 14, 14, 3)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dense_199 (Dense)           (None, 14, 14, 128)       512       \n",
      "                                                                 \n",
      " dense_200 (Dense)           (None, 14, 14, 1)         129       \n",
      "                                                                 \n",
      " dont_care_layer_775 (DontCa  (None, 14, 14, 1)        0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_776 (DontCa  (None, 14, 14, 1)        0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_777 (DontCa  (None, 14, 14, 1)        0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_778 (DontCa  (None, 14, 14, 1)        0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_779 (DontCa  (None, 14, 14, 1)        0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_780 (DontCa  (None, 14, 14, 1)        0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_781 (DontCa  (None, 14, 14, 1)        0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_782 (DontCa  (None, 14, 14, 1)        0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_783 (DontCa  (None, 14, 14, 1)        0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_784 (DontCa  (None, 14, 14, 1)        0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_785 (DontCa  (None, 14, 14, 1)        0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_786 (DontCa  (None, 14, 14, 1)        0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_787 (DontCa  (None, 14, 14, 1)        0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_788 (DontCa  (None, 14, 14, 1)        0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_789 (DontCa  (None, 14, 14, 1)        0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_790 (DontCa  (None, 14, 14, 1)        0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_791 (DontCa  (None, 14, 14, 1)        0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_792 (DontCa  (None, 14, 14, 1)        0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_793 (DontCa  (None, 14, 14, 1)        0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_794 (DontCa  (None, 14, 14, 1)        0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_795 (DontCa  (None, 14, 14, 1)        0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_796 (DontCa  (None, 14, 14, 1)        0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_797 (DontCa  (None, 14, 14, 1)        0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_798 (DontCa  (None, 14, 14, 1)        0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_799 (DontCa  (None, 14, 14, 1)        0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_800 (DontCa  (None, 14, 14, 1)        0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_801 (DontCa  (None, 14, 14, 1)        0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_802 (DontCa  (None, 14, 14, 1)        0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_803 (DontCa  (None, 14, 14, 1)        0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_804 (DontCa  (None, 14, 14, 1)        0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_805 (DontCa  (None, 14, 14, 1)        0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_806 (DontCa  (None, 14, 14, 1)        0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_807 (DontCa  (None, 14, 14, 1)        0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_808 (DontCa  (None, 14, 14, 1)        0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_809 (DontCa  (None, 14, 14, 1)        0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_810 (DontCa  (None, 14, 14, 1)        0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_811 (DontCa  (None, 14, 14, 1)        0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_812 (DontCa  (None, 14, 14, 1)        0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_813 (DontCa  (None, 14, 14, 1)        0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_814 (DontCa  (None, 14, 14, 1)        0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_815 (DontCa  (None, 14, 14, 1)        0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_816 (DontCa  (None, 14, 14, 1)        0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_817 (DontCa  (None, 14, 14, 1)        0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_818 (DontCa  (None, 14, 14, 1)        0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_819 (DontCa  (None, 14, 14, 1)        0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_820 (DontCa  (None, 14, 14, 1)        0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_821 (DontCa  (None, 14, 14, 1)        0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_822 (DontCa  (None, 14, 14, 1)        0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_823 (DontCa  (None, 14, 14, 1)        0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_824 (DontCa  (None, 14, 14, 1)        0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_825 (DontCa  (None, 14, 14, 1)        0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_826 (DontCa  (None, 14, 14, 1)        0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_827 (DontCa  (None, 14, 14, 1)        0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_828 (DontCa  (None, 14, 14, 1)        0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_829 (DontCa  (None, 14, 14, 1)        0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_830 (DontCa  (None, 14, 14, 1)        0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_831 (DontCa  (None, 14, 14, 1)        0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_832 (DontCa  (None, 14, 14, 1)        0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_833 (DontCa  (None, 14, 14, 1)        0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_834 (DontCa  (None, 14, 14, 1)        0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_835 (DontCa  (None, 14, 14, 1)        0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_836 (DontCa  (None, 14, 14, 1)        0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_837 (DontCa  (None, 14, 14, 1)        0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_838 (DontCa  (None, 14, 14, 1)        0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_839 (DontCa  (None, 14, 14, 1)        0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_840 (DontCa  (None, 14, 14, 1)        0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_841 (DontCa  (None, 14, 14, 1)        0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_842 (DontCa  (None, 14, 14, 1)        0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_843 (DontCa  (None, 14, 14, 1)        0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      " dont_care_layer_844 (DontCa  (None, 14, 14, 1)        0         \n",
      " reLayer)                                                        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 683\n",
      "Trainable params: 677\n",
      "Non-trainable params: 6\n",
      "_________________________________________________________________\n",
      "\n",
      "\n",
      "\n",
      "Model with GlobalAveragePooling2D\n",
      "Invalid layer type: GlobalAveragePooling2D\n",
      "Model is invalid. Skipping verification and build.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Verificación de los modelos\n",
    "print(\"Verifications:\")\n",
    "\n",
    "print(\"\\nModel 1: CNN_LF\")\n",
    "run_verification_and_build(model_CNN_LF)  # Ejemplo 1\n",
    "\n",
    "print(\"\\nModel 2: Reduced\")\n",
    "run_verification_and_build(model_reduced)  # Ejemplo 2\n",
    "\n",
    "print(\"\\nModel 3: Spectro CNN\")\n",
    "run_verification_and_build(model_spectro_CNN)  # Ejemplo 3\n",
    "\n",
    "print(\"\\nModel 4: Simple Conv2D\")\n",
    "run_verification_and_build(model_simple_conv)  # Ejemplo 4\n",
    "\n",
    "print(\"\\nModel 5: Dense Only\")\n",
    "run_verification_and_build(model_dense_only)  # Ejemplo 5\n",
    "\n",
    "print(\"\\nModel 6: Small CNN\")\n",
    "run_verification_and_build(model_small_CNN)  # Ejemplo 6\n",
    "\n",
    "print(\"\\nModel 7: Deep CNN\")\n",
    "run_verification_and_build(model_deep_CNN)  # Ejemplo 7\n",
    "\n",
    "print(\"\\nModel 8: Dense with Dropout\")\n",
    "run_verification_and_build(model_dense_dropout)  # Ejemplo 8\n",
    "\n",
    "print(\"\\nModel with DepthwiseConv2D\")\n",
    "run_verification_and_build(model_with_depthwise)\n",
    "\n",
    "print(\"\\nModel with GlobalAveragePooling2D\")\n",
    "run_verification_and_build(model_with_global_avg_pooling)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
