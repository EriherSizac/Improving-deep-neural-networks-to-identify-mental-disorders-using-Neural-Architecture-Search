{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-13T23:16:36.750996Z",
     "start_time": "2024-07-13T23:16:36.739508Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import torchaudio\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.callbacks import EarlyStopping, History\n",
    "from keras.optimizers import Adadelta\n",
    "import os\n",
    "import copy\n",
    "from livelossplot import PlotLossesKeras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.notebook import tqdm\n",
    "import tensorflow_addons as tfa\n",
    "tf.config.list_physical_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6bfdb6f1-6921-4d72-9c30-dae1b8fd37e7",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-13T23:16:36.870984Z",
     "start_time": "2024-07-13T23:16:36.759589Z"
    }
   },
   "outputs": [],
   "source": [
    "#LOAD AUDIOS\n",
    "def load_audios(folder_path):\n",
    "    LOADED = {}\n",
    "    samplerate = torchaudio.load(folder_path+'/'+os.listdir(folder_path)[0])[1]\n",
    "\n",
    "    for filename in tqdm(os.listdir(folder_path), desc='LOAD AUDIOS'):\n",
    "        if not filename.startswith('.'):\n",
    "            audio = torchaudio.load(folder_path+'/'+filename, normalize=True)\n",
    "            LOADED[filename] = audio[0][0]\n",
    "        if audio[1] != samplerate:\n",
    "            return 'ERROR: All audios in folder must have the same samplerate.'\n",
    "    return LOADED, samplerate\n",
    "\n",
    "#RESAMPLE\n",
    "def resampling(audio_dict, original_samplerate, new_samplerate):\n",
    "    if (new_samplerate == original_samplerate) or (new_samplerate == None):\n",
    "        return audio_dict, original_samplerate\n",
    "    else:\n",
    "        resample = torchaudio.transforms.Resample(orig_freq=original_samplerate, new_freq=new_samplerate)\n",
    "        for filename, audio in tqdm(audio_dict.items(), desc='RESAMPLING'):\n",
    "            audio_dict[filename] = resample(audio)\n",
    "        return audio_dict, new_samplerate\n",
    "\n",
    "#Fragment audios FILL WITH ZEROS\n",
    "def fragment_audio(audio_dict, samplerate, time=1):\n",
    "    for filename, audio in tqdm(audio_dict.items(),desc='FRAGMENT AUDIOS'):\n",
    "        fill = len(audio)-samplerate*time\n",
    "        if fill >0:\n",
    "            cut_clean = int(len(audio) - len(audio)%(samplerate*time))\n",
    "            audio = audio[:cut_clean]\n",
    "        else:\n",
    "            audio = torch.cat((audio,torch.zeros((abs(fill),))),0)\n",
    "        n_clips = int(len(audio)/(samplerate*time))\n",
    "        audio = audio.reshape([n_clips, int(samplerate*time)])\n",
    "        audio_dict[filename] = np.array(audio)\n",
    "    return audio_dict, time\n",
    "\n",
    "#Convert to Mel Spectrogram\n",
    "def MELspectrogram(audio_dict, samplerate):\n",
    "    audio_dict = copy.deepcopy(audio_dict)\n",
    "    n_mels = 128\n",
    "    n_fft = int(samplerate*0.029)\n",
    "    hop_length = int(samplerate*0.010)\n",
    "    win_length=int(samplerate*0.025)\n",
    "\n",
    "    for filename, waveform in tqdm(audio_dict.items(), desc='MELSPECTROGRAM'):\n",
    "        waveform = torch.from_numpy(waveform)\n",
    "        spec = torchaudio.transforms.MelSpectrogram(sample_rate=samplerate, n_fft=n_fft, n_mels=n_mels, hop_length=hop_length,win_length=win_length)(waveform); waveform\n",
    "\n",
    "        spec = torchaudio.transforms.AmplitudeToDB()(spec);spec\n",
    "        spec = spec.numpy();spec\n",
    "        spec = (spec - spec.min()) / (spec.max() - spec.min()); spec\n",
    "        #spec = spec.astype('uint8'); spec\n",
    "        audio_dict[filename] = spec\n",
    "    return audio_dict\n",
    "\n",
    "#Train Test split AUDIO\n",
    "def train_test(audio_dict):\n",
    "    df = pd.read_csv('Dataset.csv', usecols=['Participant_ID','PHQ-9 Score',],dtype={1:str})\n",
    "    df['labels'] = np.zeros([len(df),],dtype=int)\n",
    "    df.loc[df['PHQ-9 Score'] <10, 'labels'] = 0\n",
    "    df.loc[df['PHQ-9 Score'] >=10, 'labels'] = 1\n",
    "    train_labels, test_labels = train_test_split(df, test_size=0.2, train_size=0.8,shuffle=False)\n",
    "\n",
    "    train_labels = train_labels.set_index('Participant_ID').to_dict()['labels']\n",
    "    test_labels =  test_labels.set_index('Participant_ID').to_dict()['labels']\n",
    "\n",
    "    X_train = []\n",
    "    Y_train = []\n",
    "    X_test = []\n",
    "    Y_test = []\n",
    "\n",
    "    for filename, data in tqdm(audio_dict.items(), 'LABEL'):\n",
    "        ID = filename[:3]\n",
    "        if ID in train_labels:\n",
    "            dep = 0 if train_labels[ID]==0 else 1\n",
    "            [X_train.append(x) for x in data]\n",
    "            [Y_train.append(dep) for x in data]\n",
    "        if ID in test_labels:\n",
    "            dep = 0 if test_labels[ID]==0 else 1\n",
    "            [X_test.append(x) for x in data]\n",
    "            [Y_test.append(dep) for x in data]\n",
    "\n",
    "    X_train = tf.convert_to_tensor(X_train)\n",
    "    Y_train = tf.convert_to_tensor(Y_train)\n",
    "    X_test = tf.convert_to_tensor(X_test)\n",
    "    Y_test = tf.convert_to_tensor(Y_test)\n",
    "    return X_train, Y_train, X_test, Y_test\n",
    "\n",
    "def train_test2(audio_dict):\n",
    "    train_set = {116: 1, 148: 5, 112: 5, 152: 5, 5: 7, 149: 12, 3: 3, 128: 11, 138: 14, 9: 21, 134: 6, 106: 24, 142: 4, 139: 8, 114: 23, 146: 10, 151: 4, 118: 8, 130: 5, 135: 9, 4: 13, 137: 10, 143: 11, 133: 11, 140: 4, 147: 3, 153: 19, 119: 1, 121: 13, 8: 12, 107: 5, 132: 20, 103: 15, 136: 19, 117: 7, 129: 2, 123: 10, 122: 15, 131: 13, 102: 6, 145: 5}\n",
    "\n",
    "    test_set =  {111: 16, 115: 2, 108: 14, 120: 12, 113: 12, 124: 11, 125: 10, 126: 2, 144: 3, 141: 2, 127: 9}\n",
    "\n",
    "    X_train = []\n",
    "    Y_train = []\n",
    "    X_test = []\n",
    "    Y_test = []\n",
    "\n",
    "    for filename, data in tqdm(audio_dict.items(), 'LABEL'):\n",
    "        ID = int(filename[:3])\n",
    "        if ID in train_set:\n",
    "            dep = 0 if train_set[ID]< 10 else 1\n",
    "            [X_train.append(x) for x in data]\n",
    "            [Y_train.append(dep) for x in data]\n",
    "        if ID in test_set:\n",
    "            dep = 0 if test_set[ID]<10 else 1\n",
    "            [X_test.append(x) for x in data]\n",
    "            [Y_test.append(dep) for x in data]\n",
    "\n",
    "    X_train = tf.convert_to_tensor(X_train)\n",
    "    Y_train = tf.convert_to_tensor(Y_train)\n",
    "    X_test = tf.convert_to_tensor(X_test)\n",
    "    Y_test = tf.convert_to_tensor(Y_test)\n",
    "    \n",
    "    return X_train, Y_train, X_test, Y_test\n",
    "\n",
    "#XY Split Depression\n",
    "def XY_dep(audio_dict, Gender = None):\n",
    "    df = pd.read_csv('Dataset.csv', usecols=['Participant_ID','PHQ-9 Score','Gender'],dtype={1:str})\n",
    "    df = df[df.Gender == Gender] if Gender != None else df\n",
    "    df['labels'] = np.zeros([len(df),],dtype=int)\n",
    "    df.loc[df['PHQ-9 Score'] <10, 'labels'] = 0\n",
    "    df.loc[df['PHQ-9 Score'] >=10, 'labels'] = 1\n",
    "\n",
    "    labels = df.set_index('Participant_ID').to_dict()['labels']\n",
    "    X = []\n",
    "    Y = []\n",
    "\n",
    "    for filename, data in tqdm(audio_dict.items(), 'LABEL'):\n",
    "        ID = filename[:3]\n",
    "        dep = 0 if labels[ID]==0 else 1\n",
    "        [X.append(x) for x in data]\n",
    "        [Y.append(dep) for x in data]\n",
    "        \n",
    "    X = tf.convert_to_tensor(X)\n",
    "    Y = tf.convert_to_tensor(Y)\n",
    "    return X, Y\n",
    "\n",
    "#XY Split by gender\n",
    "def XY_gender(audio_dict, Gender = None):\n",
    "    df = pd.read_csv('Dataset.csv', usecols=['Participant_ID','PHQ-9 Score','Gender'],dtype={1:str})\n",
    "    df['labels'] = np.zeros([len(df),],dtype=int)\n",
    "    df.loc[df['Gender'] =='Male', 'labels'] = 0\n",
    "    df.loc[df['Gender'] == 'Female', 'labels'] = 1\n",
    "\n",
    "    labels = df.set_index('Participant_ID').to_dict()['labels']\n",
    "    X = []\n",
    "    Y = []\n",
    "\n",
    "    for filename, data in tqdm(audio_dict.items(), 'LABEL'):\n",
    "        ID = filename[:3]\n",
    "        dep = 0 if labels[ID]==0 else 1\n",
    "        [X.append(x) for x in data]\n",
    "        [Y.append(dep) for x in data]\n",
    "\n",
    "    X = tf.convert_to_tensor(X)\n",
    "    Y = tf.convert_to_tensor(Y)\n",
    "\n",
    "    return X, Y\n",
    "\n",
    "#Define F1 score as metric\n",
    "F1 = tfa.metrics.F1Score(1, threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d98fe8b66454531",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-13T23:16:36.881740Z",
     "start_time": "2024-07-13T23:16:36.877964Z"
    }
   },
   "outputs": [],
   "source": [
    "def CNN_LF(X_train,Y_train, X_test, Y_test, time, epochs, verbose=0, optimizer= 'adadelta'):\n",
    "    model = Sequential()\n",
    "    model.add(Resizing(128, 10 *time,input_shape=X_train[0].shape))\n",
    "    #model.add(Resizing(1, 1 *time,input_shape=X_train[0].shape))\n",
    "    model.add(Conv2D(30, (3, 3), strides=1, padding=\"same\", activation=\"relu\"))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPool2D((2, 2), strides=2, padding=\"same\"))\n",
    "    model.add(Conv2D(15, (3, 3), strides=1, padding=\"same\", activation=\"relu\"))\n",
    "    model.add(Dropout(0.2)) #LF\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPool2D((2, 2), strides=2, padding=\"same\"))\n",
    "    model.add(Flatten())\n",
    "    #model.add(Dense(units=512, activation=\"relu\"))\n",
    "    model.add(Dense(units=256, activation=\"relu\")) #LF\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "    #model.summary()\n",
    "\n",
    "    model.compile(loss=\"binary_crossentropy\", metrics=[\"accuracy\", 'Precision', 'Recall', F1], optimizer=optimizer)\n",
    "\n",
    "    #model.fit(X_train, Y_train, epochs=epochs, verbose=verbose,validation_data= [X_test, Y_test], shuffle=True ,callbacks = EarlyStopping(monitor='val_f1_score', patience=2, start_from_epoch=5, restore_best_weights=True, mode='max'))\n",
    "\n",
    "    model.fit(X_train, Y_train, epochs=epochs, verbose=verbose,validation_data= [X_test, Y_test], shuffle=True ,callbacks = EarlyStopping(monitor='val_f1_score', patience=2, start_from_epoch=5, restore_best_weights=True, mode='max'))\n",
    "\n",
    "    _,accuracy, prec, rec, f1 = model.evaluate(X_test, Y_test)\n",
    "\n",
    "    return accuracy, prec, rec, float(f1)"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def preprocess(directory, time, sample_rate):\n",
    "    #LOAD AUDIOS\n",
    "    audio_dict, original_samplerate = load_audios(directory)\n",
    "    #RESAMPLE AUDIOS\n",
    "    audio_dict, samplerate = resampling(audio_dict, original_samplerate, new_samplerate=sample_rate)\n",
    "    #FRAGMENT AUDIOS\n",
    "    audio_dictRAW, time = fragment_audio(audio_dict, samplerate, time=time)\n",
    "    #CONVERT TO MEL SPECTROGRAM\n",
    "    audio_dictMEL = MELspectrogram(audio_dictRAW, samplerate)\n",
    "    #RESHAPE FOR 2D CNN\n",
    "    X_train, Y_train, X_test, Y_test = train_test2(audio_dictMEL)\n",
    "\n",
    "    X_train = tf.reshape(X_train,shape = (-1,X_train.shape[1],X_train.shape[2],1))\n",
    "    X_test  = tf.reshape(X_test, shape = (-1,X_test.shape[1],X_test.shape[2],1))\n",
    "    #print(X.shape)\n",
    "\n",
    "    # X = tf.concat([X_train,X_test], axis=0)\n",
    "    # Y = tf.concat([Y_train,Y_test], axis=0)\n",
    "\n",
    "    #UNCOMMENT TO PLOT SPECTROGRAM\n",
    "    #plt.imshow(np.array(X[0]), interpolation='nearest')\n",
    "    #plt.show()\n",
    "    \n",
    "    return X_train, Y_train, X_test, Y_test"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-13T23:16:36.888682Z",
     "start_time": "2024-07-13T23:16:36.884257Z"
    }
   },
   "id": "c82fa463ff62753f",
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": [
    "### DEVICE TEST"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b1ebe440dcc4204e"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "LOAD AUDIOS:   0%|          | 0/1675 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1b3edc272cb6468692a65f14ecfb910a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "FRAGMENT AUDIOS:   0%|          | 0/1674 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "004a4b6d99e94e208e63fc61b39de207"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "MELSPECTROGRAM:   0%|          | 0/1674 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "61c262dcd6b444a08619dea7d7a30fc3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_data = [\"SHURE SM-27\", \"iPhoneSE2020\"]\n",
    "test_data = [\"SHURE SM-27\", \"iPhoneSE2020\"]\n",
    "time = 5\n",
    "sample_rate = None\n",
    "experiment_count = 1\n",
    "results_list = []  # This will be a list of dictionaries for easier DataFrame conversion\n",
    "\n",
    "highQuality = preprocess('DATASET/SHURE SM-27',time,sample_rate)\n",
    "lowQuality  = preprocess('DATASET/iPhoneSE2020',time,sample_rate)\n",
    "\n",
    "for train in training_data:\n",
    "    for test in test_data:\n",
    "        X_train, Y_train,_,_ = highQuality if train=='SHURE SM-27' else lowQuality\n",
    "        _,_,X_test,Y_test    = highQuality if test=='SHURE SM-27'     else lowQuality\n",
    "        print('TRAIN = ' + train + '  |   TEST = '+test+ '\\n\\n')\n",
    "    \n",
    "        for i in range(experiment_count):\n",
    "            accuracy, prec, rec, f1 = CNN_LF(X_train, Y_train, X_test, Y_test, time=time, epochs = 50)\n",
    "            # Create a single dictionary for each experiment\n",
    "            experiment_results = {\n",
    "                'Data': \"Train \"+ train+\" / Test: \"+test,\n",
    "                'Tests': 'Ex_' + str(i+1),\n",
    "                'Accuracy': accuracy,\n",
    "                'Precision': prec,\n",
    "                'Recall': rec,\n",
    "                'F1_Score': f1\n",
    "            }\n",
    "            results_list.append(experiment_results)\n",
    "        print('\\n---------------------------------------------------------------------------\\n')\n",
    "\n",
    "# Create DataFrame from the list of dictionaries\n",
    "results_MIC = pd.DataFrame(results_list)\n",
    "\n",
    "# Calculate the average of experiments for each training data group, ignoring non-numeric columns\n",
    "average_metrics_MIC = results_MIC.groupby('Data').mean(numeric_only=True)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-07-13T23:16:36.891305Z"
    }
   },
   "id": "92dd08331148da3b",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print(average_metrics_MIC)\n",
    "results_MIC.to_csv('/Users/luisfebrenes/Desktop/Results/results_MIC.csv')\n",
    "average_metrics_MIC.to_csv('/Users/luisfebrenes/Desktop/Results/average_metrics_MIC.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "3de5c241c523ab8f",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## SAMPLE RATE TEST"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "60fad088c68027f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "training_data = [\"SHURE SM-27\", \"iPhoneSE2020\"]\n",
    "test_data = [\"SHURE SM-27\", \"iPhoneSE2020\"]\n",
    "time = 5\n",
    "sample_rate = [2000, 4000, 8000, 16000, 44100]\n",
    "experiment_count = 1\n",
    "results_list = []  # This will be a list of dictionaries for easier DataFrame conversion\n",
    "\n",
    "for sr in sample_rate:\n",
    "    X_train, Y_train, X_test, Y_test = preprocess('DATASET/SHURE SM-27',time,sample_rate=sr)\n",
    "\n",
    "    print('SAMPLE RATE = ', sr, '\\n\\n')\n",
    "    for i in range(experiment_count):\n",
    "        accuracy, prec, rec, f1 = CNN_LF(X_train, Y_train, X_test, Y_test, time=time, epochs = 50)\n",
    "        # Create a single dictionary for each experiment\n",
    "        experiment_results = {\n",
    "            'Sample Rate': sr,\n",
    "            'Tests': 'Ex_' + str(i+1),\n",
    "            'Accuracy': accuracy,\n",
    "            'Precision': prec,\n",
    "            'Recall': rec,\n",
    "            'F1_Score': f1\n",
    "        }\n",
    "        results_list.append(experiment_results)\n",
    "    print('\\n---------------------------------------------------------------------------\\n')\n",
    "\n",
    "# Create DataFrame from the list of dictionaries\n",
    "results_SR = pd.DataFrame(results_list)\n",
    "\n",
    "# Calculate the average of experiments for each training data group, ignoring non-numeric columns\n",
    "average_metrics_SR = results_SR.groupby('Sample Rate').mean(numeric_only=True)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "42e0ea5e72653083",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print(average_metrics_SR)\n",
    "results_SR.to_csv('/Users/luisfebrenes/Desktop/Results/results_SR.csv')\n",
    "average_metrics_SR.to_csv('/Users/luisfebrenes/Desktop/Results/average_metrics_SR.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "d97483bd028a825b",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### QUALITY TEST"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1bba30a61f944dc8"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "paths = ['44.1_24', '44.1_16', '16_16', 'MP3_320', 'MP3_96', 'OGG']\n",
    "time = 5\n",
    "sample_rate = None\n",
    "experiment_count = 1\n",
    "results_list = []  # This will be a list of dictionaries for easier DataFrame conversion\n",
    "\n",
    "for path in paths:\n",
    "    directory = '/Volumes/SESSIONS_2024 1/Quality_TEST/' + path\n",
    "    X_train, Y_train, X_test, Y_test = preprocess(directory,time,sample_rate)\n",
    "    print('DATA = ' + path+'\\n')\n",
    "    \n",
    "    for i in range(experiment_count):\n",
    "        accuracy, prec, rec, f1 = CNN_LF(X_train, Y_train, X_test, Y_test, time=time, epochs = 50)\n",
    "        # Create a single dictionary for each experiment\n",
    "        experiment_results = {\n",
    "            'Data': path,\n",
    "            'Tests': 'Ex_' + str(i+1),\n",
    "            'Accuracy': accuracy,\n",
    "            'Precision': prec,\n",
    "            'Recall': rec,\n",
    "            'F1_Score': f1\n",
    "        }\n",
    "        results_list.append(experiment_results)\n",
    "    print('\\n---------------------------------------------------------------------------\\n')\n",
    "\n",
    "# Create DataFrame from the list of dictionaries\n",
    "results_QUALITY = pd.DataFrame(results_list)\n",
    "\n",
    "# Calculate the average of experiments for each training data group, ignoring non-numeric columns\n",
    "average_metrics_QUALITY = results_QUALITY.groupby('Data').mean(numeric_only=True)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "a5ad21191290953",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print(average_metrics_QUALITY)\n",
    "results_QUALITY.to_csv('/Users/luisfebrenes/Desktop/Results/results_QUALITY.csv')\n",
    "average_metrics_QUALITY.to_csv('/Users/luisfebrenes/Desktop/Results/average_metrics_QUALITY.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "feb28c394d92d44c",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
