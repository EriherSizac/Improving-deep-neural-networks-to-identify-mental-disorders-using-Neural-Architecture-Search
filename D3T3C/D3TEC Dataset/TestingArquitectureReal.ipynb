{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KR37uGe3lqIt",
    "outputId": "7e46a98e-7d1e-43e7-b7d3-0ddb166976c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Codificación real de Conv2D: [0, 16, 0, 0]\n",
      "Decodificación Conv2D: {'type': 'Conv2D', 'filters': 16, 'strides': 1, 'activation': 'relu'}\n",
      "\n",
      "Codificación real de Dropout: [3, 1, 0, 0]\n",
      "Decodificación Dropout: {'type': 'Dropout', 'rate': 0.3}\n",
      "\n",
      "Codificación real de Dense: [4, 128, 0, 0]\n",
      "Decodificación Dense: {'type': 'Dense', 'units': 128, 'activation': 'relu'}\n",
      "\n",
      "Codificación real de Repetition: [8, 3, 5, 0]\n",
      "Decodificación Repetition: {'type': 'Repetition', 'repetition_layers': 3, 'repetition_count': 5}\n"
     ]
    }
   ],
   "source": [
    "# Opciones de decodificación para otros parámetros\n",
    "layer_type_options = {\n",
    "    0: 'Conv2D',\n",
    "    1: 'BatchNorm',\n",
    "    2: 'MaxPooling',\n",
    "    3: 'Dropout',\n",
    "    4: 'Dense',\n",
    "    5: 'Flatten',\n",
    "    6: 'DepthwiseConv2D',\n",
    "    7: 'DontCare',\n",
    "    8: 'Repetition'\n",
    "}\n",
    "stride_options = {0: 1, 1: 2}\n",
    "dropout_options = {0: 0.2, 1: 0.3, 2: 0.4, 3: 0.5}\n",
    "activation_options = {0: 'relu', 1: 'leaky_relu', 2: 'sigmoid', 3: 'tanh'}\n",
    "\n",
    "# Función para codificar los parámetros de la capa\n",
    "def encode_layer_params(layer_type_idx, param1=0, param2=0, param3=0):\n",
    "    \"\"\"\n",
    "    Codifica una capa en una lista en función del tipo de capa y sus parámetros.\n",
    "\n",
    "    layer_type_idx : int : índice del tipo de capa según layer_type_options.\n",
    "    param1         : int/float : filtros, neuronas, capas de repetición, etc.\n",
    "    param2         : int : stride, número de repeticiones, etc.\n",
    "    param3         : int : índice de activación o tasa de dropout.\n",
    "    \"\"\"\n",
    "    return [layer_type_idx, param1, param2, param3]\n",
    "\n",
    "# Función para decodificar los parámetros de la capa\n",
    "def decode_layer_params(encoded_params):\n",
    "    \"\"\"\n",
    "    Decodifica una capa desde su representación codificada en parámetros interpretables.\n",
    "\n",
    "    encoded_params : list : [tipo de capa, param1, param2, param3].\n",
    "    \"\"\"\n",
    "    layer_type_idx = encoded_params[0]\n",
    "    layer_type = layer_type_options.get(layer_type_idx, 'DontCare')\n",
    "\n",
    "    # Decodificar en función del tipo de capa\n",
    "    if layer_type in ['Conv2D', 'DepthwiseConv2D']:\n",
    "        filters = max(4, min(encoded_params[1], 32))  # Limitar filtros entre 4 y 32\n",
    "        strides = stride_options.get(encoded_params[2], 1)\n",
    "        activation = activation_options.get(encoded_params[3], 'relu')\n",
    "        return {\n",
    "            'type': layer_type,\n",
    "            'filters': filters,\n",
    "            'strides': strides,\n",
    "            'activation': activation\n",
    "        }\n",
    "    elif layer_type == 'BatchNorm':\n",
    "        return {'type': 'BatchNorm'}\n",
    "    elif layer_type == 'MaxPooling':\n",
    "        strides = stride_options.get(encoded_params[1], 1)\n",
    "        return {'type': 'MaxPooling', 'strides': strides}\n",
    "    elif layer_type == 'Dropout':\n",
    "        rate = dropout_options.get(encoded_params[1], 0.2)\n",
    "        return {'type': 'Dropout', 'rate': rate}\n",
    "    elif layer_type == 'Dense':\n",
    "        units = max(1, min(encoded_params[1], 512))  # Limitar unidades entre 1 y 512\n",
    "        activation = activation_options.get(encoded_params[2], 'relu')\n",
    "        return {'type': 'Dense', 'units': units, 'activation': activation}\n",
    "    elif layer_type == 'Flatten':\n",
    "        return {'type': 'Flatten'}\n",
    "    elif layer_type == 'Repetition':\n",
    "        return {\n",
    "            'type': 'Repetition',\n",
    "            'repetition_layers': int(encoded_params[1]),\n",
    "            'repetition_count': int(encoded_params[2])\n",
    "        }\n",
    "    elif layer_type == 'DontCare':\n",
    "        return {'type': \"DontCare\"}\n",
    "\n",
    "    return None\n",
    "\n",
    "# Ejemplos de codificación y decodificación\n",
    "encoded_conv2d = encode_layer_params(0, 16, 0, 0)  # Conv2D con 16 filtros, stride 1 y activación ReLU\n",
    "decoded_conv2d = decode_layer_params(encoded_conv2d)\n",
    "print(f\"\\nCodificación real de Conv2D: {encoded_conv2d}\")\n",
    "print(f\"Decodificación Conv2D: {decoded_conv2d}\")\n",
    "\n",
    "encoded_dropout = encode_layer_params(3, 1)  # Dropout con tasa de 0.3\n",
    "decoded_dropout = decode_layer_params(encoded_dropout)\n",
    "print(f\"\\nCodificación real de Dropout: {encoded_dropout}\")\n",
    "print(f\"Decodificación Dropout: {decoded_dropout}\")\n",
    "\n",
    "encoded_dense = encode_layer_params(4, 128, 0)  # Dense con 128 neuronas y activación ReLU\n",
    "decoded_dense = decode_layer_params(encoded_dense)\n",
    "print(f\"\\nCodificación real de Dense: {encoded_dense}\")\n",
    "print(f\"Decodificación Dense: {decoded_dense}\")\n",
    "\n",
    "encoded_repetition = encode_layer_params(8, 3, 5)  # Repetition para repetir las últimas 3 capas 5 veces\n",
    "decoded_repetition = decode_layer_params(encoded_repetition)\n",
    "print(f\"\\nCodificación real de Repetition: {encoded_repetition}\")\n",
    "print(f\"Decodificación Repetition: {decoded_repetition}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "SMmKhGcf4NGU"
   },
   "outputs": [],
   "source": [
    "def int_to_real_dom(num, domain):\n",
    "  min_i, max_i = domain\n",
    "  r = (num - min_i) / (max_i - min_i)\n",
    "  return r\n",
    "\n",
    "def real_to_int_dom(num, domain):\n",
    "  min_i, max_i = domain\n",
    "  value = min_i + num * (max_i - min_i)\n",
    "  if isinstance(min_i, int) and isinstance(max_i, int):\n",
    "      value = int(round(value))\n",
    "  return value\n",
    "\n",
    "def convert_individual(ind, to_real=True):\n",
    "    real_rep = []\n",
    "    N = max(layer_type_options.keys())\n",
    "    for i in range(0, len(ind), 4):\n",
    "        layer_type_idx = ind[i]\n",
    "        domain_layer_type = [0, N]\n",
    "        if to_real:\n",
    "            real_rep.append(int_to_real_dom(layer_type_idx, domain_layer_type))\n",
    "            layer_type = layer_type_options.get(layer_type_idx, 'DontCare')\n",
    "        else:\n",
    "            real_rep.append(real_to_int_dom(layer_type_idx, domain_layer_type))\n",
    "            layer_type = layer_type_options.get(real_rep[i], 'DontCare')\n",
    "\n",
    "        # Decode based on layer type\n",
    "        if layer_type in ['Conv2D', 'DepthwiseConv2D']:\n",
    "            if to_real:\n",
    "                real_rep.append(int_to_real_dom(ind[i + 1], [4, 32]))\n",
    "                real_rep.append(int_to_real_dom(ind[i + 2], [0, 1]))\n",
    "                real_rep.append(int_to_real_dom(ind[i + 3], [0, 3]))\n",
    "            else:\n",
    "                real_rep.append(real_to_int_dom(ind[i + 1], [4, 32]))\n",
    "                real_rep.append(real_to_int_dom(ind[i + 2], [0, 1]))\n",
    "                real_rep.append(real_to_int_dom(ind[i + 3], [0, 3]))\n",
    "        elif layer_type == 'BatchNorm':\n",
    "            real_rep.extend([0, 0, 0])\n",
    "        elif layer_type == 'MaxPooling':\n",
    "            if to_real:\n",
    "                real_rep.append(int_to_real_dom(ind[i + 1], [0, 1]))\n",
    "            else:\n",
    "                real_rep.append(real_to_int_dom(ind[i + 1], [0, 1]))\n",
    "            real_rep.extend([0, 0])\n",
    "        elif layer_type == 'Dropout':\n",
    "            if to_real:\n",
    "                real_rep.append(int_to_real_dom(ind[i + 1], [0, 3]))\n",
    "            else:\n",
    "                real_rep.append(real_to_int_dom(ind[i + 1], [0, 3]))\n",
    "            real_rep.extend([0, 0])\n",
    "        elif layer_type == 'Dense':\n",
    "            if to_real:\n",
    "                real_rep.append(int_to_real_dom(ind[i + 1], [1, 512]))\n",
    "                real_rep.append(int_to_real_dom(ind[i + 2], [0, 3]))\n",
    "            else:\n",
    "                real_rep.append(real_to_int_dom(ind[i + 1], [1, 512]))\n",
    "                real_rep.append(real_to_int_dom(ind[i + 2], [0, 3]))\n",
    "            real_rep.append(0)\n",
    "        elif layer_type == 'Flatten':\n",
    "            real_rep.extend([0, 0, 0])\n",
    "        elif layer_type == 'Repetition':\n",
    "            if to_real:\n",
    "                real_rep.append(int_to_real_dom(ind[i + 1], [1, 4]))\n",
    "                real_rep.append(int_to_real_dom(ind[i + 2], [1, 32]))\n",
    "            else:\n",
    "                real_rep.append(real_to_int_dom(ind[i + 1], [1, 4]))\n",
    "                real_rep.append(real_to_int_dom(ind[i + 2], [1, 32]))\n",
    "            real_rep.append(0)\n",
    "        elif layer_type == 'DontCare':\n",
    "            real_rep.extend([0, 0, 0])\n",
    "    return real_rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ecnYEmcw8aLv",
    "outputId": "f815a803-ff1e-4905-e4da-3fb4c84bc24b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 0, 0, 6, 5, 0, 2, 8, 1, 1, 0, 4, 17, 3, 0, 4, 22, 0, 0, 7, 0, 0, 0, 0, 11, 1, 3, 3, 0, 0, 0, 3, 1, 0, 0, 4, 63, 3, 0, 1, 0, 0, 0, 5, 0, 0, 0]\n",
      "[0.125, 0, 0, 0, 0.75, 0.03571428571428571, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0, 0.5, 0.03131115459882583, 1.0, 0, 0.5, 0.0410958904109589, 0.0, 0, 0.875, 0, 0, 0, 0.0, 0.25, 1.0, 1.0, 0.375, 0.0, 0, 0, 0.375, 0.3333333333333333, 0, 0, 0.5, 0.12133072407045009, 1.0, 0, 0.125, 0, 0, 0, 0.625, 0, 0, 0]\n",
      "[1, 0, 0, 0, 6, 5, 0, 2, 8, 1, 1, 0, 4, 17, 3, 0, 4, 22, 0, 0, 7, 0, 0, 0, 0, 11, 1, 3, 3, 0, 0, 0, 3, 1, 0, 0, 4, 63, 3, 0, 1, 0, 0, 0, 5, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "x = [1, 0, 0, 0, 6, 5, 0, 2, 8, 1, 1, 0, 4, 17, 3, 0, 4, 22, 0, 0, 7, 0, 0, 0, 0, 11, 1, 3, 3, 0, 0, 0, 3, 1, 0, 0, 4, 63, 3, 0, 1, 0, 0, 0, 5, 0, 0, 0]\n",
    "print(x)\n",
    "y = convert_individual(x)\n",
    "print(y)\n",
    "z = convert_individual(y, to_real=False)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "8UHDkV27ltyD"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Clase para capas neutrales 'DontCare'\n",
    "class DontCareLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(DontCareLayer, self).__init__()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "g0eLaXdQluzl"
   },
   "outputs": [],
   "source": [
    "def encode_model_architecture(model_dict, max_alleles=48):\n",
    "    \"\"\"\n",
    "    Codifica la arquitectura del modelo en una lista de valores con un máximo de `max_alleles`.\n",
    "    Cada capa se codifica en función de sus parámetros.\n",
    "    \"\"\"\n",
    "    encoded_layers = []\n",
    "    total_alleles = 0\n",
    "\n",
    "    for layer in model_dict['layers']:\n",
    "        if layer['type'] == 'Repetition':  # Codificar capa de repetición\n",
    "            encoded_layer = encode_layer_params(\n",
    "                layer_type_idx=8,  # índice para 'Repetition'\n",
    "                param1=layer.get('repetition_layers', 0),\n",
    "                param2=layer.get('repetition_count', 1)\n",
    "            )\n",
    "        else:\n",
    "            layer_type_idx = next(\n",
    "                key for key, value in layer_type_options.items() if value == layer['type']\n",
    "            )\n",
    "\n",
    "            # Codificar parámetros específicos de cada tipo de capa\n",
    "            if layer['type'] in ['Conv2D', 'DepthwiseConv2D']:\n",
    "                # Limitar filtros dentro del rango [4, 32]\n",
    "                param1 = max(4, min(layer.get('filters', 8), 32))\n",
    "                param2 = next((key for key, value in stride_options.items() if value == layer.get('strides', 1.0)), 0)\n",
    "                param3 = next((key for key, value in activation_options.items() if value == layer.get('activation', 'relu')), 0)\n",
    "                encoded_layer = [layer_type_idx, param1, param2, param3]\n",
    "\n",
    "            elif layer['type'] == 'Dense':\n",
    "                # Limitar neuronas dentro del rango [1, 512]\n",
    "                param1 = max(1, min(layer.get('units', 1), 512))\n",
    "                param2 = next((key for key, value in activation_options.items() if value == layer.get('activation', 'relu')), 0)\n",
    "                encoded_layer = [layer_type_idx, param1, param2, 0]\n",
    "\n",
    "            elif layer['type'] == 'MaxPooling':\n",
    "                param1 = next((key for key, value in stride_options.items() if value == layer.get('strides', 1.0)), 0)\n",
    "                encoded_layer = [layer_type_idx, param1, 0, 0]\n",
    "\n",
    "            elif layer['type'] == 'Dropout':\n",
    "                param1 = next((key for key, value in dropout_options.items() if value == layer.get('rate', 0.2)), 0)\n",
    "                encoded_layer = [layer_type_idx, param1, 0, 0]\n",
    "\n",
    "            elif layer['type'] == 'BatchNorm':\n",
    "                encoded_layer = [layer_type_idx, 0, 0, 0]\n",
    "\n",
    "            elif layer['type'] == 'Flatten':\n",
    "                encoded_layer = [layer_type_idx, 0, 0, 0]\n",
    "\n",
    "            elif layer['type'] == 'DontCare':\n",
    "                encoded_layer = [layer_type_idx, 0, 0, 0]\n",
    "\n",
    "        # Añadir la codificación de la capa a la lista de alelos\n",
    "        encoded_layers.extend(encoded_layer)\n",
    "        total_alleles += len(encoded_layer)\n",
    "\n",
    "    # Rellenar con 'DontCare' si el total de alelos es menor que `max_alleles`\n",
    "    while total_alleles < max_alleles:\n",
    "        dont_care_encoding = encode_layer_params(7)  # índice de 'DontCare'\n",
    "        encoded_layers.extend(dont_care_encoding)\n",
    "        total_alleles += len(dont_care_encoding)\n",
    "\n",
    "    # Recortar si excede `max_alleles`\n",
    "    final_encoding = encoded_layers[:max_alleles]\n",
    "    #print(f\"Final Encoded Model: {final_encoding}\")\n",
    "\n",
    "    return final_encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "tDFQVy0Tlwpx"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def fixArch(encoded_model, verbose=False):\n",
    "    \"\"\"\n",
    "    Corrige la arquitectura codificada del modelo, asegurando que:\n",
    "    - Se evite la presencia de capas incompatibles después de una capa Flatten.\n",
    "    - En caso de una capa de Repetition, se ajuste el alcance de repetición si no hay suficientes capas anteriores.\n",
    "\n",
    "    Parameters:\n",
    "        encoded_model (list): Lista codificada de la arquitectura del modelo.\n",
    "        verbose (bool): Si es True, muestra las correcciones realizadas.\n",
    "\n",
    "    Returns:\n",
    "        list: Lista con la arquitectura corregida, truncada a un máximo de 48 alelos.\n",
    "    \"\"\"\n",
    "\n",
    "    fixed_layers = []  # Lista que almacenará la arquitectura corregida\n",
    "    input_is_flattened = False  # Indicador para saber si ya hay una capa Flatten en el modelo\n",
    "    index = 0  # Índice para recorrer el modelo codificado\n",
    "\n",
    "    # Procesar cada capa en el modelo sin forzar la primera capa a ser específica\n",
    "    while index < len(encoded_model) and len(fixed_layers) < 48:\n",
    "        layer_type = int(encoded_model[index])  # Obtener el tipo de capa actual\n",
    "\n",
    "        # Procesar la capa de Repetition\n",
    "        if layer_type == 8:\n",
    "            repetition_layers = int(encoded_model[index + 1])  # Número de capas a repetir\n",
    "            repetition_count = min(max(int(encoded_model[index + 2]), 0), 32)  # Cantidad de repeticiones\n",
    "\n",
    "            # Verificar si hay suficientes capas para la repetición solicitada\n",
    "            actual_layers_to_repeat = min(repetition_layers, len(fixed_layers) // 4)\n",
    "\n",
    "            if actual_layers_to_repeat != repetition_layers:\n",
    "                if verbose:\n",
    "                    print(f\"Ajustando alcance de repetición de {repetition_layers} a {actual_layers_to_repeat} debido a falta de capas.\")\n",
    "                repetition_layers = actual_layers_to_repeat\n",
    "\n",
    "            # Añadir la capa de repetición sin modificar su estructura\n",
    "            fixed_layers.extend([layer_type, repetition_layers, repetition_count, 0])\n",
    "            index += 4\n",
    "            continue\n",
    "\n",
    "        # Procesar cada tipo de capa normal con sus restricciones\n",
    "        if layer_type == 0:  # Conv2D\n",
    "            if input_is_flattened:\n",
    "                fixed_layers.extend([7, 0, 0, 0])  # DontCare\n",
    "            else:\n",
    "                # Limitar el número de filtros entre 4 y 32\n",
    "                filters = min(max(int(encoded_model[index + 1]), 4), 32)\n",
    "                stride_idx = min(max(int(encoded_model[index + 2]), 0), 1)\n",
    "                activation_idx = min(max(int(encoded_model[index + 3]), 0), 3)\n",
    "                fixed_layers.extend([layer_type, filters, stride_idx, activation_idx])\n",
    "\n",
    "        elif layer_type == 6:  # DepthwiseConv2D\n",
    "            if input_is_flattened:\n",
    "                fixed_layers.extend([7, 0, 0, 0])  # DontCare\n",
    "            else:\n",
    "                # Limitar el número de filtros entre 4 y 32\n",
    "                filters = min(max(int(encoded_model[index + 1]), 4), 32)\n",
    "                stride_idx = min(max(int(encoded_model[index + 2]), 0), 1)\n",
    "                activation_idx = min(max(int(encoded_model[index + 3]), 0), 3)\n",
    "                fixed_layers.extend([layer_type, filters, stride_idx, activation_idx])\n",
    "\n",
    "        elif layer_type == 2:  # MaxPooling\n",
    "            if input_is_flattened:\n",
    "                fixed_layers.extend([7, 0, 0, 0])  # DontCare\n",
    "            else:\n",
    "                stride_idx = min(max(int(encoded_model[index + 1]), 0), 1)\n",
    "                fixed_layers.extend([layer_type, stride_idx, 0, 0])\n",
    "\n",
    "        elif layer_type == 3:  # Dropout\n",
    "            rate_idx = min(max(int(encoded_model[index + 1]), 0), 3)\n",
    "            fixed_layers.extend([layer_type, rate_idx, 0, 0])\n",
    "\n",
    "        elif layer_type == 4:  # Dense\n",
    "            # Limitar el número de neuronas entre 1 y 512\n",
    "            neurons = min(max(int(encoded_model[index + 1]), 1), 512)\n",
    "            activation_idx = min(max(int(encoded_model[index + 2]), 0), 3)\n",
    "            fixed_layers.extend([layer_type, neurons, activation_idx, 0])\n",
    "\n",
    "        elif layer_type == 1:  # BatchNorm\n",
    "            fixed_layers.extend([layer_type, 0, 0, 0])\n",
    "\n",
    "        elif layer_type == 5:  # Flatten\n",
    "            if len(fixed_layers) < 16:\n",
    "                fixed_layers.extend([7, 0, 0, 0])  # DontCare\n",
    "            elif input_is_flattened:\n",
    "                fixed_layers.extend([7, 0, 0, 0])  # DontCare\n",
    "            else:\n",
    "                fixed_layers.extend([layer_type, 0, 0, 0])\n",
    "                input_is_flattened = True  # Marcar que ya hay un Flatten\n",
    "\n",
    "        elif layer_type == 7:  # DontCare\n",
    "            fixed_layers.extend([layer_type, 0, 0, 0])\n",
    "\n",
    "        else: # DontCare\n",
    "          fixed_layers.extend([7, 0, 0, 0])\n",
    "\n",
    "        index += 4  # Avanzar al siguiente grupo de parámetros\n",
    "\n",
    "    return fixed_layers[:48]  # Limitar a 48 alelos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "MOTScfWjl5FP"
   },
   "outputs": [],
   "source": [
    "def decode_model_architecture(encoded_model):\n",
    "    \"\"\"\n",
    "    Decodifica la arquitectura del modelo a partir de la lista codificada de valores (índices),\n",
    "    aplicando las reglas de repetición y asegurando la inclusión de una capa convolucional inicial.\n",
    "    \"\"\"\n",
    "    model_dict = {'layers': [{'type': 'Conv2D', 'filters': 32, 'strides': 1, 'activation': 'relu'}]}  # Inserta Conv2D inicial\n",
    "    index = 0\n",
    "\n",
    "    while index < len(encoded_model):\n",
    "        layer_type = int(encoded_model[index])\n",
    "        param1 = encoded_model[index + 1]\n",
    "        param2 = encoded_model[index + 2]\n",
    "        param3 = encoded_model[index + 3]\n",
    "\n",
    "        if layer_type == 8:  # Capa de Repetition\n",
    "            repetition_layers = int(param1)\n",
    "            repetition_count = int(param2)\n",
    "            # Selecciona solo el grupo válido de capas para la repetición\n",
    "            layers_to_repeat = select_group_for_repetition(model_dict['layers'], repetition_layers)\n",
    "\n",
    "            if len(layers_to_repeat) > 0:\n",
    "                for _ in range(repetition_count):\n",
    "                    model_dict['layers'].extend(layers_to_repeat)\n",
    "\n",
    "        else:\n",
    "            decoded_layer = {}\n",
    "\n",
    "            if layer_type == 0:  # Conv2D\n",
    "                decoded_layer = {\n",
    "                    'type': 'Conv2D',\n",
    "                    'filters': max(4, min(param1, 32)),  # Limita `filters` entre 4 y 32\n",
    "                    'strides': stride_options.get(param2, 1),\n",
    "                    'activation': activation_options.get(param3, 'relu')\n",
    "                }\n",
    "            elif layer_type == 6:  # DepthwiseConv2D\n",
    "                decoded_layer = {\n",
    "                    'type': 'DepthwiseConv2D',\n",
    "                    'filters': max(4, min(param1, 32)),  # Limita `filters` entre 4 y 32\n",
    "                    'strides': stride_options.get(param2, 1),\n",
    "                    'activation': activation_options.get(param3, 'relu')\n",
    "                }\n",
    "            elif layer_type == 2:  # MaxPooling\n",
    "                decoded_layer = {\n",
    "                    'type': 'MaxPooling',\n",
    "                    'strides': stride_options.get(param1, 1)\n",
    "                }\n",
    "            elif layer_type == 3:  # Dropout\n",
    "                decoded_layer = {\n",
    "                    'type': 'Dropout',\n",
    "                    'rate': dropout_options.get(param1, 0.2)\n",
    "                }\n",
    "            elif layer_type == 4:  # Dense\n",
    "                decoded_layer = {\n",
    "                    'type': 'Dense',\n",
    "                    'units': max(1, min(param1, 512)),  # Limita `units` entre 1 y 512\n",
    "                    'activation': activation_options.get(param2, 'relu')\n",
    "                }\n",
    "            elif layer_type == 1:  # BatchNorm\n",
    "                decoded_layer = {'type': 'BatchNorm'}\n",
    "            elif layer_type == 5:  # Flatten\n",
    "                decoded_layer = {'type': 'Flatten'}\n",
    "            elif layer_type == 7:  # DontCare\n",
    "                decoded_layer = {'type': 'DontCare'}\n",
    "\n",
    "            model_dict['layers'].append(decoded_layer)\n",
    "\n",
    "        index += 4\n",
    "\n",
    "    # Asegura que haya una capa Flatten antes de la capa Dense final, si no ya existe una Flatten\n",
    "    if model_dict['layers'][-1]['type'] != 'Flatten':\n",
    "        model_dict['layers'].append({'type': 'Flatten'})\n",
    "\n",
    "    # Añade la capa Dense final obligatoria\n",
    "    model_dict['layers'].append({'type': 'Dense', 'units': 1, 'activation': 'sigmoid'})\n",
    "\n",
    "    return model_dict\n",
    "\n",
    "def select_group_for_repetition(layers, repetition_layers):\n",
    "    \"\"\"\n",
    "    Selecciona el primer grupo válido para repetición en función de las reglas de compatibilidad.\n",
    "\n",
    "    Parameters:\n",
    "        layers (list): Lista de capas ya procesadas, donde cada capa es un diccionario.\n",
    "        repetition_layers (int): Número de capas hacia atrás para considerar en la repetición.\n",
    "\n",
    "    Returns:\n",
    "        list: Lista de capas compatibles para repetición.\n",
    "    \"\"\"\n",
    "    valid_layers = []\n",
    "    group_type = None\n",
    "\n",
    "    # Retrocede desde el final de `layers` para encontrar el grupo válido\n",
    "    for layer in reversed(layers[-repetition_layers:]):\n",
    "        if group_type is None:\n",
    "            # Determina el tipo de grupo\n",
    "            if layer['type'] in ['Flatten', 'Dense']:\n",
    "                group_type = 'dense'\n",
    "                valid_layers.insert(0, layer)\n",
    "            elif layer['type'] in ['Conv2D', 'DepthwiseConv2D', 'MaxPooling']:\n",
    "                group_type = 'convolutional'\n",
    "                valid_layers.insert(0, layer)\n",
    "            elif layer['type'] in ['BatchNorm', 'DontCare']:  # BatchNorm y DontCare son compatibles con ambos grupos\n",
    "                valid_layers.insert(0, layer)\n",
    "        else:\n",
    "            # Agrega solo capas compatibles con el grupo seleccionado\n",
    "            if group_type == 'dense' and layer['type'] in ['Flatten', 'Dense', 'BatchNorm', 'DontCare']:\n",
    "                valid_layers.insert(0, layer)\n",
    "            elif group_type == 'convolutional' and layer['type'] in ['Conv2D', 'DepthwiseConv2D', 'MaxPooling', 'BatchNorm', 'DontCare']:\n",
    "                valid_layers.insert(0, layer)\n",
    "\n",
    "    return valid_layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "IKKUOYNQl7N7"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv2D, Dense, MaxPooling2D, Flatten, Dropout, BatchNormalization, DepthwiseConv2D\n",
    "\n",
    "\n",
    "\n",
    "def build_tf_model_from_dict(model_dict, input_shape=(28, 28, 3)):\n",
    "    \"\"\"\n",
    "    Construye un modelo de TensorFlow a partir de un diccionario JSON expandido.\n",
    "    \"\"\"\n",
    "    print(\"\\nConstruyendo el modelo en TensorFlow desde el JSON expandido...\")\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.Input(shape=input_shape))\n",
    "\n",
    "    for layer in model_dict['layers']:\n",
    "        if layer['type'] == 'Conv2D':\n",
    "            model.add(Conv2D(filters=layer['filters'], kernel_size=(3, 3), strides=int(layer['strides']), padding='same', activation=layer['activation']))\n",
    "\n",
    "        elif layer['type'] == 'DepthwiseConv2D':\n",
    "            model.add(DepthwiseConv2D(kernel_size=(3, 3), strides=int(layer['strides']), padding='same', activation=layer['activation']))\n",
    "\n",
    "        elif layer['type'] == 'BatchNorm':\n",
    "            model.add(BatchNormalization())\n",
    "\n",
    "        elif layer['type'] == 'MaxPooling':\n",
    "            model.add(MaxPooling2D(pool_size=(2, 2), strides=int(layer['strides']), padding='same'))\n",
    "\n",
    "        elif layer['type'] == 'Flatten':\n",
    "            model.add(Flatten())\n",
    "\n",
    "        elif layer['type'] == 'Dense':\n",
    "            model.add(Dense(units=int(layer['units']), activation=layer['activation']))\n",
    "\n",
    "        elif layer['type'] == 'Dropout':\n",
    "            model.add(Dropout(rate=layer['rate']))\n",
    "\n",
    "        elif layer['type'] == 'DontCare':\n",
    "            model.add(DontCareLayer())\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "yxJFgciUmBGz"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import tensorflow as tf\n",
    "\n",
    "def generate_random_architecture():\n",
    "    num_layers = 12\n",
    "    layers = []\n",
    "\n",
    "    for _ in range(num_layers):\n",
    "        layer_type = random.choice([\n",
    "            'Conv2D', 'DepthwiseConv2D', 'BatchNorm', 'MaxPooling', 'Dropout',\n",
    "            'Dense', 'Flatten', 'DontCare', 'Repetition'\n",
    "        ])\n",
    "        layers.append(generate_layer(layer_type))\n",
    "    return {\"layers\": layers}\n",
    "\n",
    "def generate_layer(layer_type):\n",
    "    if layer_type == 'Conv2D':\n",
    "        return {\n",
    "            \"type\": \"Conv2D\",\n",
    "            \"filters\": random.randint(4, 16),  # Rango reducido para filtros\n",
    "            \"strides\": random.choice([1, 2]),\n",
    "            \"activation\": random.choice([\"relu\", \"leaky_relu\", \"sigmoid\", \"tanh\"])\n",
    "        }\n",
    "    elif layer_type == 'DepthwiseConv2D':\n",
    "        return {\n",
    "            \"type\": \"DepthwiseConv2D\",\n",
    "            \"filters\": random.randint(4, 16),  # Rango reducido para filtros\n",
    "            \"strides\": random.choice([1, 2]),\n",
    "            \"activation\": random.choice([\"relu\", \"leaky_relu\", \"sigmoid\", \"tanh\"])\n",
    "        }\n",
    "    elif layer_type == 'BatchNorm':\n",
    "        return {\"type\": \"BatchNorm\"}\n",
    "    elif layer_type == 'MaxPooling':\n",
    "        return {\n",
    "            \"type\": \"MaxPooling\",\n",
    "            \"strides\": random.choice([1, 2])\n",
    "        }\n",
    "    elif layer_type == 'Dropout':\n",
    "        return {\n",
    "            \"type\": \"Dropout\",\n",
    "            \"rate\": random.choice([0.2, 0.3, 0.4, 0.5])\n",
    "        }\n",
    "    elif layer_type == 'Dense':\n",
    "        return {\n",
    "            \"type\": \"Dense\",\n",
    "            \"units\": random.randint(1, 128),  # Rango reducido para unidades\n",
    "            \"activation\": random.choice([\"relu\", \"leaky_relu\", \"sigmoid\", \"tanh\"])\n",
    "        }\n",
    "    elif layer_type == 'Flatten':\n",
    "        return {\"type\": \"Flatten\"}\n",
    "    elif layer_type == 'DontCare':\n",
    "        return {\"type\": \"DontCare\"}\n",
    "    elif layer_type == 'Repetition':\n",
    "        return {\n",
    "            \"type\": \"Repetition\",\n",
    "            \"repetition_layers\": random.randint(1, 3),  # Limitar el número de capas para repetir\n",
    "            \"repetition_count\": random.randint(1, 2)  # Limitar el conteo de repeticiones\n",
    "        }\n",
    "    return {}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\herna\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_addons\\utils\\tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\herna\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_addons\\utils\\ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.12.0 and strictly below 2.15.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.10.1 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import torchaudio\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Resizing, Conv2D, Dropout, BatchNormalization, MaxPooling2D, MaxPool2D, Flatten, Dense, Input, LeakyReLU\n",
    "from tqdm import tqdm\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "# Importar funciones previamente definidas para codificar, decodificar y reparar arquitecturas\n",
    "# Y otras dependencias específicas como `build_tf_model_from_dict`, `generate_random_architecture`, etc.\n",
    "predefined_architectures = [\n",
    "    [0, 30, 0, 0, 3, 0, 0, 0, 1, 0, 0, 0, 2, 1, 0, 0, 0, 16, 0, 0, 3, 0, 0, 0, 1, 0, 0, 0, 2, 1, 0, 0, 5, 0, 0, 0, 4, 256, 0, 0, 3, 1, 0, 0, 4, 1, 2, 0],\n",
    "    [1, 0, 0, 0, 0, 16, 0, 1, 1, 0, 0, 0, 0, 8, 0, 1, 1, 0, 0, 0, 5, 0, 0, 0, 4, 32, 1, 0, 4, 1, 2, 0, 7, 0, 0, 0, 7, 0, 0, 0, 7, 0, 0, 0, 7, 0, 0, 0],\n",
    "    [0, 32, 0, 1, 1, 0, 0, 0, 0, 32, 0, 1, 2, 1, 0, 0, 0, 32, 0, 1, 1, 0, 0, 0, 0, 32, 0, 1, 0, 32, 0, 1, 1, 0, 0, 0, 0, 32, 0, 1, 0, 32, 0, 1, 1, 0, 0, 0]\n",
    "]\n",
    "# Configuración de parámetros\n",
    "class Config:\n",
    "    def __init__(self, architecture='random', epochs=50, sample_rate=None, time=5, n_splits=5, window_size=5):\n",
    "        self.architecture = architecture\n",
    "        self.epochs = epochs\n",
    "        self.sample_rate = sample_rate\n",
    "        self.time = time\n",
    "        self.n_splits = n_splits\n",
    "        self.window_size = window_size\n",
    "\n",
    "# Cargar datos de audio\n",
    "def load_audio_data(directory, window_size, sample_rate):\n",
    "    audio_dict = {}\n",
    "    for file_name in os.listdir(directory):\n",
    "        if file_name.endswith(\".wav\"):\n",
    "            waveform, sr = torchaudio.load(os.path.join(directory, file_name))\n",
    "            if sample_rate is None:\n",
    "                sample_rate = sr\n",
    "            num_windows = int(waveform.shape[1] / (window_size * sample_rate))\n",
    "            for i in range(num_windows):\n",
    "                start = i * window_size * sample_rate\n",
    "                end = (i + 1) * window_size * sample_rate\n",
    "                audio_dict[f\"{file_name}_{i}\"] = waveform[:, start:end].numpy()\n",
    "    return audio_dict, sample_rate\n",
    "\n",
    "# Preprocesar datos de audio\n",
    "def preprocess_audio(audio_dict, sample_rate):\n",
    "    audio_dict = copy.deepcopy(audio_dict)\n",
    "    n_mels = 128\n",
    "    n_fft = int(sample_rate * 0.029)\n",
    "    hop_length = int(sample_rate * 0.010)\n",
    "    win_length = int(sample_rate * 0.025)\n",
    "\n",
    "    for filename, waveform in tqdm(audio_dict.items(), desc='MELSPECTROGRAM'):\n",
    "        waveform = torch.from_numpy(waveform)\n",
    "        spec = torchaudio.transforms.MelSpectrogram(sample_rate=sample_rate, n_fft=n_fft, n_mels=n_mels, hop_length=hop_length, win_length=win_length)(waveform)\n",
    "        spec = torchaudio.transforms.AmplitudeToDB()(spec)\n",
    "        spec = spec.numpy()\n",
    "        spec = (spec - spec.min()) / (spec.max() - spec.min())\n",
    "        audio_dict[filename] = spec\n",
    "    return audio_dict\n",
    "\n",
    "# Padding de los espectrogramas\n",
    "def pad_and_crop_spectrograms(spectrograms, target_shape=(128, 128)):\n",
    "    padded_spectrograms = []\n",
    "    for spec in spectrograms:\n",
    "        if spec.shape[0] > target_shape[0]:\n",
    "            spec = spec[:target_shape[0], :]\n",
    "        if spec.shape[1] > target_shape[1]:\n",
    "            spec = spec[:, :target_shape[1]]\n",
    "        \n",
    "        pad_width = [(0, max(0, target_shape[0] - spec.shape[0])), \n",
    "                     (0, max(0, target_shape[1] - spec.shape[1]))]\n",
    "        \n",
    "        padded_spec = np.pad(spec, pad_width, mode='constant')\n",
    "        padded_spectrograms.append(padded_spec)\n",
    "    return np.array(padded_spectrograms)\n",
    "\n",
    "# Split de audio en train y test\n",
    "def train_test_split_audio(audio_dict):\n",
    "    df = pd.read_csv('Dataset.csv', usecols=['Participant_ID', 'PHQ-9 Score'], dtype={1: str})\n",
    "    df['labels'] = np.zeros([len(df),], dtype=int)\n",
    "    df.loc[df['PHQ-9 Score'] < 10, 'labels'] = 0\n",
    "    df.loc[df['PHQ-9 Score'] >= 10, 'labels'] = 1\n",
    "\n",
    "    labels = df.set_index('Participant_ID').to_dict()['labels']\n",
    "\n",
    "    X, Y = [], []\n",
    "    for filename, data in tqdm(audio_dict.items(), 'LABEL'):\n",
    "        ID = filename[:3]\n",
    "        if ID in labels:\n",
    "            dep = 0 if labels[ID] == 0 else 1\n",
    "            [X.append(x) for x in data]\n",
    "            [Y.append(dep) for x in data]\n",
    "\n",
    "    X = pad_and_crop_spectrograms(X)\n",
    "    Y = np.array(Y)\n",
    "\n",
    "    X = X[..., np.newaxis]\n",
    "    print(f\"X shape: {X.shape}, Y shape: {Y.shape}\")\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "# Función de especificidad\n",
    "def specificity_score(y_true, y_pred):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    return tn / (tn + fp)\n",
    "\n",
    "# Entrenamiento y evaluación de cada arquitectura decodificada\n",
    "def train_and_evaluate_model(model, X_train, Y_train, X_val, Y_val, X_test, Y_test, config):\n",
    "    model.compile(optimizer='adadelta', loss='binary_crossentropy', metrics=[\"accuracy\", 'Precision', 'Recall'])\n",
    "    model.fit(X_train, Y_train, epochs=config.epochs, validation_data=(X_val, Y_val), verbose=0)\n",
    "    results = model.evaluate(X_test, Y_test, verbose=0)\n",
    "\n",
    "    # Obtener predicciones para métricas adicionales\n",
    "    Y_pred = (model.predict(X_test) > 0.5).astype(\"int32\")\n",
    "    accuracy = results[1]\n",
    "    precision = precision_score(Y_test, Y_pred)\n",
    "    recall = recall_score(Y_test, Y_pred)\n",
    "    f1 = f1_score(Y_test, Y_pred)\n",
    "    specificity = specificity_score(Y_test, Y_pred)\n",
    "\n",
    "    return [results[0], accuracy, precision, recall, f1, specificity]\n",
    "\n",
    "\n",
    "# Asumimos que las funciones y clases como `build_tf_model_from_dict`, `generate_random_architecture`, \n",
    "# `encode_model_architecture`, `fixArch`, `decode_model_architecture`, y `train_and_evaluate_model` ya están definidas.\n",
    "\n",
    "\n",
    "# Función principal para generar y entrenar modelos (predefinidos y aleatorios)\n",
    "def generate_and_train_models(predefined_architectures, num_random_models=250, directory='./SM-27', target_shape=(128, 128, 1), use_kfold=True):\n",
    "    results_data = []\n",
    "    config = Config(epochs=50)\n",
    "\n",
    "    # Cargar y preprocesar datos de audio\n",
    "    print(\"Cargando y preprocesando datos de audio...\")\n",
    "    audio_dict, sample_rate = load_audio_data(directory, config.window_size, config.sample_rate)\n",
    "    audio_dict = preprocess_audio(audio_dict, sample_rate)\n",
    "    X, Y = train_test_split_audio(audio_dict)\n",
    "    X_train_val, X_test, Y_train_val, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "    if use_kfold:\n",
    "        kfold = KFold(n_splits=config.n_splits, shuffle=True)\n",
    "\n",
    "    # Entrenar y evaluar arquitecturas predefinidas\n",
    "    for i, architecture in enumerate(predefined_architectures):\n",
    "        print(f\"\\nEvaluando arquitectura predefinida {i + 1}/{len(predefined_architectures)}...\")\n",
    "        evaluate_and_store_model(architecture, X_train_val, X_test, Y_train_val, Y_test, config, use_kfold, kfold, target_shape, results_data)\n",
    "\n",
    "    # Generar, entrenar y evaluar arquitecturas aleatorias\n",
    "    for i in range(num_random_models):\n",
    "        print(f\"\\nGenerando y evaluando modelo aleatorio {i + 1}/{num_random_models}...\")\n",
    "        random_architecture = generate_random_architecture()\n",
    "        encoded_architecture = encode_model_architecture(random_architecture, max_alleles=48)\n",
    "        repaired_architecture = fixArch(encoded_architecture)\n",
    "        evaluate_and_store_model(repaired_architecture, X_train_val, X_test, Y_train_val, Y_test, config, use_kfold, kfold, target_shape, results_data)\n",
    "\n",
    "    # Guardar resultados en CSV\n",
    "    columns = [\"Encoded Architecture\", \"Loss\", \"Accuracy\", \"Precision\", \"Recall\", \"F1\", \"Specificity\"]\n",
    "    results_df = pd.DataFrame(results_data, columns=columns)\n",
    "    results_df.to_csv(\"./model_results_combined_50_epochs.csv\", index=False)\n",
    "    print(\"Resultados guardados en 'model_results_combined.csv'\")\n",
    "\n",
    "# Función para evaluar y almacenar los resultados de un modelo\n",
    "def evaluate_and_store_model(architecture, X_train_val, X_test, Y_train_val, Y_test, config, use_kfold, kfold, target_shape, results_data):\n",
    "    repaired_architecture = fixArch(architecture)\n",
    "    decoded_model_dict = decode_model_architecture(repaired_architecture)\n",
    "    model_results = [repaired_architecture]\n",
    "\n",
    "    if use_kfold:\n",
    "        fold_results = []\n",
    "        for fold, (train_index, val_index) in enumerate(kfold.split(X_train_val)):\n",
    "            print(f\"Entrenando fold {fold + 1}/{config.n_splits}...\")\n",
    "            X_train, X_val = X_train_val[train_index], X_train_val[val_index]\n",
    "            Y_train, Y_val = Y_train_val[train_index], Y_train_val[val_index]\n",
    "            tf_model = build_tf_model_from_dict(decoded_model_dict, input_shape=(target_shape[0], target_shape[1], 1))\n",
    "            fold_results.append(train_and_evaluate_model(tf_model, X_train, Y_train, X_val, Y_val, X_test, Y_test, config))\n",
    "            print(f\"Fold {fold + 1} completado.\")\n",
    "\n",
    "        avg_results = np.mean(fold_results, axis=0)\n",
    "        model_results.extend(avg_results)\n",
    "\n",
    "    else:\n",
    "        X_train, X_val, Y_train, Y_val = train_test_split(X_train_val, Y_train_val, test_size=0.2, random_state=42)\n",
    "        tf_model = build_tf_model_from_dict(decoded_model_dict, input_shape=(target_shape[0], target_shape[1], 1))\n",
    "        single_run_results = train_and_evaluate_model(tf_model, X_train, Y_train, X_val, Y_val, X_test, Y_test, config)\n",
    "        model_results.extend(single_run_results)\n",
    "        print(\"Modelo evaluado sin K-Fold Cross Validation.\")\n",
    "\n",
    "    results_data.append(model_results)\n",
    "\n",
    "# Ejecutar la generación y entrenamiento de modelos, incluyendo arquitecturas predefinidas y modelos aleatorios\n",
    "#generate_and_train_models(predefined_architectures, num_random_models=270, target_shape=(128, 128, 1), use_kfold=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "arqs_to_test = [[7, 0, 0, 0, 3, 0, 0, 0, 7, 0, 0, 0, 4, 1, 0, 0, 7, 0, 0, 0, 7, 0, 0, 0, 1, 0, 0, 0, 7, 0, 0, 0, 7, 0, 0, 0, 4, 1, 0, 0, 7, 0, 0,0,7,0,0,0],[6, 9, 0, 1, 3, 3, 0, 0, 2, 1, 0, 0, 7, 0, 0, 0, 5, 0, 0, 0, 3, 2, 0, 0, 7, 0, 0, 0, 7, 0, 0, 0, 7, 0, 0, 0, 8, 3, 2, 0, 1, 0, 0, 0, 7, 0, 0, 0],\n",
    "                ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MELSPECTROGRAM: 100%|██████████| 5890/5890 [00:18<00:00, 313.67it/s]\n",
      "LABEL: 100%|██████████| 5890/5890 [00:00<00:00, 654665.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (5890, 128, 128, 1), Y shape: (5890,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing architectures:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7, 0, 0, 0, 3, 0, 0, 0, 7, 0, 0, 0, 4, 1, 0, 0, 7, 0, 0, 0, 7, 0, 0, 0, 1, 0, 0, 0, 7, 0, 0, 0, 7, 0, 0, 0, 4, 1, 0, 0, 7, 0, 0, 0, 7, 0, 0, 0]\n",
      "{'layers': [{'type': 'Conv2D', 'filters': 32, 'strides': 1, 'activation': 'relu'}, {'type': 'DontCare'}, {'type': 'Dropout', 'rate': 0.2}, {'type': 'DontCare'}, {'type': 'Dense', 'units': 1, 'activation': 'relu'}, {'type': 'DontCare'}, {'type': 'DontCare'}, {'type': 'BatchNorm'}, {'type': 'DontCare'}, {'type': 'DontCare'}, {'type': 'Dense', 'units': 1, 'activation': 'relu'}, {'type': 'DontCare'}, {'type': 'DontCare'}, {'type': 'Flatten'}, {'type': 'Dense', 'units': 1, 'activation': 'sigmoid'}]}\n",
      "\n",
      "Construyendo el modelo en TensorFlow desde el JSON expandido...\n",
      "Training fold 1/10...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing architectures:   0%|          | 0/2 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"c:\\Users\\herna\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\herna\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\herna\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\herna\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 993, in train_step\n        y_pred = self(x, training=True)\n    File \"c:\\Users\\herna\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"c:\\Users\\herna\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 295, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential_3\" is incompatible with the layer: expected shape=(None, 28, 28, 3), found shape=(None, 128, 128, 1)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 23\u001b[0m\n\u001b[0;32m     21\u001b[0m Y_train, Y_val \u001b[38;5;241m=\u001b[39m Y_train_val[train_index], Y_train_val[val_index]\n\u001b[0;32m     22\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madadelta\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPrecision\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRecall\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m---> 23\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m results \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(X_test, Y_test, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFold \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m results: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\herna\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filekdremh0a.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"c:\\Users\\herna\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\herna\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\herna\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\herna\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 993, in train_step\n        y_pred = self(x, training=True)\n    File \"c:\\Users\\herna\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"c:\\Users\\herna\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 295, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential_3\" is incompatible with the layer: expected shape=(None, 28, 28, 3), found shape=(None, 128, 128, 1)\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "config = Config(epochs=50)\n",
    "# Load and preprocess data outside the loop\n",
    "audio_dict, sample_rate = load_audio_data('./SM-27', config.window_size, config.sample_rate)\n",
    "audio_dict = preprocess_audio(audio_dict, sample_rate)\n",
    "X, Y = train_test_split_audio(audio_dict)\n",
    "X_train_val, X_test, Y_train_val, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "for arq in tqdm(arqs_to_test, desc=\"Testing architectures\"):\n",
    "  print(arq)\n",
    "  fixed_arch = fixArch(arq)\n",
    "  model_dict = decode_model_architecture(fixed_arch)\n",
    "  print(model_dict)\n",
    "  # Inside the loop\n",
    "  model = build_tf_model_from_dict(model_dict)\n",
    "  kfold = KFold(n_splits=10, shuffle=True)\n",
    "  for fold, (train_index, val_index) in enumerate(kfold.split(X_train_val)):\n",
    "    print(f\"Training fold {fold + 1}/10...\")\n",
    "    X_train, X_val = X_train_val[train_index], X_train_val[val_index]\n",
    "    Y_train, Y_val = Y_train_val[train_index], Y_train_val[val_index]\n",
    "    model.compile(optimizer='adadelta', loss='binary_crossentropy', metrics=[\"accuracy\", 'Precision', 'Recall'])\n",
    "    model.fit(X_train, Y_train, epochs=config.epochs, validation_data=(X_val, Y_val), verbose=0)\n",
    "    results = model.evaluate(X_test, Y_test, verbose=0)\n",
    "    print(f\"Fold {fold + 1} results: {results}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U-Wn8zMQSpva",
    "outputId": "a483f55b-1869-445c-c1c5-0560babd8122"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5086797843097632"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model = max(best_models, key=lambda x: x['fitness'])\n",
    "best_model['fitness']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aAVZIg3-bbII",
    "outputId": "1ae23da3-61bf-494f-e982-78c403e5c7a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6, 9, 0, 1, 3, 3, 0, 0, 2, 1, 0, 0, 7, 0, 0, 0, 5, 0, 0, 0, 3, 2, 0, 0, 7, 0, 0, 0, 7, 0, 0, 0, 7, 0, 0, 0, 8, 3, 2, 0, 1, 0, 0, 0, 7, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(best_model['individual'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7wGVVEFTS844",
    "outputId": "249d7596-ce36-4bee-ac0a-76b799ca1168"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'layers': [{'type': 'Conv2D',\n",
       "   'filters': 32,\n",
       "   'strides': 1,\n",
       "   'activation': 'relu'},\n",
       "  {'type': 'DepthwiseConv2D',\n",
       "   'filters': 9,\n",
       "   'strides': 1,\n",
       "   'activation': 'leaky_relu'},\n",
       "  {'type': 'Dropout', 'rate': 0.5},\n",
       "  {'type': 'MaxPooling', 'strides': 2},\n",
       "  {'type': 'DontCare'},\n",
       "  {'type': 'Flatten'},\n",
       "  {'type': 'Dropout', 'rate': 0.4},\n",
       "  {'type': 'DontCare'},\n",
       "  {'type': 'DontCare'},\n",
       "  {'type': 'DontCare'},\n",
       "  {'type': 'DontCare'},\n",
       "  {'type': 'DontCare'},\n",
       "  {'type': 'DontCare'},\n",
       "  {'type': 'DontCare'},\n",
       "  {'type': 'DontCare'},\n",
       "  {'type': 'DontCare'},\n",
       "  {'type': 'BatchNorm'},\n",
       "  {'type': 'DontCare'},\n",
       "  {'type': 'Flatten'},\n",
       "  {'type': 'Dense', 'units': 1, 'activation': 'sigmoid'}]}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dict = decode_model_architecture(best_model['individual'])\n",
    "model_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M5AVvAx2S-5Q",
    "outputId": "edc1d9b4-e9b4-4b58-d0bd-0cebdf300549"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Construyendo el modelo en TensorFlow desde el JSON expandido...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Sequential name=sequential, built=True>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn = build_tf_model_from_dict(model_dict)\n",
    "cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "en2Yo7OtTAay",
    "outputId": "f3cd2e93-fa33-4f24-f7c4-95727a0fad37"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "cnn.save('/drive/MyDrive/NAS/EC_Project/best_model_mu100_gens1000_F0p5_5000mu_autoadapt.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "ieouY-Z7TBuV"
   },
   "outputs": [],
   "source": [
    "cnn.save('/drive/MyDrive/NAS/EC_Project/best_model_mu100_gens1000_F0p5_5000mu_autoadapt.keras')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
